# Generative AI

It is a subset of traditional ML. The ML algorithms that work behind generative
AI do so by exploiting the statistical patterns present in the massive datasets of
content that was originally generated by humans.

## LLMs

### Definition

LLMs (Large Language Models) are generative AI models specifically designed
to understand text. All LLMs are powered by the Transformer (Google, 2017)
architecture. They are designed to take in input text and repeatedly generate the
next token or word that appropriately “completes” the input text. For example,
an LLM can be given the input:
    Where is Ganymede located in the solar system?
In response, the LLM might generate the following output:
    Ganymede is a moon of Jupiter and is located in the solar system
    within Jupiter’s orbit.
Here, the model essentially completed the given input by repeatedly generating
the next word or token that fits appropriately.
These models have abilities beyond just language and are capable of breaking
down complex tasks, reasoning and problem solving.
It is commonly accepted that as the size (in terms of number of parameters) of
an LLM increases, so does its understanding of language. At the same time, it is
also true the smaller models can be fine-tuned to perform well on specific tasks.

## Foundation Model

A foundation model is a powerful AI tool that can do many different things
after being trained on lots of diverse data. These models are incredibly
versatile and provide a solid base for creating various AI applications,
like a strong foundation holds up different kind of buildings. By using a
foundation model, we have a strong starting point for building specialized AI tasks.

Foundation Models (GPT, BERT) and Traditional Models (Linear regression,
SVM) are two distinct approaches in the field of artificial intelligence
with different strengths. Foundation Models, which are built on large,
diverse datasets, have the incredible ability to adapt and perform well
on many different tasks. In contrast, Traditional Models specialize in
specific tasks by learning from smaller, focused datasets, making them
more straightforward and efficient for targeted applications.

## Terminology

* Prompt: The input given to an LLM is called the prompt.
* Context Window: The space/memory that is available to the prompt is called the
 context window. It is essentially the maximum size of the prompt that the model
 can handle before it performs poorly. It is limited to a few thousand of words
 but also varies model to model.
* Completion: The output of the an LLM when given a prompt is called the completion.
    Generally, the completion consists of the prompt and the text generated by
    the model by repeatedly generating the next token or word, though almost all
    applications omit the prompt from the model’s output when showing it to users

## Transformers

TBD

## Prompting and Prompt Engineering

### Prompting

The text that is fed to LLMs as input is called the prompt and the act of
providing the input is called prompting.

### Prompt Engineering

The process of tweaking the prompt provided to an LLM so that it gives the
best possible result is called prompt engineering. Some common techniques are
given below.

### In-Context Learning (ICL)

In ICL, we add examples of the task we are doing in the prompt. This adds
more context for the model in the prompt, allowing the model to “learn” more
about the task detailed in the prompt.

#### Zero-Shot Inference

For example, we might be doing semantic classification using our LLM. In that
case, a prompt could be:

```bash
Classify this review: I loved this movie!
Sentiment:
```

This prompt works well with large LLMs but smaller LLMs might fail to follow
the instruction due to their size and fewer number of features. This is also called
zero-shot inference since our prompt has zero examples regarding what the
model is expected to output.

#### Few-Shot Inference

This is where ICL comes into play. By adding examples to the prompt, even a
smaller LLM might be able to follow the instruction and figure out the correct
output. An example of such a prompt is shown below. This is also called
one-shot inference since we are providing a single example in the prompt:

```bash
Classify this review: I loved this movie!
Sentiment: Positive
Classify this review: I don’t like this chair.
Sentiment:
```

Here, we first provide an example to the model and then ask it to figure out the
output for the I don’t like this chair review.
Sometimes, a single example won’t be enough for the model, for example when
the model is even smaller. We’d then add multiple examples in the prompt. This
is called few-shot inference.

In other words:

* Larger models are good at zero-shot inference.
* For smaller models, we might need to add examples to the prompt, for
few-shot inference.

### Inference Configuration Parameters

* Max New Tokens: This is used to limit the maximum number of new tokens that
    should be generated by the model in its output. The model might output
    fewer tokens (for example,it predicts <EOS> before reaching the limit)
     but not more than this number.
* Greedy vs Random Sampling: Some models also give the user control over whether
 the model should use greedy or random sampling.
* Sample Top-K and Sample Top-P: Sample Top-K and Sample Top-P are used to limit
 the random sampling of a model.
A top-K value instructs the model to only consider K words with the highest
probabilities in its random sampling. Consider the following softmax output:

    ```bash
    Probability Word
    0.20 cake
    0.10 donut
    0.02 banana
    0.01 apple
    . . . . . .
    ```

    If K = 3, the model will select one of cake, donut or banana. This allows the
    model to have variability while preventing the selection of some highly improbable
    words in its output.
    The top-P value instructs the model to only consider words with the highest
    probabilities such that their cumulative probability, p1 + p2 + · · · + pK ≤
    P. For example, considering the above output, if we set P = 0.30, the model
    will only consider the words cake and donut since 0.20 + 0.10 ≤ 0.30.

* Temperature: Temperature is also another parameter used to control random
sampling. It determines the shape of the probability distribution that the model
 calculates for the next word.
    Intuitively, a higher temperature increases the randomness of the model while
    a lower temperature decreases the randomness of the model. This temperature is
    passed as a scaling factor to the final softmax layer of the decoder.
    If we pick a cooler temperature (T < 1), the probability distribution is strongly
    peaked. In other words, one (or a few more) words have very high probabilities
    while the rest of the words have very low probabilities:

    ```bash
    Probability Word
    0.001 apple
    0.002 banana
    0.400 cake
    0.012 donut
    . . . . . .
    ```

    Notice how cake has a 40% chance of being picked while other words have very
    small chances of being picked. The resulting text will be less random.
    On the other hand, if we pick a warmer temperature (T > 1), the probability
    distribution is broader, flatter and more evenly spread over the tokens:

        ```bash
        Probability Word
        0.040 apple
        0.080 banana
        0.150 cake
        0.120 donut
        . . . . . .
        ```
    Notice how none of the words have a clear advantage over the other words.
    The model generates text with a higher amount of randomness and has more
    variability in its output.
    Clearly, when T = 1, the model uses the softmax output as is for random
    sampling.

## Pre-training Large Language Models

### Initial Training Process (Pre-training)

The initial training process of an LLM is called as pre-training. LLMs work by
learning a deep statistical representation of language and this deep representation
is developed during pre-training.
At a high-level, during pre-training, the model is fed large amounts of unstructured
 textual data, ranging from gigabytes to petabytes in size. The data is
pulled from many sources such as web crawling and corpora of text compiled
specifically to train LLMs.
The pre-training process is self-supervised. The model internalizes the patterns
and structures present in the language. These patterns then unable the model to
complete its training objective, which depends on the architecture of the model.
In other words, during pre-training, the model weights get updated to minimize
the loss of training objective.
Clearly, this step requires a lot of compute and the use of GPUs.

Additionally, since the data is coming from public sources such as the internet,
there is often a data quality filter applied before feeding the data to the LLM
so that the training data is of high quality, has low bias and does not have
harmful content. Due to this, only about 1-3% of the original tokens are used
for pre-training.

### Training Objectives for Transformer Variants

The three configurations of a Transformer are trained with different training
objectives and thus, learn to perform different tasks.

#### Encoder-only Models (Autoencoding Models)

The encoder-only variants of Transformers are also called autoencoding models.
They are pre-trained using Masked Language Modeling (MLM). In MLM,
tokens in the input sequence are randomly masked and the training objective is
to predict the masked tokens in order to reconstruct the original input sequence.
This is also called a denoising objective since the masking of the tokens can be
thought of as adding noise to the input sequence and then predicting the masked
tokens can be thought of as removing that noise from the input sequence.
Autoencoding models build bidirectional context representations of the input
sequence, meaning that model has an understanding of the full context of the
token rather than just the tokens that come before it.

![Enconder Only](encoder_only.png)

These models are usually suited to tasks that benefit from this bidirectional
context such as sentiment analysis, named entity recognition and word classification,
etc.
Examples: BERT, ROBERTA.

#### Decoder-only Models (Autoregressive Models)

The decoder-only variants of Transformers are also called autoregressive models.
They are pre-trained using Causal Language Modeling (CLM). In CLM, the
training objective is to predict the next token based on the previous sequence of
tokens. The tokens of the input sequence are masked and the model can only see
the input tokens leading up to the token being predicted at the moment. The
model has no knowledge of the tokens that come after this token. The model
then iterates over the input sequence one-by-one to predict the next token. Thus,
in contrast to autoencoding models, the model builds a unidirectional context
for each token.

![Deconder Only](decoder_only.png)

By learning to predict the next token from a vast number of examples, the model
builds a statistical representation of the language. Predicting the next token is
sometimes called full language modeling by researchers.
These mode,ls are most suitable for text generation but large autoregressive
models also show strong zero-shot inference ability and can perform a variety of
tasks.
Examples: GPT, BLOOM.

#### Encoder-Decoder Models (Sequence-to-Sequence Models)

Encoder-Decoder Models (Sequence-to-Sequence Models)
The encoder-decoder variants of Transformers are also called sequence-tosequence
 models. The exact details of pre-training objective vary from model to model.
 For example, FLAN-T5 is trained using span corruption. In span corruption, a part
of the input sequence is masked and replaced by a sentinel token. These sentinel
tokens are special tokens added to the vocabulary that to do not correspond to any
actual word from the dataset. The decoder then has to reconstruct the sentence
autoregressively. The output is the sentinel token followed by the predicted
tokens.

![Encoder-decoder](encoder_decoder.png)

We can use such models for tasks such as translation, summarization and question
answering. They are most useful where the input and output both are bodies of
text.
Examples: FLAN-T5, BART.

### Data

* [Common crawl](https://commoncrawl.org/)
* [Github dataset](https://www.githubarchive.org/)
* [Wikipedia](https://dumps.wikimedia.org/)
* [Gutenberg](https://www.gutenberg.org/)
