{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Hugging Face Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll explore Hugging Face's tokenizers by using a pretrained\n",
    "model. Hugging Face has many tokenizers available that have already been trained\n",
    "for specific models and tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a pretrained tokenizer to use\n",
    "my_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding: Text to Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: String Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!']\n"
     ]
    }
   ],
   "source": [
    "# Simple method getting tokens from text\n",
    "raw_text = '''Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!'''\n",
    "tokens = my_tokenizer.tokenize(raw_text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# This method also returns special tokens depending on the pretrained tokenizer\n",
    "detailed_tokens = my_tokenizer(raw_text).tokens()\n",
    "\n",
    "print(detailed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: Integer ID Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "# Way to get tokens as integer IDs\n",
    "print(my_tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n",
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "print(detailed_tokens)\n",
    "\n",
    "# Tokenizer method to get the IDs if we already have the tokens as strings\n",
    "detailed_ids = my_tokenizer.convert_tokens_to_ids(detailed_tokens)\n",
    "print(detailed_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way can look a little complex but can be useful when working with\n",
    "tokenizers for certain tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns an object that has a few different keys available\n",
    "my_tokenizer(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n"
     ]
    }
   ],
   "source": [
    "# focus on `input_ids` which are the IDs associated with the tokens.\n",
    "print(my_tokenizer(raw_text).input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding: Tokens to Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We of course can use the tokenizer to go from token IDs to tokens and back to text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] Rory's shoes are magenta and so are Corey's but they aren't nearly as dark! [SEP]\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Integer IDs for tokens\n",
    "ids = my_tokenizer.encode(raw_text)\n",
    "\n",
    "# The inverse of the .enocde() method: .decode()\n",
    "my_tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To ignore special tokens (depending on pretrained tokenizer)\n",
    "my_tokenizer.decode(ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Rory',\n",
       " \"'\",\n",
       " 's',\n",
       " 'shoes',\n",
       " 'are',\n",
       " 'mage',\n",
       " '##nta',\n",
       " 'and',\n",
       " 'so',\n",
       " 'are',\n",
       " 'Corey',\n",
       " \"'\",\n",
       " 's',\n",
       " 'but',\n",
       " 'they',\n",
       " 'aren',\n",
       " \"'\",\n",
       " 't',\n",
       " 'nearly',\n",
       " 'as',\n",
       " 'dark',\n",
       " '!',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of tokens as strings instead of one long string\n",
    "my_tokenizer.convert_ids_to_tokens(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on the Unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> One thing to consider is if a string is outside of the tokenizer's vocabulary,\n",
    "> also known as an \"unkown\" token.\n",
    "> \n",
    "> They are typically represented with `[UNK]` or\n",
    "> some other similar variant.\n",
    "\n",
    "\n",
    "<!--\n",
    "If the tokenizer encoded the text so each character was a token (which is\n",
    "actually not as easy as it sounds), then it would be impossible to have an\n",
    "\"unknown\" token. Word-based tokenization will always be in danger of having \n",
    "\"unknown\" tokens since it's virtually impossible to have every possible word (\n",
    "and \"non-word\") in its vocabulary!\n",
    "\n",
    "And so you might think that subword tokenization wouldn't have an issue with\n",
    "\"unknown\" tokens. And although there are fewer than word-based tokenization, it\n",
    "does happen!\n",
    "\n",
    "--------------------------------------------------------------------------------\n",
    "\n",
    "Tokenizers are specific so it's important to use a tokenizer that will recognize\n",
    "most of the text you're working with! For example, a lot of tokenizers might not\n",
    "consider emoji as tokens but could be really important if emoji are especially\n",
    "numerous in your data (like a corpus of chat messages)!\n",
    "\n",
    "If you're seeing a lot of \"unknown\" tokens with the text you're working with,\n",
    "might consider using a different tokenizer appropiate for the task. Or it's also\n",
    "possible to fine-tune a pretrained model or train one from scratch!\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¥± the dog next door kept barking all night!!\n",
      "['[CLS]', '[UNK]', 'the', 'dog', 'next', 'door', 'kept', 'barking', 'all', 'night', '!', '!', '[SEP]']\n",
      "[CLS] [UNK] the dog next door kept barking all night!! [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = 'ðŸ¥± the dog next door kept barking all night!!'\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wow my dad thought mcdonalds sold tacos ðŸ’€\n",
      "['[CLS]', 'w', '##ow', 'my', 'dad', 'thought', 'm', '##c', '##don', '##ald', '##s', 'sold', 'ta', '##cos', '[UNK]', '[SEP]']\n",
      "[CLS] wow my dad thought mcdonalds sold tacos [UNK] [SEP]\n"
     ]
    }
   ],
   "source": [
    "phrase = '''wow my dad thought mcdonalds sold tacos \\N{SKULL}'''\n",
    "ids = my_tokenizer.encode(phrase)\n",
    "print(phrase)\n",
    "print(my_tokenizer.convert_ids_to_tokens(ids))\n",
    "print(my_tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Properties of Hugging Face's Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load a couple different models:\n",
    "\n",
    "* `bert-base-cased` ([doc](https://huggingface.co/docs/transformers/model_doc/bert))\n",
    "* `xlm-roberta-base` ([doc](https://huggingface.co/docs/transformers/model_doc/xlm-roberta))\n",
    "* `google/pegasus-xsum` ([doc](https://huggingface.co/docs/transformers/model_doc/pegasus))\n",
    "* `allenai/longformer-base-4096` ([doc](https://huggingface.co/docs/transformers/model_doc/longformer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 615/615 [00:00<00:00, 1.03MB/s]\n",
      "sentencepiece.bpe.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.07M/5.07M [00:01<00:00, 3.49MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.10M/9.10M [00:00<00:00, 14.6MB/s]\n",
      "tokenizer_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87.0/87.0 [00:00<00:00, 176kB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.39k/1.39k [00:00<00:00, 4.83MB/s]\n",
      "spiece.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.91M/1.91M [00:00<00:00, 5.35MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.52M/3.52M [00:00<00:00, 26.7MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 65.0/65.0 [00:00<00:00, 82.0kB/s]\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 694/694 [00:00<00:00, 1.88MB/s]\n",
      "vocab.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 899k/899k [00:00<00:00, 2.36MB/s]\n",
      "merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 231MB/s]\n",
      "tokenizer.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.36M/1.36M [00:00<00:00, 49.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_names = (\n",
    "    \"bert-base-cased\",\n",
    "    \"xlm-roberta-base\",\n",
    "    \"google/pegasus-xsum\",\n",
    "    \"allenai/longformer-base-4096\",\n",
    ")\n",
    "\n",
    "model_tokenizers = {\n",
    "    model_name: AutoTokenizer.from_pretrained(model_name) for model_name in model_names\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `model_max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tmax length: 512\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tmax length: 4096\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    max_length = temp_tokenizer.model_max_length\n",
    "    print(f\"{model_name}\\n\\tmax length: {max_length}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've already mentioned special tokens like the \"unknown\" token. Different models use different ways to distinguish special tokens and not all models cover all the special tokens since it's dependent on the model's task it was trained for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tspecial tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tspecial tokens: ['</s>', '<unk>', '<pad>', '<mask_2>', '<mask_1>', '<unk_2>', '<unk_3>', '<unk_4>', '<unk_5>', '<unk_6>', '<unk_7>', '<unk_8>', '<unk_9>', '<unk_10>', '<unk_11>', '<unk_12>', '<unk_13>', '<unk_14>', '<unk_15>', '<unk_16>', '<unk_17>', '<unk_18>', '<unk_19>', '<unk_20>', '<unk_21>', '<unk_22>', '<unk_23>', '<unk_24>', '<unk_25>', '<unk_26>', '<unk_27>', '<unk_28>', '<unk_29>', '<unk_30>', '<unk_31>', '<unk_32>', '<unk_33>', '<unk_34>', '<unk_35>', '<unk_36>', '<unk_37>', '<unk_38>', '<unk_39>', '<unk_40>', '<unk_41>', '<unk_42>', '<unk_43>', '<unk_44>', '<unk_45>', '<unk_46>', '<unk_47>', '<unk_48>', '<unk_49>', '<unk_50>', '<unk_51>', '<unk_52>', '<unk_53>', '<unk_54>', '<unk_55>', '<unk_56>', '<unk_57>', '<unk_58>', '<unk_59>', '<unk_60>', '<unk_61>', '<unk_62>', '<unk_63>', '<unk_64>', '<unk_65>', '<unk_66>', '<unk_67>', '<unk_68>', '<unk_69>', '<unk_70>', '<unk_71>', '<unk_72>', '<unk_73>', '<unk_74>', '<unk_75>', '<unk_76>', '<unk_77>', '<unk_78>', '<unk_79>', '<unk_80>', '<unk_81>', '<unk_82>', '<unk_83>', '<unk_84>', '<unk_85>', '<unk_86>', '<unk_87>', '<unk_88>', '<unk_89>', '<unk_90>', '<unk_91>', '<unk_92>', '<unk_93>', '<unk_94>', '<unk_95>', '<unk_96>', '<unk_97>', '<unk_98>', '<unk_99>', '<unk_100>', '<unk_101>', '<unk_102>']\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tspecial tokens: ['<s>', '</s>', '<unk>', '<pad>', '<mask>']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    special_tokens = temp_tokenizer.all_special_tokens\n",
    "    print(f\"{model_name}\\n\\tspecial tokens: {special_tokens}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also call the specific token you're interested in to see its representation.Ã§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK]'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_tokenizers[\"bert-base-cased\"].unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-cased\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='[UNK]'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token=None\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='[MASK]'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='[SEP]'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='[CLS]'\n",
      "\n",
      "\n",
      "xlm-roberta-base\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n",
      "google/pegasus-xsum\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token=None\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask_2>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token=None\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token=None\n",
      "\n",
      "\n",
      "allenai/longformer-base-4096\n",
      "\tUnknown: \n",
      "\t\ttemp_tokenizer.unk_token='<unk>'\n",
      "\tBeginning of Sequence: \n",
      "\t\ttemp_tokenizer.bos_token='<s>'\n",
      "\tEnd of Sequence: \n",
      "\t\ttemp_tokenizer.eos_token='</s>'\n",
      "\tMask: \n",
      "\t\ttemp_tokenizer.mask_token='<mask>'\n",
      "\tSentence Separator: \n",
      "\t\ttemp_tokenizer.sep_token='</s>'\n",
      "\tClass of Input: \n",
      "\t\ttemp_tokenizer.cls_token='<s>'\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model_name, temp_tokenizer in model_tokenizers.items():\n",
    "    print(f\"{model_name}\")\n",
    "    print(f\"\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}\")\n",
    "    print(f\"\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}\")\n",
    "    print(f\"\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}\")\n",
    "    print(f\"\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}\")\n",
    "    print(f\"\\tSentence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}\")\n",
    "    print(f\"\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tokenizers will have different special tokens defined. They might have tokens representing:\n",
    "\n",
    "* Unknown token\n",
    "* Beginning of sequence token\n",
    "* Separator token\n",
    "* Token used for padding\n",
    "* Classifier token\n",
    "* Token used for masking values\n",
    "\n",
    "Additionally, there may be multiple subtypes of each special token. For example, some tokenizers have multiple different unknown tokens (e.g. `<unk>` and `<unk_2>`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hugging Face Tokenizers Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different tokenizers can create very different tokens for the same piece of text. When choosing a tokenizer, consider what properties are important to you, such as the maximum length and the special tokens.\n",
    "\n",
    "If none of the available tokenizers perform the way you need them to, you can also fine-tune a tokenizer to adjust it for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation on Hugging Face Tokenizers and Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Hugging Face's PreTrainedTokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer)\n",
    "* [Hugging Face's AutoTokenizer](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoTokenizer)\n",
    "\n",
    "Documentation on some available models:\n",
    "* [bert-base-cased](https://huggingface.co/docs/transformers/model_doc/bert)\n",
    "* [xlm-roberta-base](https://huggingface.co/docs/transformers/model_doc/xlm-roberta)\n",
    "* [google/pegasus-xsum](https://huggingface.co/docs/transformers/model_doc/pegasus)\n",
    "* [allenai/longformer-base-4096](https://huggingface.co/docs/transformers/model_doc/longformer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
