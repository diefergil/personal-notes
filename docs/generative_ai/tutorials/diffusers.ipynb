{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "989abb84",
   "metadata": {},
   "source": [
    "# Diffusion models and the ``diffusers`` library from ðŸ¤—\n",
    "\n",
    "Diffusion models are at the forefront of the research and the applications of Gen-AI in Computer Vision. Many research papers and innovations are published every day in the field. Most of the open-source models and results are released and implemented in the ðŸ¤— (Hugging Face) libraries: [transformers](https://huggingface.co/docs/transformers/index) for LLMs and [diffusers](https://huggingface.co/docs/diffusers/index) for Diffusion Models. Both of them are high-level libraries built on top of PyTorch.\n",
    "\n",
    "In this exercise we are going to have fun with some of the amazing capabilities of the ``diffusers`` library along with the ðŸ¤— ecosystem of [Models](https://huggingface.co/models), an online repository where open-source models are hosted and are available for use free of charge.\n",
    "\n",
    "> Please note: always check the license of the models before using them in a professional setting because some restrict commercial use.\n",
    "\n",
    "It is worth noting here that the ðŸ¤— also includes freely-available [Datasets](https://huggingface.co/datasets) and [Spaces](https://huggingface.co/spaces), where people can publish demos of interesting applications of Gen-AI and more.\n",
    "\n",
    "> **IMPORTANT**: when using this notebook within the Udacity Workspace, you need to restart the notebook when requested because you will run out of GPU memory otherwise. Be on the lookout for the **RESTART NOW** message\n",
    "\n",
    "> **NOTE**: because we are using a lot of different models, the first time you run them you will see a lot of messages from the diffusers library alerting you that it is downloading files from the internet. Those are expected, and they do NOT constitute an error. Just continue on.\n",
    "\n",
    "Let's start by importing the elements we are going to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44517ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86acb4fd",
   "metadata": {},
   "source": [
    "Now we're going to see a few applications in the field of image generation and editing, as well as video generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa27e732",
   "metadata": {},
   "source": [
    "## Unconditional generation\n",
    "\n",
    "In this type of image generation, the generator is free to do whatever it wants, i.e., it is in \"slot machine\" mode: we pull the lever and the model will generate something related to the training set it has been trained on. The only control we have here is the random seed.\n",
    "\n",
    "You can see all the available unconditional diffusion models compatible with the ``diffusers`` library [here](https://huggingface.co/models?pipeline_tag=unconditional-image-generation&library=diffusers&sort=trending). For example, at the moment the first few results look like this:\n",
    "\n",
    "\n",
    "You can substitute the ``model_name`` value in the cell below with any of these model names, like for example ``google/ddpm-cifar10-32`` or ``WiNE-iNEFF/Minecraft-Skin-Diffusion-V3``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_gen = torch.manual_seed(12418351)\n",
    "\n",
    "model_name = 'google/ddpm-celebahq-256'\n",
    "\n",
    "model = DiffusionPipeline.from_pretrained(model_name).to(\"cuda\")\n",
    "image = model(generator=rand_gen).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7251e6dd",
   "metadata": {},
   "source": [
    "While most of them can be used this way, some require some special handling. In that case, the code needed to use them is typically reported in the Model Card that can be accessed by simply clicking on the name of the models in [this](https://huggingface.co/models?pipeline_tag=unconditional-image-generation&library=diffusers&sort=trending) list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5366717",
   "metadata": {},
   "source": [
    "## Text-to-image\n",
    "\n",
    "This is a class of conditional image generation models. The conditioning happens through text: we provide a text prompt, and the model creates an image following that description.\n",
    "\n",
    "You can find a list of available text-to-image models [here](https://huggingface.co/models?pipeline_tag=text-to-image&library=diffusers&sort=trending).\n",
    "\n",
    "For example, here we use the Stable Diffusion XL Turbo (a new version of Stable Diffusion XL optimized for super-fast inference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\", \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f7b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A photo of a wild horse jumping in the desert dramatic sky intricate details National Geographic 8k high details\"\n",
    "\n",
    "rand_gen = torch.manual_seed(423122981)\n",
    "\n",
    "image = pipe(\n",
    "    prompt=prompt, \n",
    "    num_inference_steps=1, # For this model you can use 1, but for normal Stable Diffusion you should use 25 or 50\n",
    "    guidance_scale=1.0, # For this model 1 is fine, for normal Stable Diffusion you should use 6 or 7, or up to 10 or so\n",
    "    negative_prompt=[\"overexposed\", \"underexposed\"], \n",
    "    generator=rand_gen\n",
    ").images[0]\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031869c5",
   "metadata": {},
   "source": [
    "> **RESTART NOW**: please restart the notebook, then start running from the next cell and continue on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b719f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e6c4b",
   "metadata": {},
   "source": [
    "This is another example, where we use a nice model by Playground AI that generates artistic images instead of photorealistic ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da3571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"playgroundai/playground-v2-1024px-aesthetic\",\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    add_watermarker=False,\n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ccb845",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A scifi astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\n",
    "rand_gen = torch.manual_seed(42312981)\n",
    "\n",
    "image  = pipe(prompt=prompt, guidance_scale=3.0, generator=rand_gen).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff313f25",
   "metadata": {},
   "source": [
    "## Image-to-image\n",
    "\n",
    "In the image-to-image task we condition the production of the Diffusion Model through an input image. There are many ways of doing this. Here we look at transforming a barebone sketch of a scene in a beautiful, highly-detailed representation of the same.\n",
    "\n",
    "Let's start by creating a sketch. We could create that manually, but since we're here, let's use SDXL-Turbo instead.\n",
    "\n",
    "> NOTE: it is important for the sketch not to be too detailed and complicated. Flat colors typically work best, although this is not an absolute rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b259457",
   "metadata": {},
   "source": [
    "> **PLEASE RESTART NOW** to free GPU memory, then continue on from the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91c584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f87c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A tree and a house, made by a child with 3 colors\"\n",
    "rand_gen = torch.manual_seed(423121)\n",
    "\n",
    "pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "    \"stabilityai/sdxl-turbo\", \n",
    "    torch_dtype=torch.float16, \n",
    "    variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "image  = pipe(\n",
    "    prompt=prompt, \n",
    "    num_inference_steps=2,\n",
    "    guidance_scale=2,\n",
    "    generator=rand_gen\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be906b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.save(\"sketch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391671a",
   "metadata": {},
   "source": [
    "> **PLEASE RESTART NOW** to free GPU memory, then continue on from the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e3c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "import PIL\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ae827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PIL.Image.open(\"sketch.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd29e78f",
   "metadata": {},
   "source": [
    "Now we can use the [Kandinsky](https://github.com/ai-forever/Kandinsky-2) model to generate an image that respects the subjects and their positions in our sketch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8909b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline\n",
    "\n",
    "prior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n",
    "pipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246c2e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "original_image = image.copy().resize((768, 768))\n",
    "\n",
    "prompt = \"A photograph of a house in the fall, high details, broad daylight\"\n",
    "negative_prompt = \"low quality, bad quality\"\n",
    "    \n",
    "rand_gen = torch.manual_seed(67806801)\n",
    "image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, generator=rand_gen).to_tuple()\n",
    "\n",
    "new_image = pipeline(\n",
    "    image=original_image, \n",
    "    image_embeds=image_embeds, \n",
    "    negative_image_embeds=negative_image_embeds, \n",
    "    height=768, \n",
    "    width=768, \n",
    "    strength=0.35,\n",
    "    generator=rand_gen\n",
    ").images[0]\n",
    "fig = make_image_grid([original_image.resize((512, 512)), new_image.resize((512, 512))], rows=1, cols=2)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee3d928",
   "metadata": {},
   "source": [
    "## Inpainting\n",
    "\n",
    "Diffusion models can also be used to do inpainting, which means filling regions of an image according to a prompt (or just according to the sourroundings of the hole to fill).\n",
    "\n",
    "Typically, we start from an image and a mask. The mask indicates the pixels to be inpainted, i.e., removed from the original image and filled with new content generated by the model.\n",
    "\n",
    "> **PLEASE RESTART NOW** to free GPU memory, then continue on from the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964de116",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DiffusionPipeline, AutoPipelineForText2Image\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6287a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import AutoPipelineForInpainting\n",
    "from diffusers.utils import load_image, make_image_grid\n",
    "\n",
    "pipeline = AutoPipelineForInpainting.from_pretrained(\n",
    "    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n",
    ")\n",
    "pipeline.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2130a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image = load_image(\"monalisa.png\").resize((512, 512))\n",
    "mask_image = load_image(\"monalisa_mask.png\").resize((512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaef833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "prompt = \"oil painting of a woman, sfumato, renaissance, low details, Da Vinci\"\n",
    "negative_prompt = \"bad anatomy, deformed, ugly, disfigured\"\n",
    "\n",
    "rand_gen = torch.manual_seed(74294536)\n",
    "image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image, generator=rand_gen, guidance_scale=1.5).images[0]\n",
    "fig = make_image_grid([init_image, mask_image, image], rows=1, cols=3)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1120010",
   "metadata": {},
   "source": [
    "## Beyond images\n",
    "\n",
    "Diffusion models can also be used for video generation. At the moment of the writing, this field is still in its infancy, but it is progressing fast so keep an eye on the available models as there might be much better ones by the time you are reading this.\n",
    "\n",
    "The list of available model for text-to-video is available [here](https://huggingface.co/models?pipeline_tag=text-to-video&library=diffusers&sort=trending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda2ef4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import get_video\n",
    "from IPython.display import Video\n",
    "\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n",
    "\n",
    "prompt = \"Earth sphere from space\"\n",
    "\n",
    "rand_gen = torch.manual_seed(42312981)\n",
    "frames = pipe(prompt, generator=rand_gen).frames\n",
    "\n",
    "Video(get_video(frames, \"earth.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9b5b51",
   "metadata": {},
   "source": [
    "We can also generate a video starting from an image. For example, let's consider the following image (which was generated with Stable Diffusion XL and then [outpainted](https://openai.com/blog/dall-e-introducing-outpainting) using DALLE-2):\n",
    "\n",
    "<div>\n",
    "<img src=\"in_the_desert_outpaint.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "> **PLEASE RESTART NOW** to free GPU memory, then continue on from the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b2887",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from helpers import get_video\n",
    "from IPython.display import Video\n",
    "\n",
    "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n",
    ").to(\"cuda\")\n",
    "# These two settings lower the VRAM usage\n",
    "pipe.enable_model_cpu_offload()\n",
    "# pipe.unet.enable_forward_chunking()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3651d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the conditioning image\n",
    "image = load_image(\"in_the_desert_outpaint.png\")\n",
    "image = image.resize((1024, 576))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d74a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.manual_seed(999)\n",
    "res = pipe(\n",
    "    image, \n",
    "    decode_chunk_size=2, \n",
    "    generator=generator, \n",
    "    num_inference_steps=15, \n",
    "    num_videos_per_prompt=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce084df",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(get_video(res.frames[0], \"horse2.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21166179",
   "metadata": {},
   "source": [
    "Overall the animation looks a bit off: the appearance of the legs can definitely be improved, although the overall motion seems correct. Let's try with a different object which has less parts:\n",
    "\n",
    "<div>\n",
    "<img src=\"xwing.jpeg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aa0448",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = load_image(\"xwing.jpeg\")\n",
    "image = image.resize((1024, 576))\n",
    "\n",
    "generator = torch.manual_seed(999)\n",
    "res = pipe(\n",
    "    image, \n",
    "    decode_chunk_size=2, \n",
    "    generator=generator, \n",
    "    num_inference_steps=25, \n",
    "    num_videos_per_prompt=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c241fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(get_video(res.frames[0], \"xwing.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dc91fc",
   "metadata": {},
   "source": [
    "The animation is definitely more realistic here, but still there's a lot to be desired. However, feel free to try other images and prompts and see what you get!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
