{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "199109c0",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network: training\n",
    "\n",
    "In this exercise we will practice how to train a GAN on a real dataset and generate our first synthetic images. Let's get started!\n",
    "\n",
    "First let's define some parameters for our GAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863edbfe",
   "metadata": {},
   "source": [
    "Generative Adversarial Networks (GANs) represent a significant leap in the field of artificial intelligence, particularly in image generation. At its core, a GAN consists of two main components: the Generator and the Discriminator.\n",
    "\n",
    "1. The Generator: Crafting Synthetic Images\n",
    "\n",
    "The Generator's role is to create images. It starts with a random noise vector, often sampled from a high-dimensional distribution. This vector, known as latent z, is then passed through the Generator network, which uses strided convolutions to convert this latent representation into a synthetic image. For example, a latent vector of 100 elements might be transformed into a 64x64 pixel image, which translates to 4096 numbers.\n",
    "\n",
    "2. The Discriminator: The Arbiter of Realism\n",
    "\n",
    "The Discriminator's job is to distinguish between real and generated (fake) images. It performs a binary classification to determine the authenticity of each image. In classical GANs, the Discriminator is typically a standard Convolutional Neural Network (CNN) used for image classification.\n",
    "\n",
    "3. Training the GAN: A Dance Between Generator and Discriminator\n",
    "\n",
    "Training a GAN involves an iterative process where the Generator and Discriminator continuously improve through competition. Initially, the Generator creates images, and the Discriminator learns to distinguish them from real ones. As the Generator improves, it becomes better at fooling the Discriminator. The training process is a cycle of alternating between training the Discriminator and the Generator, each time making them more adept at their tasks.\n",
    "\n",
    "In summary, GANs harness the power of two neural networks in a unique setup, where one creates and the other critiques, leading to the generation of increasingly realistic images. This dynamic interplay between the Generator and Discriminator underpins the success of GANs in creating convincing and high-quality synthetic images.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fb732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "CONFIG = {\n",
    "    # For repeatibility we will fix the random seed\n",
    "    \"manual_seed\": 42,\n",
    "    # This defines a set of augmentations we will perform, see below\n",
    "    \"policy\": \"color,translation\",  # ,cutout\n",
    "    # Dimension of the latent space\n",
    "    \"latent_dimension\": 256,\n",
    "    # Batch size for training\n",
    "    \"batch_size\": 256,\n",
    "    # Number of epochs. We will use 1200 epochs which corresponds to\n",
    "    # approximately 20 min of training\n",
    "    \"n_epochs\": 40,\n",
    "    # Input images will be resized to this, the generator will generate\n",
    "    # images with this size\n",
    "    \"image_size\": 64,  # 64x64 pixels\n",
    "    # Number of channels in the input images\n",
    "    \"num_channels\": 3,  # RGB\n",
    "    # Learning rate\n",
    "    \"lr\": 0.002,\n",
    "    # Momentum for Adam: in GANs you want to use a lower momentum to\n",
    "    # allow the Generator and the Discriminator to interact quicker\n",
    "    \"beta1\": 0.7,\n",
    "    # Number of feature maps in each layer of the Generator\n",
    "    \"g_feat_map_size\": 64,\n",
    "    # Number of feature maps in each layer of the Discriminator\n",
    "    \"d_feat_map_size\": 64,\n",
    "    # Where to save the data\n",
    "    \"data_path\": \"data/\",\n",
    "    # Number of workers to use to load the data\n",
    "    \"workers\": multiprocessing.cpu_count(),\n",
    "    # We will display progress every \"save_iter\" epochs\n",
    "    \"save_iter\": 10,\n",
    "    # Where to save the progress\n",
    "    \"outdir\": \"data/stanford_cars/\",\n",
    "    # Unused\n",
    "    \"clip_value\": 0.01,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac95bc86",
   "metadata": {},
   "source": [
    "In order to make the training repeatible, let's fix the random seed and set pytorch to use deterministic algorithms. This is normally not necessary, although it might not be a bad idea to keep your experimentation ordered. Keep in mind that the initial random seed can have quite an impact on the training of the GAN.\n",
    "\n",
    "One thing to consider is that deterministic algorithms can be significantly slower than non-deterministic ones, so we pay a performance penalty for setting things this way. In a real training scenario you might want to reconsider this tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0acc570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "random.seed(CONFIG[\"manual_seed\"])\n",
    "torch.manual_seed(CONFIG[\"manual_seed\"])\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee88674",
   "metadata": {},
   "source": [
    "Let's import a few other modules, methods and functions that we will need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9450e843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.utils.spectral_norm as spectral_norm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "from torchvision.utils import make_grid\n",
    "import os\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from ema_pytorch import EMA\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "# Create the output directory\n",
    "output_dir = Path(CONFIG[\"outdir\"])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the configuration there for safekeeping\n",
    "with open(output_dir / \"config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "# Make sure CUDA is available (i.e. the GPU is setup correctly)\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574e43ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de23e790",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a911eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    \"\"\"Custom weight initialization.\"\"\"\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def get_positive_labels(size, device, smoothing=True, random_flip=0.05):\n",
    "    if smoothing:\n",
    "        # Random positive numbers between 0.8 and 1.2 (label smoothing)\n",
    "        labels = 0.8 + 0.4 * torch.rand(size, device=device)\n",
    "    else:\n",
    "        labels = torch.full((size,), 1.0, device=device)\n",
    "\n",
    "    if random_flip > 0:\n",
    "        # Let's flip some of the labels to make it slightly harder for the discriminator\n",
    "        num_to_flip = int(random_flip * labels.size(0))\n",
    "\n",
    "        # Get random indices and set the first \"num_to_flip\" of them to 0\n",
    "        indices = torch.randperm(labels.size(0))[:num_to_flip]\n",
    "        labels[indices] = 0\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_negative_labels(size, device):\n",
    "    return torch.full((size,), 0.0, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7aeb73",
   "metadata": {},
   "source": [
    "### Diff augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiffAugment(x, policy=\"\", channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(\",\"):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (\n",
    "        torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2\n",
    "    ) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (\n",
    "        torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5\n",
    "    ) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(\n",
    "        -shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device\n",
    "    )\n",
    "    translation_y = torch.randint(\n",
    "        -shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device\n",
    "    )\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = (\n",
    "        x_pad.permute(0, 2, 3, 1)\n",
    "        .contiguous()[grid_batch, grid_x, grid_y]\n",
    "        .permute(0, 3, 1, 2)\n",
    "        .contiguous()\n",
    "    )\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(\n",
    "        0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device\n",
    "    )\n",
    "    offset_y = torch.randint(\n",
    "        0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device\n",
    "    )\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(\n",
    "        grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1\n",
    "    )\n",
    "    grid_y = torch.clamp(\n",
    "        grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1\n",
    "    )\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    \"color\": [rand_brightness, rand_saturation, rand_contrast],\n",
    "    \"translation\": [rand_translation],\n",
    "    \"cutout\": [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84741d",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56525fc",
   "metadata": {},
   "source": [
    "* The original Kaggle dataset: https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset?datasetId=30084&sortBy=dateCreated&select=cars_test\n",
    "* The devkit: car_devkit.tgz\n",
    "* The cars_test_annos_withlabels.mat file: https://www.kaggle.com/code/subhangaupadhaya/pytorch-stanfordcars-classification/input?select=cars_test_annos_withlabels+%281%29.mat\n",
    "\n",
    "The directory structure you provided earlier works well once we add the missing file!\n",
    "\n",
    "```bash\n",
    "└── stanford_cars\n",
    "    └── cars_test_annos_withlabels.mat\n",
    "    └── cars_train\n",
    "        └── *.jpg\n",
    "    └── cars_test\n",
    "        └── .*jpg\n",
    "    └── devkit\n",
    "        ├── cars_meta.mat\n",
    "        ├── cars_test_annos.mat\n",
    "        ├── cars_train_annos.mat\n",
    "        ├── eval_train.m\n",
    "        ├── README.txt\n",
    "        └── train_perfect_preds.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b64bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    root_path,\n",
    "    image_size,\n",
    "    batch_size,\n",
    "    workers=multiprocessing.cpu_count(),\n",
    "    donwload=False,\n",
    "):\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(image_size),\n",
    "            transforms.CenterCrop((image_size, image_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    dataset_train = datasets.StanfordCars(\n",
    "        root=root_path, download=donwload, split=\"train\", transform=transform\n",
    "    )\n",
    "\n",
    "    dataset_test = datasets.StanfordCars(\n",
    "        root=root_path, download=donwload, split=\"test\", transform=transform\n",
    "    )\n",
    "\n",
    "    dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_test])\n",
    "\n",
    "    print(f\"Using {workers} workers\")\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=workers,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True if workers > 0 else False,\n",
    "        #         collate_fn=collate_fn\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec02ed",
   "metadata": {},
   "source": [
    "### Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20bd164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the output tensor as a grayscale image\n",
    "def visualize_batch(batch):\n",
    "    b = batch.detach().cpu()\n",
    "    fig, sub = plt.subplots(dpi=150)\n",
    "    sub.imshow(np.transpose(make_grid(b, padding=0, normalize=True).cpu(), (1, 2, 0)))\n",
    "    _ = sub.axis(\"off\")\n",
    "\n",
    "\n",
    "def training_tracking(D_losses, G_losses, D_acc, fake_data):\n",
    "    fig = plt.figure(dpi=150)\n",
    "\n",
    "    gs = gridspec.GridSpec(2, 8)\n",
    "\n",
    "    # Create subplots\n",
    "    ax_a = fig.add_subplot(gs[0, :3])  # Top-left subplot\n",
    "    ax_b = fig.add_subplot(gs[1, :3])  # Bottom-left subplot\n",
    "    ax_c = fig.add_subplot(gs[:, 4:])  # Right subplot spanning both rows\n",
    "\n",
    "    subs = [ax_a, ax_b, ax_c]\n",
    "\n",
    "    # Losses\n",
    "    subs[0].plot(D_losses, label=\"Discriminator\")\n",
    "    subs[0].plot(G_losses, label=\"Generator\")\n",
    "    subs[0].legend()\n",
    "    subs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "    # Accuracy\n",
    "    subs[1].plot(D_acc)\n",
    "    subs[1].set_ylabel(\"D accuracy\")\n",
    "\n",
    "    # Examples of generated images\n",
    "    subs[2].imshow(\n",
    "        np.transpose(\n",
    "            make_grid(\n",
    "                fake_data.detach().cpu(), padding=0, normalize=True, nrow=4\n",
    "            ).cpu(),\n",
    "            (1, 2, 0),\n",
    "        )\n",
    "    )\n",
    "    subs[2].axis(\"off\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f50f2e0",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c72c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator class for DCGAN.\n",
    "    \n",
    "    The latent is passed through the generator network that ouptupts a synthetic image.\n",
    "\n",
    "    :param image_size: size of the input image (assumed to be square). Must be a power of 2\n",
    "    :param latent_dimension: dimension of the latent space\n",
    "    :param feat_map_size: number of feature maps in the last layer of the generator\n",
    "    :param num_channels: number of channels in the input image\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, latent_dimension, feat_map_size, num_channels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # The following defines the architecture in a way that automatically\n",
    "        # scales the number of blocks depending on the size of the input image\n",
    "\n",
    "        # Number of blocks between the first and the last (excluded)\n",
    "        n_blocks = int(np.log2(image_size)) - 3\n",
    "\n",
    "        # Initial multiplicative factor for the number of feature maps\n",
    "        factor = 2 ** (n_blocks)\n",
    "\n",
    "        # The first block takes us from the latent space to the feature space with a\n",
    "        # 4x4 kernel with stride 1 and no padding\n",
    "        blocks = [\n",
    "            self._get_transpconv_block(\n",
    "                latent_dimension, feat_map_size * factor, 4, 1, 0, nn.LeakyReLU(0.2)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        # The following blocks are transposed convolutional layers with stride 2 and\n",
    "        # kernel size 4x4. Every block halves the number of feature maps but double the\n",
    "        # size of the image (upsampling)\n",
    "        # (NOTE that we loop in reverse order)\n",
    "        prev_dim = feat_map_size * factor\n",
    "        for f in range(int(np.log2(factor) - 1), -1, -1):\n",
    "            blocks.append(\n",
    "                self._get_transpconv_block(\n",
    "                    prev_dim, feat_map_size * 2**f, 4, 2, 1, nn.LeakyReLU(0.2)\n",
    "                )\n",
    "            )\n",
    "            prev_dim = feat_map_size * 2**f\n",
    "\n",
    "        # Add last layer\n",
    "        blocks.append(\n",
    "            self._get_transpconv_block(\n",
    "                feat_map_size, num_channels, 4, 2, 1, nn.Tanh(), batch_norm=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*blocks)\n",
    "\n",
    "    def _get_transpconv_block(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        activation,\n",
    "        batch_norm=True,\n",
    "    ):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=kernel_size,\n",
    "                stride=stride,\n",
    "                padding=padding,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        return self.model(latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd8e4b6",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d443ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Discriminator class for DCGAN.\n",
    "    \n",
    "    The Discriminator network tries to divide fake from real images.\n",
    "    \n",
    "    :param image_size: size of the input image (assumed to be square). Must be a power of 2\n",
    "    :param feat_map_size: number of feature maps in the first layer of the discriminator\n",
    "    :param num_channels: number of channels in the input image\n",
    "    :param dropout: dropout probability\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, feat_map_size, num_channels, dropout=0):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        prev_dim = num_channels\n",
    "        for i in range(int(np.log2(image_size)) - 2):\n",
    "            blocks.append(\n",
    "                self._get_conv_block(\n",
    "                    in_channels=prev_dim,\n",
    "                    out_channels=feat_map_size * (2**i),\n",
    "                    kernel_size=4,\n",
    "                    stride=2,\n",
    "                    padding=1,\n",
    "                    dropout=dropout,\n",
    "                    activation=nn.LeakyReLU(0.2, inplace=True),\n",
    "                    batch_norm=False if i == 0 else True,\n",
    "                )\n",
    "            )\n",
    "            prev_dim = feat_map_size * (2**i)\n",
    "\n",
    "        blocks.append(\n",
    "            self._get_conv_block(\n",
    "                in_channels=prev_dim,\n",
    "                out_channels=1,\n",
    "                kernel_size=4,\n",
    "                stride=1,\n",
    "                padding=0,\n",
    "                dropout=0,\n",
    "                activation=nn.Sigmoid(),\n",
    "                batch_norm=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(*blocks)\n",
    "\n",
    "    def _get_conv_block(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        kernel_size,\n",
    "        stride,\n",
    "        padding,\n",
    "        dropout,\n",
    "        activation,\n",
    "        batch_norm=True,\n",
    "    ):\n",
    "        return nn.Sequential(\n",
    "            spectral_norm(\n",
    "                nn.Conv2d(\n",
    "                    in_channels,\n",
    "                    out_channels,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    bias=not batch_norm,\n",
    "                )\n",
    "            ),\n",
    "            nn.Dropout(p=dropout) if dropout > 0 else nn.Identity(),\n",
    "            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n",
    "            activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.model(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b6958",
   "metadata": {},
   "source": [
    "## Input dataset: real data\n",
    "\n",
    "In order to train a GAN we need to show it real data of the type we want to generate. In this case we are going to focus on the Stanford Cars dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b121efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data loader\n",
    "dataloader = get_dataloader(\n",
    "    CONFIG[\"data_path\"],\n",
    "    CONFIG[\"image_size\"],\n",
    "    CONFIG[\"batch_size\"],\n",
    "    CONFIG[\"workers\"],\n",
    "    donwload=False,\n",
    ")\n",
    "print(f\"Total number of examples: {len(dataloader.dataset)}\")\n",
    "visualize_batch(next(iter(dataloader))[0][:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592a379",
   "metadata": {},
   "source": [
    "These images represent cars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3e72b",
   "metadata": {},
   "source": [
    "## Generator\n",
    "\n",
    "Generator Training and Inference\n",
    "Generative Adversarial Networks (GANs) have revolutionized the field of AI-driven image generation. A crucial aspect of their success lies in the training of the Generator, which is responsible for creating realistic synthetic images.\n",
    "\n",
    "1. The Generator's Objective\n",
    "\n",
    "    The Generator in a GAN starts with a random noise vector, known as latent z, and transforms it into a synthetic image. The goal of the Generator is to create images so convincing that they can fool the Discriminator into believing they are real. This is accomplished by trying to maximize the loss of the Discriminator on the fake data.\n",
    "\n",
    "2. Training the Generator\n",
    "\n",
    "    The training process of the Generator involves several key steps:\n",
    "\n",
    "    Generating Fake Images: The Generator creates fake images from the latent z vector. We are actually going to reuse the fake images we have generated previously during the training of the Discriminator, but this is just an optimization.\n",
    "    Discriminator's Evaluation: These fake images are then passed through the Discriminator, which is kept frozen during this phase. The Discriminator evaluates these images and assigns a probability score to each, indicating how likely they are to be real.\n",
    "    Loss Calculation and Backpropagation: The Generator then adjusts its parameters to maximize the loss derived from the Discriminator’s evaluation. This loss reflects how well the Generator is fooling the Discriminator.\n",
    "\n",
    "3. Binary Cross Entropy Trick\n",
    "\n",
    "    It can be shown mathematically that maximizing the Binary Cross Entropy loss of the Discriminator on fake data (with label y=0) is equivalent to minimizing the same loss assigning y=1 instead of y=0.\n",
    "\n",
    "    Let's create the Generator network and look at its architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ba1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models and optimizers\n",
    "G = Generator(\n",
    "    CONFIG[\"image_size\"],\n",
    "    CONFIG[\"latent_dimension\"],\n",
    "    CONFIG[\"g_feat_map_size\"],\n",
    "    CONFIG[\"num_channels\"],\n",
    ").to(device)\n",
    "\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ae4a",
   "metadata": {},
   "source": [
    "> \n",
    "\n",
    "Let's create a latent vector and put it through the Generator. What shape would you expect?\n",
    "\n",
    "> **Complete the code marked by the YOUR CODE HERE placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd5a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a latent vector of shape (1, CONFIG['latent_dimension'], 1, 1)\n",
    "# Remember that the latent vector is just a vector of noise taken from a \n",
    "# Normal distribution\n",
    "# HINT: you can use torch.randn to sample from a Normal distribution\n",
    "latent = torch.randn(1, CONFIG['latent_dimension'], 1, 1)\n",
    "\n",
    "latent = latent.to(device)\n",
    "fake_img = G(latent)\n",
    "print(fake_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0437c2c9",
   "metadata": {},
   "source": [
    "Let's look at what the Generator is producing right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d95a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_batch(G(latent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dda911",
   "metadata": {},
   "source": [
    "This is of course just noise, because the Generator has not been trained yet. Now let's look at the shape of the tensor as it flows through the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63114fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = latent\n",
    "for i in range(5):\n",
    "    x = G.model[i](x.cuda())\n",
    "    b, c, w, h = x.shape\n",
    "    \n",
    "    print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d556f4",
   "metadata": {},
   "source": [
    "We can see that the input latent is mapped to 512 feature maps of size 4x4 pixels. After the first convolution, we have 256 feature maps of size 8x8 pixels, and so on, until we get to 3 output channels and a size of 64x64 pixels, which is the expected size for our fake image (matching the size of the input dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f1f8b7",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "Now let's have a look at the Discriminator:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e12c6a",
   "metadata": {},
   "source": [
    "The discrimnator take the input of a picture and classify the image.\n",
    "\n",
    "\n",
    "\n",
    "1. The Role of the Discriminator\n",
    "\n",
    "The Discriminator's task in a GAN is to distinguish between real and generated images. During training, this component learns to identify nuances that differentiate authentic images from those created by the Generator.\n",
    "\n",
    "2. Training Process: The Split-Batch Method\n",
    "\n",
    "A popular method for training the Discriminator is the 'split-batch' technique. This involves two main steps:\n",
    "\n",
    "* Step 1: Handling Real Images\n",
    "    The Discriminator is fed real images and learns to identify them as authentic.\n",
    "    This process involves a forward pass of real data through the Discriminator, generating a probability score for each image being real.\n",
    "    The Binary Cross Entropy (BCE) loss is then calculated by comparing the Discriminator's predictions against the true labels (real images).\n",
    "* Step 2: Dealing with Fake Images\n",
    "    Next, the Discriminator is presented with fake images produced by the Generator.\n",
    "    These images undergo a similar process, with the Discriminator learning to label them as fake.\n",
    "    The BCE loss is again used to compare the Discriminator's predictions against the true labels (fake images).\n",
    "* 3. Updating the Discriminator\n",
    "\n",
    "After processing both real and fake images, the Discriminator's weights are updated. This is done using the gradients accumulated from both sets of data, ensuring that the Discriminator improves its ability to differentiate real from fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca840c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = (\n",
    "    Discriminator(\n",
    "        CONFIG[\"image_size\"], CONFIG[\"d_feat_map_size\"], CONFIG[\"num_channels\"], dropout=0.1\n",
    "    )\n",
    "    .to(device)#.eval()\n",
    ")\n",
    "\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f642e6",
   "metadata": {},
   "source": [
    "the Discriminator is composed of 5 blocks (from 0 to 4), represented by the Sequential modules. This is a standard classification CNN for Binary classification but it does not use any pooling layer. Instead, all convolutional layers are using a stride of 2 so the feature maps become smaller and smaller at every iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f7022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = fake_img\n",
    "for i in range(5):\n",
    "    x = D.model[i](x)\n",
    "    b, c, w, h = x.shape\n",
    "    \n",
    "    print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d8d788",
   "metadata": {},
   "source": [
    "## Loss and optimizers\n",
    "\n",
    "Like in any other task involving the training of neural networks, we need to setup the loss function we want to minimize and the optimizer.\n",
    "\n",
    "In the case of GANs, we have two optimizers: the optimizer for the Generator, and the optimizer for the Discriminator:\n",
    "\n",
    "> **Complete the code marked by the YOUR CODE HERE placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f299d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code using the appropriate loss for the\n",
    "# binary classification task of the Discriminator\n",
    "# HINT: some possible losses available in pytorch are:\n",
    "# nn.MSELoss()\n",
    "# nn.BCELoss()\n",
    "# nn.CrossEntropyLoss()\n",
    "# nn.NLLoss()\n",
    "# Pick the one appropriate for binary classification\n",
    "criterion = nn.BCELoss()  # YOUR CODE HERE\n",
    "\n",
    "# Optimizer for the Generator\n",
    "# Instance the optimizer for the Generator\n",
    "# HINT: the first parameter of optim.Adam()\n",
    "# should be a list of parameters to optimize.\n",
    "# Given a network N, you can obtain its parameters\n",
    "# just by doing N.parameters(). Now do the same for the\n",
    "# Generator\n",
    "optimizerG = optim.Adam(\n",
    "    G.parameters(),  # YOUR CODE HERE,\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    betas=(CONFIG[\"beta1\"], 0.999),\n",
    ")\n",
    "\n",
    "# Optimizer for the Discriminator\n",
    "# Do the same thing you did for the Generator, but this time\n",
    "# for the Discriminator (i.e., complete the initialization\n",
    "# of the Adam optimizer with the parameters of D)\n",
    "optimizerD = optim.Adam(\n",
    "    D.parameters(),  # YOUR CODE HERE,\n",
    "    lr=CONFIG[\"lr\"] / 4,\n",
    "    betas=(CONFIG[\"beta1\"], 0.999),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d44bc8b",
   "metadata": {},
   "source": [
    "### Trick 1: Exponential Moving Average\n",
    "\n",
    "GANs are notoriously difficult to train as the balance between the Generator and the Discriminator is easy to break. There are many tricks that can be used to stabilize that, and we're going to apply some here.\n",
    "\n",
    "The first trick is the [Exponential Moving Average](https://arxiv.org/abs/1806.04498): while the Generator is training, we keep a moving average of its weights. At the end we use this smoothed version of the model to generate inference. This model jumps around less and it is less sensitive to sudden changes.\n",
    "\n",
    "The EMA class accepts a parameter called beta, which controls the size of the window used for averaging. The number of steps (i.e. batches) we are going to average over is approximately equal to 1 / (1 - beta). Since there are 20 batches in our dataloader, if we use beta=0.995 we are averaging over 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1e27e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ema_G = EMA(\n",
    "    G,\n",
    "    beta = 0.995,              # average over the last ~10 epochs\n",
    "    update_after_step = 100,    # start averaging after the first 5 epochs\n",
    "    update_every = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d691ffdb",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "### Trick 2: Gaussian initialization\n",
    "\n",
    "The original DCGAN paper suggests to initialize the weights of both G and D with a Gaussian distribution (instead of the default pytorch initialization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720dcd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights\n",
    "_ = G.apply(initialize_weights)\n",
    "_ = D.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d775ce",
   "metadata": {},
   "source": [
    "ok, we're now ready to start training! We will use the \"split-batch\" technique we have seen in the lesson, where we compute separately the gradients for the Discriminator, first on a batch of real images and then on a batch of fake images. Then we accumulate the gradients and perform one backward pass. For the Generator, we adopt the trick of maximizing log(D(G(z))) instead of minimizing log(1−D(G(z))). This is accomplished by setting the labels for the fake images generated by the Generator to 1 (\"real\") instead of 0 (\"fake\"), as we have seen in the lesson.\n",
    "\n",
    "But first let's look at some more tricks we're using in the training loop.\n",
    "\n",
    "### Trick 3: Label smoothing\n",
    "\n",
    "Label smoothing is a general technique originally proposed in [this paper](https://arxiv.org/abs/1512.00567) and described in detail [here](https://arxiv.org/abs/1906.02629). It consists of substituting the probability for the target class from 1 (``hard labels``) to something lower than 1. In case of Binary Classification, the BCELoss gets as input the probability for the positive class, so Label Smoothing becomes as simple as substituting 1 with a random number between 0.8 and 1.2. Label smoothing promotes less overconfidence in the Discriminator and slow down its convergence, especially at the beginning when the Generator is still pretty bad at generating realistic images.\n",
    "\n",
    "### Trick 4: Random flipping\n",
    "In order to make the work of the Discriminator a little harder and prevent it to immediately overwhelm the Generator, it is suggested to add some random noise in the labels for the Discriminator. This is equivalent to flipping some labels from positive to negative. This effectively prevents the Discriminator to ever achieve zero loss.\n",
    "\n",
    "### Trick 5: DiffAugment\n",
    "In [this](https://arxiv.org/abs/2006.10738) paper the authors introduce a simple set of augmentations to be applied on both the real and the fake images that prevent overfitting in the Discriminator. Since here we only have 5000 examples of real images, overfitting is very easy and this technique will prove very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(16, CONFIG['latent_dimension'], 1, 1, device=device)\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_acc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414b1d4f",
   "metadata": {},
   "source": [
    "> **Complete the code marked by the YOUR CODE HERE placeholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7445fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "\n",
    "tstart = time.time()\n",
    "n_frame = 0\n",
    "\n",
    "for epoch in range(CONFIG[\"n_epochs\"]):\n",
    "    # Keep track of losses and accuracy of the Discriminator\n",
    "    # for each batch, we will average them at the end of the\n",
    "    # epoch\n",
    "    batch_G_losses = []\n",
    "    batch_D_losses = []\n",
    "    batch_D_acc = []\n",
    "\n",
    "    # Loop over batch of real data (we throw away the labels\n",
    "    # provided by the dataloader by saving them into `_`)\n",
    "    for data, _ in tqdm.tqdm(dataloader, total=len(dataloader)):\n",
    "        # Move batch to GPU and record its size\n",
    "        # (remember that the last batch could be smaller than batch_size)\n",
    "        data = data.to(device)\n",
    "        b_size = data.size(0)\n",
    "\n",
    "        # This function implements tricks 3 and 4 (smoothing and random label flipping)\n",
    "        labels = get_positive_labels(b_size, device, smoothing=True, random_flip=0.2)\n",
    "        print(labels)\n",
    "\n",
    "        ################################################\n",
    "        # Discriminator training                       #\n",
    "        ################################################\n",
    "\n",
    "        # The generator is frozen, gradients flow only\n",
    "        D.zero_grad()  # Resets the gradients of all optimized torch\n",
    "\n",
    "        # Forward pass real batch through D using DiffAugment\n",
    "        # augmentation\n",
    "        D_pred = D(DiffAugment(data, policy=CONFIG[\"policy\"])).view(\n",
    "            -1\n",
    "        )  # probability to be real\n",
    "\n",
    "        # Measure accuracy for the positive batch\n",
    "        acc_pos = (D_pred > 0.5).sum() / D_pred.size(0)\n",
    "\n",
    "        # Loss on the real data\n",
    "\n",
    "        # Compute the loss on the real data by calling the\n",
    "        # criterion we have defined above\n",
    "        # >>>>>>>>>>>> YOUR CODE HERE\n",
    "        loss_on_real_data = criterion(D_pred, labels)\n",
    "\n",
    "        # Compute the gradients on the real data\n",
    "        # The improve the discriminator\n",
    "        # HINT: you can compute the gradients by calling\n",
    "        # .backward() on the loss\n",
    "        loss_on_real_data.backward()  # YOUR CODE HERE\n",
    "        # No .step () to update weights, we need more work\n",
    "\n",
    "        # Now pass a batch of fake data through the model\n",
    "        # Generate batch of latent vectors\n",
    "        # HINT: generate a latent using torch.randn, the shape of the\n",
    "        # latent should be (b_size, CONFIG['latent_dimension'], 1, 1)\n",
    "        # NOTE: add the device=device option as in torch.randn(..., device=device)\n",
    "        # so the latent is created on the GPU (otherwise you'll get an error later)\n",
    "        latent_vectors = torch.randn(\n",
    "            b_size, CONFIG[\"latent_dimension\"], 1, 1, device=device\n",
    "        )  # YOUR CODE HERE\n",
    "\n",
    "        # Generate fake image batch with G\n",
    "        # HINT: just call the generator using the latent\n",
    "        fake_data = G(latent_vectors)  # YOUR CODE HERE\n",
    "\n",
    "        # Assign negative label as ground truth\n",
    "        # (ground truth labels)\n",
    "        labels.fill_(0)  # 0 is the label for fake images\n",
    "\n",
    "        # Get predictions from the Discriminator\n",
    "        # (applying DiffAugment augmentations)\n",
    "        # NOTE: here it is VERY important to use .detach() on the (augmented)\n",
    "        # fake data because we do NOT want the Generator to be part of the computation\n",
    "        # graph used to compute the gradients (we don't want to update the Generator yet)\n",
    "        D_pred = D(\n",
    "            DiffAugment(fake_data, policy=CONFIG[\"policy\"]).detach()  # VERY IMPORTANT\n",
    "        ).view(-1)\n",
    "        # we need to reuse this data later on in the bacakr of the generator\n",
    "        # , and without the .detach() it will be destroyed\n",
    "\n",
    "        # Get accuracy for this all-fake batch\n",
    "        acc_neg = (D_pred < 0.5).sum() / D_pred.size(0)\n",
    "\n",
    "        # Loss on fake data\n",
    "        # HINT: call the criterion defined above providing the\n",
    "        # discriminator prediction D_pred and the ground truth\n",
    "        # labels\n",
    "        loss_on_fake_data = criterion(D_pred, labels)  # YOUR CODE HERE\n",
    "\n",
    "        # This computes the gradients after the fake data\n",
    "        # forward pass and stores them in the tensors\n",
    "        # (model parameters are NOT updated here)\n",
    "        # Remember that .backward() by default does NOT replace\n",
    "        # the gradients coming from the backward pass on the real data.\n",
    "        # Instead, it sums the new gradients with the old gradients\n",
    "        loss_on_fake_data.backward()\n",
    "\n",
    "        # Now we can finally update the Discriminator\n",
    "        # HINT: call a step on the optimizer of the Discriminator\n",
    "        # (optimizerD)\n",
    "        # >>> YOUR CODE HERE\n",
    "        optimizerD.step()\n",
    "        # This will use the gradient accumulated on both: the real and\n",
    "        # the fake data\n",
    "\n",
    "        # Compute error of D as sum over the fake and the real batches\n",
    "        # for safekeeping\n",
    "        total_loss = loss_on_real_data + loss_on_fake_data\n",
    "\n",
    "        ################################################\n",
    "        # Generator training                           #\n",
    "        ################################################\n",
    "        # Depending on how good the discriminator is at this stage\n",
    "        # it will give us a higher or lower classification loss\n",
    "        # We then take a backward step and update the parameters\n",
    "        # of the generator in order to decrease the loss obtained\n",
    "        # from the prediction of the discriminator\n",
    "        # The discriminartor is part of the competition graph for this step\n",
    "        # however its weight are frozen so it plays a passive part\n",
    "        # In this phase the weight of the generator are changed\n",
    "        # so that it will fool more and more the discriminator\n",
    "        # Explanaition of the generator objective can be found here:\n",
    "        # https://www.youtube.com/watch?v=wMF0sQO7sNw&t=204s\n",
    "        # the summary is that the objective of the generator is\n",
    "        # equivalent to minimizing the BCE loss of D(G(z)) when\n",
    "        # imposing the binary label equal to 1\n",
    "        # L = -y*log(D(G(z)))\n",
    "        G.zero_grad()  # Resets the gradients of all optimized torch\n",
    "\n",
    "        # Remember that BCELoss is −[y logx + (1−y)⋅log(1−x)]\n",
    "        labels.fill_(1)  # 1 is the label for \"real\".\n",
    "\n",
    "        # Since we just updated D, perform another forward pass of\n",
    "        # the all-fake batch we already generated as part of the previous\n",
    "        # part (with DiffAugment)\n",
    "        # NOTE how we are NOT using .detach now, as this time we want the\n",
    "        # gradients for this operation to be accumulated\n",
    "        D_pred = D(DiffAugment(fake_data, policy=CONFIG[\"policy\"])).view(-1)\n",
    "        # Probability of these images to be real according to the discriminator\n",
    "        # Because the weights of the discriminator has changed in the previous step\n",
    "        # minimizing the objective function of the GAN for the generator\n",
    "        # is equivalen to minimizing the BCE loss on the prediction of the discriminator\n",
    "        # when the labels are all positive.\n",
    "\n",
    "        # Loss from the Discriminator prediction that is going\n",
    "        # to be used to update G\n",
    "        # HINT: call the criterion on the prediction of the discriminator\n",
    "        # D_pred and the labels\n",
    "        loss_on_fake_G = criterion(D_pred, labels)  # YOUR CODE HERE\n",
    "\n",
    "        # Calculate gradients for G\n",
    "        # HINT: you did this before\n",
    "        loss_on_fake_G.backward()  # YOUR CODE HERE\n",
    "\n",
    "        # Update G\n",
    "        # HINT: call a step on the optimizer for the Generator\n",
    "        # (optimizer G)\n",
    "        # >>> YOUR CODE HERE\n",
    "        optimizerG.step()\n",
    "        # Update the Exponential Moving Average copy\n",
    "        ema_G.update()\n",
    "\n",
    "        # Save all losses\n",
    "        batch_G_losses.append(loss_on_fake_G.item())\n",
    "        batch_D_losses.append(total_loss.item())\n",
    "        batch_D_acc.append((0.5 * (acc_pos + acc_neg)).item())\n",
    "\n",
    "    # Take the mean over the epoch\n",
    "    G_losses.append(np.mean(batch_G_losses))\n",
    "    D_losses.append(np.mean(batch_D_losses))\n",
    "    D_acc.append(np.mean(batch_D_acc))\n",
    "\n",
    "    if epoch % CONFIG[\"save_iter\"] == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_viz_data = G(fixed_noise).detach().cpu()\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "        fig = training_tracking(D_losses, G_losses, D_acc, fake_viz_data)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        fig.savefig(f\"{CONFIG['outdir']}/frame_{n_frame:05d}.png\")\n",
    "        n_frame += 1\n",
    "\n",
    "print(f\"Finished in {(time.time() - tstart)/60:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_batch(ema_G(fixed_noise).detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb047d",
   "metadata": {},
   "source": [
    "Cars start to really appear, although we would probably need quite a bit more training (and parameter tuning) to make it really work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155bec41",
   "metadata": {},
   "source": [
    "1. Challenges in Training GANs\n",
    "\n",
    "    * Unstable Balance: Training GANs is delicate; if either the Generator (G) or Discriminator (D) becomes too proficient too quickly, the other lags, disrupting the training process.\n",
    "\n",
    "    * No Clear Convergence Indicator: Unlike traditional neural networks, GANs lack a clear metric like validation loss to signify convergence, making it hard to determine the optimal stopping point.\n",
    "    * Mode Collapse: A critical issue where the Generator discovers a specific image type that always fools the Discriminator, leading to a lack of diversity in generated images.\n",
    "2. Advanced Variants of GANs\n",
    "\n",
    "To address these challenges, several GAN variants have been developed:\n",
    "\n",
    "* [Wasserstein GAN (W-GAN)](https://arxiv.org/abs/1701.07875): Introduces a Critic instead of a Discriminator, which assigns continuous scores to images, enhancing training dynamics and reducing mode collapse.\n",
    "\n",
    "* [Progressive GANs](https://arxiv.org/abs/1710.10196): These GANs begin by generating low-resolution images, progressively adding details. This approach aids in faster convergence and enables the creation of high-resolution images.\n",
    "\n",
    "* Style GANs ([v1](https://arxiv.org/abs/1812.04948), [v2](https://arxiv.org/abs/1912.04958) and [v3](https://arxiv.org/abs/2106.12423)): Incorporate a mapping network to convert the latent vector into a style vector, which is fed along with the latent into the Generator. This, combined with added random noise and a few other innovations, significantly enhances sample quality and robustness.\n",
    "\n",
    "3. Conditional GANs\n",
    "\n",
    "A notable extension of GANs is the development of conditional GANs (see for example [here](https://arxiv.org/abs/1907.10786)). They allow for manipulation of specific attributes in the output images, such as changing the view angle, gender, or adding a smile."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234648b3",
   "metadata": {},
   "source": [
    "The Pros and Cons of Generative Adversarial Networks (GANs)\n",
    "Generative Adversarial Networks (GANs) have made significant strides in the field of AI-driven image generation. Understanding their strengths and weaknesses is key to leveraging their full potential.\n",
    "\n",
    "Pros of GANs:\n",
    "\n",
    "1. Speed: One of the standout features of GANs is their speed during inference. They require only a single forward pass of the latent vector, resulting in sub-second latency on modern GPUs. This makes them incredibly efficient for generating images quickly.\n",
    "\n",
    "2. High Sample Quality: GANs are renowned for their excellent sample quality. They consistently rank as state-of-the-art in terms of Fréchet Inception Distance (FID) across various datasets, even when compared to newer generation algorithms like diffusion models. The level of detail in synthetic faces and animals created by GANs is often astonishingly high.\n",
    "\n",
    "Cons of GANs:\n",
    "\n",
    "1. Poor Mode Coverage: A notable drawback of GANs is their tendency to have poor coverage. The Generator in a GAN often prefers exploitation over exploration. Once it finds a method to deceive the Discriminator or Critic, it tends to overuse this approach, leading to a lack of diversity in the generated images.\n",
    "\n",
    "2. Training Challenges: GANs are notorious for being tricky to train. They require a deep understanding and implementation of numerous training techniques to function efficiently and produce high-quality results.\n",
    "\n",
    "In summary, while GANs boast impressive speed and sample quality, they face challenges in terms of mode coverage and training complexity. These factors must be considered when deploying GANs for practical applications in image generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
