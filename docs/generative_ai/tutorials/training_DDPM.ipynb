{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49229ab1",
   "metadata": {},
   "source": [
    "# Train a Denoising Diffusion Probabilistic Model from scratch\n",
    "\n",
    "Welcome! In this exercise you will train a DDPM from scratch. After training the model will be able to generate images of cars.\n",
    "\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "## Initial setup\n",
    "\n",
    "Here we import a few modules and we set up the notebook so it is fully reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f90af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make results fully reproducible:\n",
    "import os\n",
    "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Import a few things we will need\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.optim import Adam, RAdam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "\n",
    "from torchvision import transforms \n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c37a227",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's start by loading our training dataset. We are going to use the Stanford Cars dataset. It consists of 196 classes of cars with a total of 16,185 images. For this exercise we do not need any label, and we also do not need a test dataset, so we are going to load both the training and the test dataset and concatenate them. We are also going to transform the images to 64x64 so the exercise can complete more quickly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6012fae3",
   "metadata": {},
   "source": [
    "* The original Kaggle dataset: https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset?datasetId=30084&sortBy=dateCreated&select=cars_test\n",
    "* The devkit: car_devkit.tgz\n",
    "* The cars_test_annos_withlabels.mat file: https://www.kaggle.com/code/subhangaupadhaya/pytorch-stanfordcars-classification/input?select=cars_test_annos_withlabels+%281%29.mat\n",
    "\n",
    "The directory structure you provided earlier works well once we add the missing file!\n",
    "\n",
    "```bash\n",
    "└── stanford_cars\n",
    "    └── cars_test_annos_withlabels.mat\n",
    "    └── cars_train\n",
    "        └── *.jpg\n",
    "    └── cars_test\n",
    "        └── .*jpg\n",
    "    └── devkit\n",
    "        ├── cars_meta.mat\n",
    "        ├── cars_test_annos.mat\n",
    "        ├── cars_train_annos.mat\n",
    "        ├── eval_train.m\n",
    "        ├── README.txt\n",
    "        └── train_perfect_preds.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af537c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 50 # reduce depends of yout gpu - initial was 100\n",
    "\n",
    "def get_dataset(path):\n",
    "    data_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            # We flip horizontally with probability 50%\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            # Scales data into [-1,1] \n",
    "            transforms.Normalize(0.5, 0.5)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    train = torchvision.datasets.StanfordCars(root=path, download=False, \n",
    "                                         transform=data_transform)\n",
    "\n",
    "    test = torchvision.datasets.StanfordCars(root=path, download=False, \n",
    "                                         transform=data_transform, split='test')\n",
    "    \n",
    "    return torch.utils.data.ConcatDataset([train, test])\n",
    "\n",
    "data = get_dataset(\"data/\")\n",
    "dataloader = DataLoader(\n",
    "    data, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    drop_last=False, \n",
    "    pin_memory=True, \n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e642a7",
   "metadata": {},
   "source": [
    "Let's look at a batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b078191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch\n",
    "batch, _ = next(iter(dataloader))\n",
    "\n",
    "# Display it\n",
    "def display_sequence(imgs, dpi=75, nrow=8):\n",
    "    \n",
    "    fig, sub = plt.subplots(dpi=dpi)\n",
    "    sub.imshow(\n",
    "        np.transpose(\n",
    "            make_grid(\n",
    "                imgs, \n",
    "                padding=0,\n",
    "                normalize=True,\n",
    "                nrow=nrow,\n",
    "            ).cpu(),\n",
    "            (1,2,0)\n",
    "        )\n",
    "    )\n",
    "    _ = sub.axis(\"off\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "_ = display_sequence(batch[:8], dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeeb1b0",
   "metadata": {},
   "source": [
    "## Noise scheduling and precomputation\n",
    "\n",
    "In the forward process we need to add random noise according to a schedule. Here we use a linear schedule with 512 diffusion steps.\n",
    "\n",
    "Let's define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19406f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define beta schedule\n",
    "T = 512  # number of diffusion steps\n",
    "# YOUR CODE HERE\n",
    "betas = torch.linspace(start=0.0001, end=0.02, steps=T)  # linear schedule\n",
    "\n",
    "plt.plot(range(T), betas.numpy(), label='Beta Values')\n",
    "plt.xlabel('Diffusion Step')\n",
    "plt.ylabel('Beta Value')\n",
    "_ = plt.title('Beta Schedule over Diffusion Steps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6141ad",
   "metadata": {},
   "source": [
    "As we have seen in the lesson, we need to use a re-parametrization of the forward process that allows us to generate noisy images at any step without having to sequentially go through all the previous steps:\n",
    "\n",
    "$$\n",
    "\\left\\{\n",
    "\\begin{align*}\n",
    "\\bar{\\alpha}_t &= \\prod_{s=1}^t (1 - \\beta_s) \\\\\n",
    "q(x_t | x_0) &= \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_t} x_0, \\ (1 - \\bar{\\alpha}_t) \\mathbf{I}\\right)\n",
    "\\end{align*}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "At inference time we will also need the quantities involved in these other formulas:\n",
    "\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, t) \\right) + \\sigma_t z\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma_t^2 = \\frac{(1-\\bar{\\alpha}_t-1)}{(1-\\bar{\\alpha}_t)} \\beta_t\n",
    "$$\n",
    "\n",
    "Here we define and precompute all these constants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce840e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-calculate different terms for closed form\n",
    "alphas = 1. - betas\n",
    "# alpha bar\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "# alpha bar at t-1\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "# sqrt of alpha bar\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "\n",
    "# Inference:\n",
    "# 1 / sqrt(alpha)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "# sqrt of one minus alpha bar\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
    "# sigma_t\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37273172",
   "metadata": {},
   "source": [
    "Here we define two utility functions, one to visualize the forward diffusion process, and the other one to make an inference call on an existing DDPM:\n",
    "\n",
    "> **Fill the sections marked with YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2073e8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure CUDA is available (i.e. the GPU is setup correctly)\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3de093",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def forward_diffusion_viz(image, device='cpu', num_images=16, dpi=75, interleave=False):\n",
    "    \"\"\"\n",
    "    Generate the forward sequence of noisy images taking the input image to pure noise\n",
    "    \"\"\"\n",
    "    # Visualize only num_images diffusion steps, instead of all of them\n",
    "    stepsize = int(T/num_images)\n",
    "    \n",
    "    imgs = []\n",
    "    noises = []\n",
    "    \n",
    "    for i in range(0, T, stepsize):\n",
    "        t = torch.full((1,), i, device=device, dtype=torch.long)\n",
    "\n",
    "        # Forward diffusion process\n",
    "        bs = image.shape[0]\n",
    "        noise = torch.randn_like(image, device=device)\n",
    "        img = (\n",
    "            sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * image + \n",
    "            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise\n",
    "        )\n",
    "\n",
    "        imgs.append(torch.clamp(img, -1, 1).squeeze(dim=0))\n",
    "        noises.append(torch.clamp(noise, -1, 1).squeeze(dim=0))\n",
    "    \n",
    "    if interleave:\n",
    "        imgs = [item for pair in zip(imgs, noises) for item in pair]\n",
    "        \n",
    "    fig = display_sequence(imgs, dpi=dpi)\n",
    "    \n",
    "    return fig, imgs[-1]\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def make_inference(input_noise, return_all=False):\n",
    "    \"\"\"\n",
    "    Implements the sampling algorithm from the DDPM paper\n",
    "    \"\"\"\n",
    "    \n",
    "    x = input_noise\n",
    "    bs = x.shape[0]\n",
    "    \n",
    "    imgs = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    for time_step in range(0, T)[::-1]:\n",
    "        \n",
    "        noise = torch.randn_like(x) if time_step > 0 else 0\n",
    "        \n",
    "        t = torch.full((bs,), time_step, device=device, dtype=torch.long)\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        x = sqrt_recip_alphas[t].view(bs, 1, 1, 1) * (\n",
    "            x - betas[t].view(bs, 1, 1, 1) * model(x, t) / \n",
    "            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1)\n",
    "        ) + torch.sqrt(posterior_variance[t].view(bs, 1, 1, 1)) * noise\n",
    "        \n",
    "        imgs.append(torch.clamp(x, -1, 1))\n",
    "    \n",
    "    if return_all:\n",
    "        return imgs\n",
    "    else:\n",
    "        return imgs[-1]\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb3c374",
   "metadata": {},
   "source": [
    "### Forward process\n",
    "\n",
    "Let's now simulate our forward process. If everything went well, you should see a few images like this one:\n",
    "\n",
    "\n",
    "which show a few of the diffusion steps, from the original image to the left all the way to pure noise to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f4d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in batch[:5]:\n",
    "    _ = forward_diffusion_viz(image.unsqueeze(dim=0), num_images=7, dpi=150, interleave=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8641f6",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Model definition\n",
    "\n",
    "Here we define the model we are going to train. We import a simple UNet model from the ``unet.py`` file (you can look into it if you like, but it is not required):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08daf4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: U-Net model for Denoising Diffusion Probabilistic Models (DDPM)\n",
    "summary: >\n",
    "  UNet model for Denoising Diffusion Probabilistic Models (DDPM)\n",
    "---\n",
    "\n",
    "# U-Net model for [Denoising Diffusion Probabilistic Models (DDPM)](index.html)\n",
    "\n",
    "This is a [U-Net](../../unet/index.html) based model to predict noise\n",
    "$\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$.\n",
    "\n",
    "U-Net is a gets it's name from the U shape in the model diagram.\n",
    "It processes a given image by progressively lowering (halving) the feature map resolution and then\n",
    "increasing the resolution.\n",
    "There are pass-through connection at each resolution.\n",
    "\n",
    "![U-Net diagram from paper](../../unet/unet.png)\n",
    "\n",
    "This implementation contains a bunch of modifications to original U-Net (residual blocks, multi-head attention)\n",
    " and also adds time-step embeddings $t$.\n",
    " \n",
    "The MIT License (MIT)\n",
    "\n",
    "Copyright (c) 2020 Varuna Jayasiri\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE.\n",
    "\n",
    "(taken from https://github.com/labmlai/annotated_deep_learning_paper_implementations)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Swish actiavation function\n",
    "\n",
    "    $$x \\cdot \\sigma(x)$$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Embeddings for $t$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of dimensions in the embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        # First linear layer\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        # Activation\n",
    "        self.act = Swish()\n",
    "        # Second linear layer\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        # Create sinusoidal position embeddings\n",
    "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
    "        #\n",
    "        # \\begin{align}\n",
    "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
    "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
    "        # \\end{align}\n",
    "        #\n",
    "        # where $d$ is `half_dim`\n",
    "        half_dim = self.n_channels // 8\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        emb = self.lin2(emb)\n",
    "\n",
    "        #\n",
    "        return emb\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Residual block\n",
    "\n",
    "    A residual block has two convolution layers with group normalization.\n",
    "    Each resolution is processed with two residual blocks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        time_channels: int,\n",
    "        n_groups: int = 32,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        * `in_channels` is the number of input channels\n",
    "        * `out_channels` is the number of input channels\n",
    "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        * `dropout` is the dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Group normalization and the first convolution layer\n",
    "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
    "        self.act1 = Swish()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        # Group normalization and the second convolution layer\n",
    "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
    "        self.act2 = Swish()\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        # If the number of input channels is not equal to the number of output channels we have to\n",
    "        # project the shortcut connection\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "        # Linear layer for time embeddings\n",
    "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
    "        self.time_act = Swish()\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # First convolution layer\n",
    "        h = self.conv1(self.act1(self.norm1(x)))\n",
    "        # Add time embeddings\n",
    "        h += self.time_emb(self.time_act(t))[:, :, None, None]\n",
    "        # Second convolution layer\n",
    "        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
    "\n",
    "        # Add the shortcut connection and return\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Attention block\n",
    "\n",
    "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32\n",
    "    ):\n",
    "        \"\"\"\n",
    "        * `n_channels` is the number of channels in the input\n",
    "        * `n_heads` is the number of heads in multi-head attention\n",
    "        * `d_k` is the number of dimensions in each head\n",
    "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Default `d_k`\n",
    "        if d_k is None:\n",
    "            d_k = n_channels\n",
    "        # Normalization layer\n",
    "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
    "        # Projections for query, key and values\n",
    "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
    "        # Linear layer for final transformation\n",
    "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
    "        # Scale for dot-product attention\n",
    "        self.scale = d_k**-0.5\n",
    "        #\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_k\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size, time_channels]`\n",
    "        \"\"\"\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        # Get shape\n",
    "        batch_size, n_channels, height, width = x.shape\n",
    "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
    "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
    "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
    "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
    "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
    "        attn = torch.einsum(\"bihd,bjhd->bijh\", q, k) * self.scale\n",
    "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
    "        attn = attn.softmax(dim=2)\n",
    "        # Multiply by values\n",
    "        res = torch.einsum(\"bijh,bjhd->bihd\", attn, v)\n",
    "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
    "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
    "        # Transform to `[batch_size, seq, n_channels]`\n",
    "        res = self.output(res)\n",
    "\n",
    "        # Add skip connection\n",
    "        res += x\n",
    "\n",
    "        # Change to shape `[batch_size, in_channels, height, width]`\n",
    "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
    "\n",
    "        #\n",
    "        return res\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Down block\n",
    "\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Up block\n",
    "\n",
    "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
    "        # from the first half of the U-Net\n",
    "        self.res = ResidualBlock(\n",
    "            in_channels + out_channels, out_channels, time_channels\n",
    "        )\n",
    "        if has_attn:\n",
    "            self.attn = AttentionBlock(out_channels)\n",
    "        else:\n",
    "            self.attn = nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res(x, t)\n",
    "        x = self.attn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MiddleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Middle block\n",
    "\n",
    "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
    "    This block is applied at the lowest resolution of the U-Net.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int, time_channels: int):\n",
    "        super().__init__()\n",
    "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "        self.attn = AttentionBlock(n_channels)\n",
    "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        x = self.res1(x, t)\n",
    "        x = self.attn(x)\n",
    "        x = self.res2(x, t)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale up the feature map by $2 \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
    "        # to match with `ResidualBlock`.\n",
    "        _ = t\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    ## U-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_channels: int = 3,\n",
    "        n_channels: int = 64,\n",
    "        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
    "        is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n",
    "        n_blocks: int = 2,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
    "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
    "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
    "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
    "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Number of resolutions\n",
    "        n_resolutions = len(ch_mults)\n",
    "\n",
    "        # Project image into feature map\n",
    "        self.image_proj = nn.Conv2d(\n",
    "            image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1)\n",
    "        )\n",
    "\n",
    "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
    "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
    "\n",
    "        # #### First half of U-Net - decreasing resolution\n",
    "        down = []\n",
    "        # Number of channels\n",
    "        out_channels = in_channels = n_channels\n",
    "        # For each resolution\n",
    "        for i in range(n_resolutions):\n",
    "            # Number of output channels at this resolution\n",
    "            out_channels = in_channels * ch_mults[i]\n",
    "            # Add `n_blocks`\n",
    "            for _ in range(n_blocks):\n",
    "                down.append(\n",
    "                    DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i])\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "            # Down sample at all resolutions except the last\n",
    "            if i < n_resolutions - 1:\n",
    "                down.append(Downsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.down = nn.ModuleList(down)\n",
    "\n",
    "        # Middle block\n",
    "        self.middle = MiddleBlock(\n",
    "            out_channels,\n",
    "            n_channels * 4,\n",
    "        )\n",
    "\n",
    "        # #### Second half of U-Net - increasing resolution\n",
    "        up = []\n",
    "        # Number of channels\n",
    "        in_channels = out_channels\n",
    "        # For each resolution\n",
    "        for i in reversed(range(n_resolutions)):\n",
    "            # `n_blocks` at the same resolution\n",
    "            out_channels = in_channels\n",
    "            for _ in range(n_blocks):\n",
    "                up.append(\n",
    "                    UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i])\n",
    "                )\n",
    "            # Final block to reduce the number of channels\n",
    "            out_channels = in_channels // ch_mults[i]\n",
    "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
    "            in_channels = out_channels\n",
    "            # Up sample at all resolutions except last\n",
    "            if i > 0:\n",
    "                up.append(Upsample(in_channels))\n",
    "\n",
    "        # Combine the set of modules\n",
    "        self.up = nn.ModuleList(up)\n",
    "\n",
    "        # Final normalization and convolution layer\n",
    "        self.norm = nn.GroupNorm(8, n_channels)\n",
    "        self.act = Swish()\n",
    "        self.final = nn.Conv2d(\n",
    "            in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
    "        * `t` has shape `[batch_size]`\n",
    "        \"\"\"\n",
    "\n",
    "        # Get time-step embeddings\n",
    "        t = self.time_emb(t)\n",
    "\n",
    "        # Get image projection\n",
    "        x = self.image_proj(x)\n",
    "\n",
    "        # `h` will store outputs at each resolution for skip connection\n",
    "        h = [x]\n",
    "        # First half of U-Net\n",
    "        for m in self.down:\n",
    "            x = m(x, t)\n",
    "            h.append(x)\n",
    "\n",
    "        # Middle (bottom)\n",
    "        x = self.middle(x, t)\n",
    "\n",
    "        # Second half of U-Net\n",
    "        for m in self.up:\n",
    "            if isinstance(m, Upsample):\n",
    "                x = m(x, t)\n",
    "            else:\n",
    "                # Get the skip connection from first half of U-Net and concatenate\n",
    "                s = h.pop()\n",
    "                x = torch.cat((x, s), dim=1)\n",
    "                #\n",
    "                x = m(x, t)\n",
    "\n",
    "        # Final normalization and convolution\n",
    "        return self.final(self.act(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(ch_mults = (1, 2, 1, 1))\n",
    "\n",
    "# Uncomment this\n",
    "# if you want to do the _VERY_ long training,\n",
    "# model = UNet(ch_mults = (1, 2, 2, 2))\n",
    "\n",
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "print(\n",
    "    f\"Number of parameters: {n_params:,}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848e1d2",
   "metadata": {},
   "source": [
    "Our model has around 9.1 Million parameters. When compared to Stable Diffusion, which has 1 Billion parameters, it is very small! However, for this dataset, it can still give remarkable results.\n",
    "\n",
    "### Training loop\n",
    "\n",
    "Let's now do some preparation for the training loop. First we transfer the model as well as all our precomputed quantities to the GPU, so they can be used efficiently during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb0d436",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Move everything to GPU\n",
    "model.to(device)\n",
    "\n",
    "sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device)\n",
    "alphas = alphas.to(device)\n",
    "alphas_cumprod = alphas_cumprod.to(device)\n",
    "alphas_cumprod_prev = alphas_cumprod_prev.to(device)\n",
    "sqrt_recip_alphas = sqrt_recip_alphas.to(device)\n",
    "sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device)\n",
    "sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device)\n",
    "posterior_variance = posterior_variance.to(device)\n",
    "betas = betas.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe055c7",
   "metadata": {},
   "source": [
    "Now we can define the loss we are going to minimize:\n",
    "\n",
    "> **Complete the section marked with YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1d353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4593ab6",
   "metadata": {},
   "source": [
    "Then we define a few parameters for our training. We are going to use Cosine Annealing for the learning rate, with a warmup period. This means that we are going to start from a very low learning rate, increase it linearly for a few epochs, then start decreasing it again with a cosine shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c8ea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr = 0.0006 # Maximum learning rate we will use\n",
    "epochs = 300 # Total number of epochs\n",
    "T_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle\n",
    "warmup_epochs = 2  # Number of warm-up epochs\n",
    "\n",
    "# Uncomment the following lines\n",
    "# if you want to do the _VERY_ long training,\n",
    "# base_lr = 0.0001 # Maximum learning rate we will use\n",
    "# epochs = 300 # Total number of epochs\n",
    "# T_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle\n",
    "# warmup_epochs = 10  # Number of warm-up epochs\n",
    "\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=base_lr)\n",
    "scheduler = CosineAnnealingLR(\n",
    "    optimizer, \n",
    "    T_max=T_max - warmup_epochs,\n",
    "    eta_min=base_lr / 10  # starting value for the LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1fca3",
   "metadata": {},
   "source": [
    "Finally let's train! We train only for 5 epochs, which should mean around 20 min of training time. This won't get us to a good result, but you will see a few hints of cars appearing little by little.\n",
    "\n",
    "> **Complete the section marked with YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977ea172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this noise to generate some images during training to check\n",
    "# where we stand\n",
    "fixed_noise = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "\n",
    "alpha = 0.1  # Smoothing factor\n",
    "ema_loss = None  # Initialize EMA loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warm-up\n",
    "        lr = base_lr * (epoch + 1) / warmup_epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "    else:\n",
    "        # Cosine Annealing after warm-up\n",
    "        scheduler.step()\n",
    "\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "    for batch, _ in tqdm(dataloader):\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "        bs = batch.shape[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        t = torch.randint(0, T, (batch.shape[0],), device=device).long()\n",
    "        \n",
    "        # Generate targets for the UNet and apply them to the images\n",
    "        noise = torch.randn_like(batch, device=device)\n",
    "        x_noisy = (\n",
    "            sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * batch + \n",
    "            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise\n",
    "        )\n",
    "        \n",
    "        noise_pred = model(x_noisy, t)\n",
    "        loss = criterion(noise, noise_pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if ema_loss is None:\n",
    "            # First batch\n",
    "            ema_loss = loss.item()\n",
    "        else:\n",
    "            # Exponential moving average of the loss\n",
    "            ema_loss = alpha * loss.item() + (1 - alpha) * ema_loss\n",
    "    \n",
    "    if epoch == epochs-1:\n",
    "        with torch.no_grad():\n",
    "    #         fig, _ = sample_image(fixed_noise, forward=False, device=device)\n",
    "            imgs = make_inference(fixed_noise, return_all=True)\n",
    "            fig = display_sequence([imgs[0].squeeze(dim=0)] + [x.squeeze(dim=0) for x in imgs[63::64]], nrow=9, dpi=150)\n",
    "            plt.show(fig)\n",
    "        os.makedirs(\"diffusion_output_long\", exist_ok=True)\n",
    "        fig.savefig(f\"diffusion_output_long/frame_{epoch:05d}.png\")\n",
    "    #plt.close(fig)\n",
    "    \n",
    "    print(f\"epoch {epoch+1}: loss: {ema_loss:.3f}, lr: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e63cad1",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We can now have a look at what our model can produce:\n",
    "\n",
    "> **Complete the section marked with YOUR CODE HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be2019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "input_noise = torch.randn((8, 3, IMG_SIZE, IMG_SIZE), device=device)\n",
    "imgs = make_inference(input_noise)\n",
    "_ = display_sequence(imgs, dpi=75, nrow=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7836b9",
   "metadata": {},
   "source": [
    "This is a fairly good result considering how small the model is and how little we trained it. We can already tell that it is indeed creating cars, with windshields and wheels, although it is still very early on. If we were to train for much longer, and/or use a larger model (for example, the one defined above in the commented lines has 55 Million parameters) and let it train for several hours, we would get something even better, like this:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
