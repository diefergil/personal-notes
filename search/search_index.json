{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Home", "text": ""}, {"location": "#welcome-to-mkdocs", "title": "Welcome to MkDocs", "text": "<p>For full documentation visit mkdocs.org.</p>"}, {"location": "#commands", "title": "Commands", "text": "<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"}, {"location": "#project-layout", "title": "Project layout", "text": "<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"}, {"location": "about/", "title": "About", "text": ""}, {"location": "about/#pendet-illud-mutilatae-certa-urgetque-populi-radiis", "title": "Pendet illud mutilatae certa urgetque populi radiis", "text": ""}, {"location": "about/#et-nostra-quod-quamvis-tamen", "title": "Et nostra quod quamvis tamen", "text": "<p>Lorem markdownum erat perque, colebat dedit; collo habuit relictis falcato et fide mente iugulatus intrat. Tenet orbe ignoscite Saturnia valeant ulla neque orant positus mea aspicit aegide loquentem animae postquam cecidere aras. Prosilit quod ignavo in crinis metus unda, di Stygias florebat lacrimantem vellent nunc, et undis moenia mecum?</p>"}, {"location": "about/#ferat-teli-exitio-acrisius-et-modo-veteres", "title": "Ferat teli exitio Acrisius et modo veteres", "text": "<p>Idas feram an esse paruerit feres; cadet tonitruque nostra femur ipse. Ut errat tenet magni ultra n\u00e9 signa, sub, obstantis legit non, auctor.</p> <ul> <li>Haec quondam relaxant litora auxiliaria ferro Ampycides</li> <li>Corda sceleri</li> <li>Furiisque stimulis domos quod per palla</li> <li>Ire responderat legit i qua frugum fuit</li> <li>Ignarus nepotem do gravis</li> </ul>"}, {"location": "about/#et-averna-cernimus-adsuetos-aiax-interea-perque", "title": "Et Averna cernimus adsuetos Aiax interea perque", "text": "<p>Mihi aversa ignisque flumina: miscet: ab deo sive avidisque. Ad veni deponendique pars interdum Byblis noctem, sed nostro, nec satis ignotissima. Subitus longis, faciemque amorem. Nube ilia opus vulnere mentis mihi sorores referam sperato, hos ignis possedit et invenit, mens saecula aetas comitesque.</p>"}, {"location": "about/#montibus-aurora-barba-achaide", "title": "Montibus Aurora barba Achaide", "text": "<p>Tutaque verumque monimenta clamata et pretium gemellos latratu Minoa aequore; puerum. Tartareas priori inscripta spretae sua, iactat adspice peregrinosque metallis expellitur. Duobus sed vigoris illa mutatus, multicavo animosa!</p> <ul> <li>Abdita laticesque lepores ferro sibi suam per</li> <li>Litora sub Cecropide</li> <li>Me vincula quod dabat flumen mensuraque secura</li> <li>Pulsavere cantus redeuntem peritura</li> <li>Et luctatusque aequantia caedis praesagia montis certamina</li> <li>Suos est lucem fine velox ubi nam</li> </ul> <p>Fessos animis custodit cumque, Priamidas, lucem mihi Pyrrha; namque. Ille nihil: illo ultor nisi materque sit sensit, Cyllenaeo opus.</p>"}, {"location": "hooks/", "title": "Hooks", "text": "In\u00a0[\u00a0]: Copied! <pre>import os\nimport shutil\n</pre> import os import shutil In\u00a0[\u00a0]: Copied! <pre>def copy_ads_txt(config, **kwargs):\n    site_dir = config['site_dir']\n    shutil.copy('theme/assets/ads.txt', os.path.join(site_dir, 'ads.txt'))\n</pre> def copy_ads_txt(config, **kwargs):     site_dir = config['site_dir']     shutil.copy('theme/assets/ads.txt', os.path.join(site_dir, 'ads.txt'))"}, {"location": "cloud/aws/app_runner/", "title": "App Runner", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#aws-app-runner", "title": "AWS App Runner", "text": "<p>App Runner is a fully managed service that makes it easy for developers to  quickly build, deploy, and run containerized applications. It was announced by   AWS (Amazon Web Services) in May 2021.</p> <p>Amazon App Runner is designed to simplify the process of deploying applications  in containers, making it easier for developers to focus on writing code rather   than managing infrastructure. The service automatically handles all the    operational aspects like building and running containers, scaling up or down     based on traffic, and monitoring application health.</p> <p>This guide will walk you through the process of deploying a containerized  FastAPI application using AWS App Runner.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#deploy-a-fastapi-app-in-app-runner", "title": "Deploy A FastApi app in App Runner", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>An AWS account.</li> <li>Docker installed on your machine.</li> <li>A FastAPI application to deploy.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-1-containerizing-your-fastapi-application", "title": "Step 1: Containerizing your FastAPI Application", "text": "<p>Once you have your Fastapi app builded, in this  example we are going to use a simple summatory function:</p> <pre><code># main.py\nfrom fastapi import FastAPI\nimport uvicorn\napp = FastAPI()\n@app.get('/')\nasync def root():\nreturn {'message': 'Hello Duke'}\n@app.get('/add/{num1}/{num2}')\nasync def add(num1: int, num2: int):\n'''Add two numbers together'''\ntotal = num1 + num2\nreturn {'total': total}\nif __name__ == '__main__':\nuvicorn.run(app, port=8080, host='0.0.0.0')\n</code></pre> <p>You can curl your app in local doing:</p> <pre><code>curl http:/0.0.0.0:8080/add/2/2\n</code></pre> <p>You'll need to create a Dockerfile in the root directory of your FastAPI  application. This file will instruct Docker on how to build a container for   your app.</p> <p>Here's a simple Dockerfile for a FastAPI app:</p> <pre><code>FROM public.ecr.aws/lambda/python:3.8 # (1)\nRUN mkdir -p /app\nCOPY . main.py /app/\nWORKDIR /app\nRUN pip install -r requirements.txt\nEXPOSE 8080\nCMD [ 'main.py' ]\nENTRYPOINT [ 'python', 'main.py' ]\n</code></pre> <ol> <li>The container that you need to run app runner.</li> </ol> <p>Then, build the Docker image with the following command:</p> <pre><code>docker build -t your-image-name .\n</code></pre>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-2-push-your-docker-image-to-amazon-ecr", "title": "Step 2: Push your Docker Image to Amazon ECR", "text": "<p>AWS App Runner needs to pull your Docker image from a registry. We'll use  Amazon's Elastic Container Registry (ECR).</p> <p>First, create a new repository in ECR:</p> <pre><code>aws ecr create-repository --repository-name your-repo-name\n</code></pre> <p>Next, authenticate Docker to your ECR registry:</p> <pre><code>aws ecr get-login-password --region your-region | docker login --username AWS\n--password-stdin your-ecr-url\n</code></pre> <p>Then, tag your image with the ECR repository:</p> <pre><code>docker tag your-image-name:latest your-ecr-url/your-repo-name:latest\n</code></pre> <p>Finally, push your image to ECR:</p> <pre><code>docker push your-ecr-url/your-repo-name:latest\n</code></pre>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-3-deploy-your-fastapi-app-using-aws-app-runner", "title": "Step 3: Deploy your FastAPI App using AWS App Runner", "text": "<p>Navigate to the AWS App Runner console and follow these steps:</p> <ol> <li>Click 'Create an App Runner service'.</li> <li>Select 'Source' as 'Container registry'.</li> <li>Enter the ECR image URI from the previous step.</li> <li>Configure the build settings and deployment settings as per your requirements.</li> <li>Click 'Create and Deploy'.</li> </ol> <p>You should now have your FastAPI application running on AWS App Runner!</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#conclusion", "title": "Conclusion", "text": "<p>AWS App Runner is a powerful service for deploying containerized applications.  With it, you can focus more on developing your FastAPI applications and less   on managing infrastructure.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#references", "title": "References", "text": "<ul> <li>Fastapi</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/batch/", "title": "Batch", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#aws-batch", "title": "AWS Batch", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#introduction", "title": "Introduction", "text": "<p>AWS Batch is a cloud service provided by Amazon Web Services (AWS) that enables  developers and scientists to easily and efficiently run hundreds of thousands   of batch computing jobs. AWS Batch dynamically provisions the optimal    quantity and type of compute resources based on the volume and specific     resource requirements of the batch jobs submitted.</p> <p>In the context of machine learning (ML), AWS Batch can be a powerful tool for  training models.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#what-is-aws-batch", "title": "What is AWS Batch?", "text": "<p>AWS Batch is a set of batch management capabilities that enables developers,  scientists, and engineers to easily and efficiently run hundreds of thousands   of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal    quantity and type of compute resources (e.g., CPU or memory-optimized     instances) based on the volume and specific resource requirements of the      batch jobs submitted.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#why-use-aws-batch-for-machine-learning", "title": "Why Use AWS Batch for Machine Learning?", "text": "<p>Training machine learning models often involves running large-scale compute  jobs. These jobs can take a long time to complete and require significant   compute resources. AWS Batch is designed to handle this kind of workload    efficiently. Here are some reasons why AWS Batch is a good fit for ML model     training:</p> <ul> <li> <p>Scalability: AWS Batch can automatically scale up to handle large jobs    and scale down when resources are not needed, helping you to use resources     efficiently.</p> </li> <li> <p>Cost-Effectiveness: With AWS Batch, you pay only for the compute time you    consume. It also integrates with Spot Instances, allowing you to take     advantage of unused EC2 capacity at a significant discount.</p> </li> <li> <p>Integration with AWS Services: AWS Batch integrates with other AWS    services like Amazon S3, Amazon EC2, and AWS IAM, making it easier to set up     and manage your ML training jobs.</p> </li> <li> <p>Simplified Operations: AWS Batch removes the need to install and manage    batch computing software, allowing you to focus on analyzing results and     solving problems.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#getting-started-with-aws-batch-for-machine-learning", "title": "Getting Started with AWS Batch for Machine Learning", "text": "<p>Here are the general steps to use AWS Batch for training ML models:</p> <ol> <li> <p>Prepare Your Training Code: Write your ML model training code and     package it into a Docker container. This container will be the job that AWS      Batch runs.</p> </li> <li> <p>Upload Your Data: Upload your training data to a storage service like     Amazon S3.</p> </li> <li> <p>Create a Compute Environment: In the AWS Batch console, create a     compute environment that specifies the type of instances that you want to      use for your jobs.</p> </li> <li> <p>Create a Job Queue: Create a job queue that is associated with the     compute environment you created.</p> </li> <li> <p>Submit a Job: Submit a job to the job queue. In the job definition,     specify the Docker container with your training code and the location of      your training data.</p> </li> <li> <p>Monitor Your Job: Use the AWS Batch console or CloudWatch Logs to     monitor the progress of your job.</p> </li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/lambda/", "title": "Lambda functions", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#using-python-with-aws-lambda-a-comprehensive-guide", "title": "Using Python with AWS Lambda: A Comprehensive Guide", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#introduction", "title": "Introduction", "text": "<p>Amazon Web Services (AWS) Lambda is a serverless computing service that allows  you to run your applications without having to manage servers. It executes   your code only when required and scales automatically, from a few requests    per day to thousands per second. You only pay for the compute time you     consume - there is no charge when your code is not running.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#setting-up-aws-lambda", "title": "Setting Up AWS Lambda", "text": "<p>Before you can start using AWS Lambda with Python, you need to set up your AWS  environment. Here are the steps to do so:</p> <ol> <li>Create an IAM Role: AWS Identity and Access Management (IAM) roles are     used to grant permissions to your Lambda function. You can create an IAM      role from the AWS Management Console following the instructions in the       official AWS IAM User Guide.</li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#writing-python-code-for-aws-lambda", "title": "Writing Python Code for AWS Lambda", "text": "<p>Once you've set up your AWS environment, you can start writing Python code for  AWS Lambda. Here's a simple example of a Lambda function written in Python:</p> <pre><code>def lambda_handler(event, context):\n# print the event details\nprint('Received event: ' + str(event))\n# return a response\nreturn {\n'statusCode': 200,\n'body': 'Hello from Lambda!'\n}\n</code></pre> <p>In this example, <code>lambda_handler</code> is the entry point to your Lambda function.  AWS Lambda passes event data to this handler as the first parameter, and   context information as the second parameter.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#deploying-your-lambda-function", "title": "Deploying Your Lambda Function", "text": "<p>After writing your Python code, you need to deploy your Lambda function to the  AWS environment. Here are the steps to do so:</p> <ol> <li> <p>Package Your Code: Zip your code and any dependencies into a deployment     package. For Python, your deployment package can be as simple as a .zip     file containing your .py files.</p> </li> <li> <p>Create a Lambda Function: Go to the AWS Management Console, navigate to     the Lambda service, and click on 'Create function'. You can then provide      your function name, select Python as your runtime, and upload your .zip file.</p> </li> <li> <p>Test Your Function: After creating your function, you can test it by     clicking on 'Test' in the AWS Management Console. You can define a test      event and see the result of your function execution.</p> </li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#monitoring-and-debugging", "title": "Monitoring and Debugging", "text": "<p>AWS provides several tools for monitoring and debugging your Lambda functions:</p> <ul> <li> <p>AWS CloudWatch: AWS CloudWatch allows you to collect and track metrics,    collect and monitor log files, and set alarms. You can use CloudWatch to     gain system-wide visibility into resource utilization, application      performance, and operational health.</p> </li> <li> <p>AWS X-Ray: AWS X-Ray helps you debug and analyze your microservices    applications with request tracing. You can use X-Ray to trace requests from     start to end and get a detailed view of the entire request path.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#handling-events-in-aws-lambda-functions-with-python", "title": "Handling Events in AWS Lambda Functions with Python", "text": "<p>AWS Lambda is an event-driven computing service that executes your code in  response to events. These events can come from a variety of sources, such as   HTTP requests via Amazon API Gateway, modifications to objects in Amazon S3    buckets, table updates in Amazon DynamoDB, and state transitions in AWS Step Functions.</p> <p>In this section, we will discuss how to handle events in AWS Lambda functions  using Python.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#understanding-the-event-object", "title": "Understanding the Event Object", "text": "<p>When AWS Lambda executes your function, it passes an event object to the  handler. This object contains information about the event that triggered the   function. The structure of the event object varies depending on the event    source. For example, an event from Amazon S3 might look like this:</p> <pre><code>{\n\"Records\": [\n{\n\"eventVersion\": \"2.1\",\n\"eventSource\": \"aws:s3\",\n\"awsRegion\": \"us-west-2\",\n\"eventTime\": \"2021-05-22T00:17:44.695Z\",\n\"eventName\": \"ObjectCreated:Put\",\n\"s3\": {\n\"s3SchemaVersion\": \"1.0\",\n\"configurationId\": \"testConfigRule\",\n\"bucket\": {\n\"name\": \"mybucket\",\n\"ownerIdentity\": {\n\"principalId\": \"EXAMPLE\"\n},\n\"arn\": \"arn:aws:s3:::mybucket\"\n},\n\"object\": {\n\"key\": \"HappyFace.jpg\",\n\"size\": 1024,\n\"eTag\": \"d41d8cd98f00b204e9800998ecf8427e\",\n\"sequencer\": \"0A1B2C3D4E5F678901\"\n}\n}\n}\n]\n}\n</code></pre> <p>In this case, the event object contains information about the S3 bucket and the  object that was created.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#accessing-event-data", "title": "Accessing Event Data", "text": "<p>You can access the data in the event object just like you would with any Python  dictionary. Here's an example of a Lambda function that prints the name of the   S3 bucket and the key of the object:</p> <pre><code>def lambda_handler(event, context):\n# get the bucket name\nbucket = event['Records'][0]['s3']['bucket']['name']\n# get the object key\nkey = event['Records'][0]['s3']['object']['key']\n# print the bucket name and object key\nprint(f'Bucket: {bucket}, Key: {key}')\nreturn {\n'statusCode': 200,\n'body': f'Bucket: {bucket}, Key: {key}'\n}\n</code></pre> <p>In this example, <code>event['Records'][0]['s3']['bucket']['name']</code> accesses the  name of the S3 bucket, and <code>event['Records'][0]['s3']['object']['key']</code>   accesses the key of the object.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#conclusion", "title": "Conclusion", "text": "<p>Handling events in AWS Lambda functions with Python involves understanding the  structure of the event object and accessing its data. The event object   provides valuable information about the event that triggered the function,    allowing you to write code that responds appropriately to the event.</p> <p>Remember, the structure of the event object depends on the event source, so be  sure to check the AWS Lambda documentation   for details about the event object structure for different event sources.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#understanding-the-context-object-in-aws-lambda-functions-with-python", "title": "Understanding the Context Object in AWS Lambda Functions with Python", "text": "<p>In addition to the event object, AWS Lambda also passes a context object to  your function. This object provides methods and properties that provide   information about the invocation, function, and execution environment.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#properties-of-the-context-object", "title": "Properties of the Context Object", "text": "<p>Here are some of the properties provided by the context object:</p> <ul> <li><code>aws_request_id</code>: The identifier of the invocation request.</li> <li><code>log_group_name</code>: The log group for the function.</li> <li><code>log_stream_name</code>: The log stream for the function instance.</li> <li><code>function_name</code>: The name of the Lambda function.</li> <li><code>memory_limit_in_mb</code>: The amount of memory available to the function in MB.</li> <li><code>function_version</code>: The version of the function.</li> <li><code>invoked_function_arn</code>: The Amazon Resource Name (ARN) used to invoke the    function. It can be function ARN or alias ARN. An unqualified ARN executes     the <code>$LATEST</code> version and aliases execute the function version it is      pointing to.</li> <li><code>identity</code> and <code>client_context</code>: For AWS Mobile SDK invocations, these    provide information about the client application and device.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#methods-of-the-context-object", "title": "Methods of the Context Object", "text": "<p>The context object also provides the following methods:</p> <ul> <li><code>get_remaining_time_in_millis()</code>: Returns the number of milliseconds left    before the execution times out.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#using-the-context-object", "title": "Using the Context Object", "text": "<p>Here's an example of how to use some of the properties and methods of the  context object:</p> <pre><code>def lambda_handler(event, context):\n# print the AWS request ID and memory limit\nprint(f'AWS Request ID: {context.aws_request_id}')\nprint(f'Memory Limit: {context.memory_limit_in_mb}')\n# get the remaining execution time\nremaining_time = context.get_remaining_time_in_millis()\nprint(f'Remaining Time: {remaining_time}ms')\nreturn {\n'statusCode': 200,\n'body': 'Hello from Lambda!'\n}\n</code></pre> <p>In this example, <code>context.aws_request_id</code> and <code>context.memory_limit_in_mb</code> are  used to print the AWS request ID and memory limit, respectively. The <code>context.  get_remaining_time_in_millis()</code> method is used to get the remaining execution time.</p> <p>The context object is a powerful tool that provides valuable information about  the invocation and execution environment of your AWS Lambda function. By   understanding and utilizing the properties and methods of the context object,    you can write more robust and efficient Lambda functions.</p> <p>For more information about the context object, check out the AWS Lambda documentation.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#chaining-aws-lambda-functions-with-python", "title": "Chaining AWS Lambda Functions with Python", "text": "<p>In AWS Lambda, you can create a sequence of Lambda functions where the output  of one function becomes the input of the next. This is often referred to as   \"chaining\" Lambda functions. Chaining can be useful when you need to create a    pipeline of processing steps, each handled by a separate Lambda function.</p> <p>There are several ways to chain Lambda functions, but one of the most common  methods is using AWS Step Functions.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#aws-step-functions", "title": "AWS Step Functions", "text": "<p>AWS Step Functions is a serverless  workflow service that lets you coordinate multiple AWS services into   serverless workflows. You can design and run workflows that stitch together    services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into     feature-rich applications.</p> <p>Here's a basic example of how you can use AWS Step Functions to chain two  Lambda functions:</p> <ol> <li> <p>Create Your Lambda Functions: First, you need to create your Lambda     functions. For example, you might have a function <code>functionA</code> that      processes an input and produces an output, and a function <code>functionB</code> that       takes the output of <code>functionA</code> as its input.</p> <pre><code># functionA\ndef lambda_handler(event, context):\n# process the event\nprocessed_event = process_event(event)\nreturn processed_event\n# functionB\ndef lambda_handler(event, context):\n# the event is the output of functionA\nresult = do_something_with(event)\nreturn result\n</code></pre> </li> <li> <p>Define Your Step Functions State Machine: A state machine in AWS Step     Functions is a JSON-based, visual workflow of your application's steps.      Here's an example of a state machine that chains <code>functionA</code> and <code>functionB</code>:</p> </li> </ol> <pre><code>{\n\"Comment\": \"A simple AWS Step Functions state machine that chains two Lambda functions.\",\n\"StartAt\": \"functionA\",\n\"States\": {\n\"functionA\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:functionA\",\n\"Next\": \"functionB\"\n},\n\"functionB\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:functionB\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this state machine, the <code>StartAt</code> field specifies the first state to run  (<code>functionA</code>). The <code>Next</code> field in the <code>functionA</code> state specifies the next   state to run after <code>functionA</code> (<code>functionB</code>). The <code>End</code> field in the    <code>functionB</code> state indicates that <code>functionB</code> is the final state.</p> <ol> <li>Create Your State Machine: After defining your state machine, you can     create it in the AWS Step Functions console. You can then start an      execution of your state machine, providing an initial JSON input. AWS Step       Functions will run your Lambda functions in the order defined by your        state machine, passing the output of one function as the input to the         next.</li> </ol> <p>Remember, error handling and retry policies are important considerations when  chaining Lambda functions. AWS Step Functions provides built-in support for   error handling and retries, which you can customize in your state machine definition.</p> <p>For more information about AWS Step Functions, check out the  official AWS Step Functions Developer Guide.</p> <p>You can find the section about AWS steps function here</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#conclusion_1", "title": "Conclusion", "text": "<p>AWS Lambda and Python are a powerful combination for serverless computing. With  AWS Lambda, you can focus on writing code without having to worry about   managing servers. And with Python, you can write readable and maintainable    code that can be easily deployed to AWS Lambda.</p> <p>This guide has covered the basics of using Python with AWS Lambda, but there's  much more to learn. Be sure to check out the official AWS Lambda Developer   Guide and the    official Python documentation for more     information. Happy coding!</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/steps/", "title": "Steps", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#aws-step-functions", "title": "AWS Step Functions", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#introduction", "title": "Introduction", "text": "<p>AWS Step Functions is a serverless workflow service provided by Amazon Web  Services. It allows developers to design and execute workflows that coordinate   between multiple AWS services such as AWS Lambda, Amazon SNS, and Amazon    DynamoDB. These workflows, known as state machines, are defined using a     JSON-based, Amazon States Language (ASL).</p> <p>In this article, we will explore the basics of AWS Step Functions, how to  create a state machine, and how to integrate it with other AWS services.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#what-are-aws-step-functions", "title": "What are AWS Step Functions?", "text": "<p>AWS Step Functions is a service that helps you coordinate multiple AWS services  into serverless workflows so you can build and update apps quickly. Using Step   Functions, you can design and run workflows that stitch together services    such as AWS Lambda and Amazon ECS into feature-rich applications.</p> <p>Workflows are made up of a series of steps, with the output of one step acting  as input into the next. AWS Step Functions is fully managed, so it scales,   operates, and ensures the reliability of your operational tasks so you can    focus on your application.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#components-of-aws-step-functions", "title": "Components of AWS Step Functions", "text": "<p>The primary components of AWS Step Functions are:</p> <ul> <li> <p>State Machine: A state machine is the core component that you interact    with. It defines the workflow of the application and is described using the     Amazon States Language.</p> </li> <li> <p>States: Each step in the workflow is represented as a state. There are    various types of states like Task, Choice, Wait, Succeed, Fail, Parallel,     and Map.</p> </li> <li> <p>Transitions: Transitions are the movement between states in a state    machine.</p> </li> <li> <p>Tasks: Tasks represent a single unit of work that the state machine needs    to perform.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#creating-a-state-machine", "title": "Creating a State Machine", "text": "<p>Creating a state machine involves defining the state machine structure using  the Amazon States Language in a JSON format. Here's an example of a simple   state machine:</p> <pre><code>{\n\"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\",\n\"StartAt\": \"HelloWorld\",\n\"States\": {\n\"HelloWorld\": {\n\"Type\": \"Pass\",\n\"Result\": \"Hello, World!\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this example, there is a single state named <code>HelloWorld</code>. The <code>Type</code> field  indicates that this is a <code>Pass</code> state, which is a state that does nothing and   passes its input to its output. The <code>Result</code> field contains a static string    that is the output of the state. The <code>End</code> field indicates that this is the     end of the execution.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#integrating-with-other-aws-services", "title": "Integrating with Other AWS Services", "text": "<p>AWS Step Functions can integrate with various AWS services. For example, you  can use AWS Lambda functions as tasks within your state machine. Here's an   example of a state machine that uses a Lambda function:</p> <pre><code>{\n\"Comment\": \"A simple AWS Step Functions state machine that executes a Lambda function\",\n\"StartAt\": \"InvokeLambdaFunction\",\n\"States\": {\n\"InvokeLambdaFunction\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this example, the <code>Resource</code> field contains the ARN of the Lambda function  to invoke.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#monitoring-and-debugging", "title": "Monitoring and Debugging", "text": "<p>AWS Step Functions provides detailed logging for each step of your execution in  CloudWatch Logs. You can use these logs to monitor executions and to   troubleshoot issues. AWS Step Functions also provides visual workflows in the    AWS Management Console, which allows you to see the path that your execution     took through the state machine.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#conclusion", "title": "Conclusion", "text": "<p>AWS Step Functions is a powerful service for orchestrating multi-step workflows  in a reliable and scalable manner. By defining workflows as state machines,   you can simplify complex processes and coordinate between multiple AWS services.</p> <p>For more information about AWS Step Functions, check out the  official AWS Step Functions Developer Guide.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/github/codespaces/", "title": "Codespaces", "text": ""}, {"location": "cloud/github/codespaces/#github-codespaces", "title": "Github codespaces", "text": ""}, {"location": "cloud/github/codespaces/#resources", "title": "Resources", "text": "<ul> <li>Fine tuning hugginface model</li> </ul>"}, {"location": "cloud/google/app_engine/", "title": "App Engine", "text": "", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#google-app-engine", "title": "Google App Engine", "text": "", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#deploy-a-fastapi-app-in-google-app-engine", "title": "Deploy a Fastapi App in Google App engine", "text": "<p>This guide will help you deploy a FastAPI application on Google App Engine.</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>A Google Cloud account</li> <li>Google Cloud SDK installed on your machine</li> <li>A FastAPI application</li> </ul>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-1-prepare-your-fastapi-application", "title": "Step 1: Prepare Your FastAPI Application", "text": "<p>Your FastAPI application is ready to go. Here's the <code>main.py</code> for reference:</p> <pre><code>from fastapi import FastAPI\nimport uvicorn\napp = FastAPI()\n@app.get('/')\nasync def root():\nreturn {'message': 'Hello Duke'}\n@app.get('/add/{num1}/{num2}')\nasync def add(num1: int, num2: int):\n'''Add two numbers together'''\ntotal = num1 + num2\nreturn {'total': total}\nif __name__ == '__main__':\nuvicorn.run(app, port=8080, host='0.0.0.0')\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-2-create-appyaml-file", "title": "Step 2: Create app.yaml File", "text": "<p>Next, you'll need to create an <code>app.yaml</code> file in the root directory of your  project. This file configures your App Engine application's settings.</p> <p>Here's an example <code>app.yaml</code> file:</p> <pre><code>runtime: python39  # Use the Python 3.9 runtime\ninstance_class: F2  # Choose a class with at least 256MB to run FastAPI and Uvicorn\nentrypoint: uvicorn main:app --host 0.0.0.0 --port $PORT\nautomatic_scaling:\ntarget_cpu_utilization: 0.65\nmin_instances: 1\nmax_instances: 10\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-3-create-a-requirementstxt-file", "title": "Step 3: Create a requirements.txt File", "text": "<p>Create a <code>requirements.txt</code> file in the root directory of your project and add  the necessary dependencies:</p> <pre><code>fastapi==0.68.1\nuvicorn[standard]==0.15.0\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-4-deploy-your-application", "title": "Step 4: Deploy Your Application", "text": "<p>Before you deploy, make sure you're authenticated to Google Cloud:</p> <pre><code>gcloud auth login\n</code></pre> <p>Next, set your project ID:</p> <pre><code>gcloud config set project your-project-id\n</code></pre> <p>Finally, deploy your app:</p> <pre><code>gcloud app deploy\n</code></pre> <p>Your FastAPI application should now be deployed to Google App Engine!</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#bonus-deploy-with-github-actions", "title": "Bonus: Deploy with github actions", "text": "<p>To deploy your FastAPI application on Google App Engine using GitHub Actions,  you'll first need to create a service account in your Google Cloud Project.   This service account should have the 'App Engine Deployer' and 'Storage    Admin' roles to allow it to deploy applications and upload to the Cloud     Storage bucket. Download the JSON key file for this service account and      store it as a secret (let's say GCP_SA_KEY) in your GitHub repository.</p> <p>Here's a simple GitHub Actions workflow that can be used for deployment. Create  a new file under .github/workflows in your repository named deploy.yml and add   the following content:</p> <pre><code>name: Deploy to Google App Engine\non:\npush:\nbranches:\n- main  # Trigger the workflow on push to main branch\njobs:\ndeploy:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout code\nuses: actions/checkout@v2\n- name: Set up Python 3.9\nuses: actions/setup-python@v2\nwith:\npython-version: 3.9\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Setup gcloud CLI\nuses: google-github-actions/setup-gcloud@master\nwith:\nservice_account_key: ${{ secrets.GCP_SA_KEY }}\nproject_id: your-gcp-project-id\nexport_default_credentials: true\n- name: Deploy to App Engine\nrun: gcloud app deploy --quiet\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#conclusion", "title": "Conclusion", "text": "<p>Google App Engine is a powerful platform for deploying Python web applications.  With it, you can focus on building your FastAPI application and leave the  infrastructure management to Google.</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#references", "title": "References", "text": "<ul> <li>Fastapi</li> <li>Github Actions</li> </ul>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/cloud_functions/", "title": "Cloud Functions", "text": ""}, {"location": "cloud/google/cloud_functions/#google-cloud-functions-serverless-computing-made-easy", "title": "Google Cloud Functions: Serverless Computing Made Easy", "text": "<p>In today's fast-paced digital landscape, businesses are increasingly looking  for ways to optimize their application development and deployment processes.   Serverless computing has emerged as a popular solution, offering scalability,    cost-effectiveness, and simplified management. Among the various serverless     platforms available, Google Cloud Functions stands out as a powerful and      user-friendly option. In this article, we will explore the features and       benefits of Google Cloud Functions and how it can help developers build        and deploy applications effortlessly.</p>"}, {"location": "cloud/google/cloud_functions/#what-are-google-cloud-functions", "title": "What are Google Cloud Functions", "text": "<p>Google Cloud Functions is a serverless computing platform that enables developers to build and run applications without worrying about infrastructure  management. With Cloud Functions, developers can write and deploy code in a   serverless environment, where the cloud provider handles all the operational    aspects such as scaling, patching, and monitoring. This allows developers to     focus solely on writing the application logic, resulting in faster      development cycles and increased productivity.</p>"}, {"location": "cloud/google/cloud_functions/#key-features", "title": "Key Features", "text": ""}, {"location": "cloud/google/cloud_functions/#1-event-driven-computing", "title": "1. Event-Driven Computing", "text": "<p>Google Cloud Functions is designed around the concept of event-driven  computing. Developers can write functions that respond to various types of   events, such as changes in data, incoming HTTP requests, or messages from a    messaging system. This event-driven model allows applications to be highly     reactive and responsive, triggering functions only when needed, reducing      costs and optimizing resource utilization.</p>"}, {"location": "cloud/google/cloud_functions/#2-language-support", "title": "2. Language Support", "text": "<p>Cloud Functions supports a wide range of programming languages, including  JavaScript (Node.js), Python, Go, and more. This flexibility allows developers   to use their preferred language and leverage existing code and libraries.    Whether you're building a web application, processing data, or creating a     microservice, you can find the right language for your needs.</p>"}, {"location": "cloud/google/cloud_functions/#3-automatic-scaling", "title": "3. Automatic Scaling", "text": "<p>One of the significant advantages of serverless computing is automatic scaling.  With Cloud Functions, you don't have to worry about provisioning or managing   resources based on anticipated traffic. The platform automatically scales the    resources up or down based on the incoming request rate, ensuring optimal     performance and cost efficiency. You pay only for the actual execution time      of your functions, making it a highly cost-effective solution.</p>"}, {"location": "cloud/google/cloud_functions/#4-seamless-integration-with-google-cloud-services", "title": "4. Seamless Integration with Google Cloud Services", "text": "<p>Google Cloud Functions seamlessly integrates with other services provided by  Google Cloud, such as Cloud Storage, Cloud Pub/Sub, Cloud Firestore, and more.   This tight integration allows you to create powerful applications that    leverage the full potential of Google Cloud's ecosystem. Whether you need to     process incoming data, trigger actions based on file uploads, or perform      real-time analytics, Cloud Functions provides the necessary tools for       seamless integration.</p>"}, {"location": "cloud/google/cloud_functions/#5-monitoring-and-logging", "title": "5. Monitoring and Logging", "text": "<p>Google Cloud Functions offers robust monitoring and logging capabilities,  allowing developers to gain insights into their applications' performance and   troubleshoot issues effectively. You can monitor the execution metrics, view    logs, and set up alerts to ensure your functions are running smoothly.</p>"}, {"location": "cloud/google/cloud_functions/#writing-and-triggering-a-google-cloud-function", "title": "Writing and Triggering a Google Cloud Function", "text": "<p>Let's take a look at an example of writing and triggering a Google Cloud  Function using Python.</p>"}, {"location": "cloud/google/cloud_functions/#writing-the-function", "title": "Writing the Function", "text": "<pre><code>def hello_world(request):\n\"\"\"HTTP Cloud Function.\n    Args:\n        request (flask.Request): The request object.\n        &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Request&gt;\n    Returns:\n        The response text, or any set of values that can be turned into a\n        Response object using `make_response`\n        &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response&gt;.\n    \"\"\"\nreturn 'Hello, World!'\n</code></pre> <p>In this example, we have a simple function named <code>hello_world</code> that takes a  Flask <code>request</code> object as an argument and returns the string \"Hello, World!\".</p>"}, {"location": "cloud/google/cloud_functions/#deploying-the-function", "title": "Deploying the Function", "text": "<p>To deploy the function to Google Cloud Functions, follow these steps:</p> <ol> <li> <p>Make sure you have the Google Cloud SDK     installed and configured on your machine.</p> </li> <li> <p>Open a terminal or command prompt and navigate to the directory containing     your Python function code.</p> </li> <li> <p>Run the following command to deploy the function:</p> </li> </ol> <pre><code>gcloud functions deploy hello_world \\\n--runtime python39 \\\n--trigger-http \\\n--allow-unauthenticated\n</code></pre> <p>This command deploys the function with the name <code>hello_world</code>, specifies the  Python 3.9 runtime, and sets the trigger type to HTTP. The   <code>--allow-unauthenticated</code> flag allows the function to be triggered without authentication.</p>"}, {"location": "cloud/google/cloud_functions/#triggering-and-testing-the-function", "title": "Triggering and Testing the Function", "text": "<p>Once the function is deployed, you can trigger it by sending an HTTP request to  its URL. You can use tools like <code>curl</code> or <code>httpie</code> to test the function   locally or use services like Postman for more    advanced testing.</p> <p>Here's an example using <code>curl</code> to send a GET request to the function's URL:</p> <pre><code>curl https://REGION-PROJECT_ID.cloudfunctions.net/hello_world\n</code></pre> <p>Replace <code>REGION</code> with the region where the function is deployed (e.g., <code>us-central1</code>) and <code>PROJECT_ID</code> with your Google Cloud project ID.</p> <p>After triggering the function, you should receive the response \"Hello, World!\".</p>"}, {"location": "cloud/google/cloud_functions/#conclusion", "title": "Conclusion", "text": "<p>Google Cloud Functions offers a powerful and convenient platform for building  and deploying serverless applications. With its event-driven model, support   for multiple programming languages, automatic scaling, seamless integration    with Google Cloud services, and robust monitoring capabilities, developers     can focus on writing application logic without worrying about      infrastructure management. By leveraging the power of serverless       computing, businesses can optimize resource utilization, reduce costs,        and accelerate their application development process.</p>"}, {"location": "cloud/google/cloud_run/", "title": "Cloud Run", "text": ""}, {"location": "cloud/google/cloud_run/#cloud-run-scaling-machine-learning-applications-with-ease", "title": "Cloud Run: Scaling Machine Learning Applications with Ease", "text": "<p>As machine learning continues to revolutionize various industries, the need for scalable and efficient deployment solutions for ML applications becomes crucial. Google Cloud Run offers a powerful and flexible platform for running containerized applications, making it an ideal choice for deploying machine learning models. In this article, we will explore the features and benefits of Google Cloud Run and how it can be leveraged to deploy a machine learning application using Python.</p>"}, {"location": "cloud/google/cloud_run/#what-is-google-cloud-run", "title": "What is Google Cloud Run", "text": "<p>Google Cloud Run is a fully managed serverless platform that allows developers to deploy containerized applications quickly and easily. It provides automatic scaling, networking, and infrastructure management, enabling developers to focus on building and deploying applications without worrying about the underlying infrastructure. With Cloud Run, you can deploy stateless HTTP services and take advantage of on-demand scaling, cost-efficiency, and ease of management.</p>"}, {"location": "cloud/google/cloud_run/#key-features", "title": "Key Features", "text": ""}, {"location": "cloud/google/cloud_run/#1-serverless-scalability", "title": "1. Serverless Scalability", "text": "<p>Cloud Run offers automatic scaling based on the incoming request rate, allowing your application to handle any level of traffic without manual intervention. It scales containers up and down quickly, ensuring that your machine learning application can handle peak loads efficiently. You only pay for the resources consumed during execution, making it a cost-effective solution for running ML workloads.</p>"}, {"location": "cloud/google/cloud_run/#2-container-compatibility", "title": "2. Container Compatibility", "text": "<p>Cloud Run supports containerized applications, allowing you to package your machine learning model and dependencies into a Docker container. This flexibility enables you to use any programming language, library, or framework that can be containerized. With Python being a popular choice for machine learning, you can easily deploy Python-based ML applications on Cloud Run.</p>"}, {"location": "cloud/google/cloud_run/#3-rapid-deployment", "title": "3. Rapid Deployment", "text": "<p>Deploying a machine learning application on Cloud Run is straightforward. You can use the command-line interface (CLI) or integrate with continuous integration and deployment (CI/CD) pipelines to automate the deployment process. Cloud Run provides a seamless experience for deploying new versions or rolling back to previous versions, enabling rapid iteration and deployment cycles.</p>"}, {"location": "cloud/google/cloud_run/#4-easy-integration", "title": "4. Easy Integration", "text": "<p>Cloud Run seamlessly integrates with other Google Cloud services, such as Cloud Storage, BigQuery, and Pub/Sub. This integration allows you to utilize additional services for data storage, data processing, and event-driven architectures. For example, you can trigger your ML application based on new data arriving in Cloud Storage or process predictions asynchronously using Pub/Sub.</p>"}, {"location": "cloud/google/cloud_run/#deploying-a-machine-learning-application-with-cloud-run", "title": "Deploying a Machine Learning Application with Cloud Run", "text": "<p>Let's explore how to deploy a machine learning application using Python and Cloud Run.</p>"}, {"location": "cloud/google/cloud_run/#building-the-docker-container", "title": "Building the Docker Container", "text": "<p>To deploy a machine learning application on Cloud Run, you need to package your application and its dependencies into a Docker container. Here are the steps to build the container:</p> <ol> <li> <p>Create a <code>Dockerfile</code> in your project directory with the following content:</p> <pre><code># Use the official Python runtime as the base image\nFROM python:3.9-slim\n# Set the working directory in the container\nWORKDIR /app\n# Copy the requirements file and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Define the command to run your application\nCMD [ \"python\", \"app.py\" ]\n</code></pre> </li> <li> <p>Create a <code>requirements.txt</code> file listing the Python dependencies required     by your machine learning application.</p> </li> <li> <p>Build the Docker image by running the following command in the project     directory:</p> </li> </ol> <pre><code>docker build -t gcr.io/PROJECT_ID/IMAGE_NAME .\n</code></pre> <p>Replace <code>PROJECT_ID</code> with your Google Cloud project ID and <code>IMAGE_NAME</code> with the desired name for your Docker image.</p> <ol> <li>Push the Docker image to the Google Container Registry (GCR):</li> </ol> <pre><code>docker push gcr.io/PROJECT_ID/IMAGE_NAME\n</code></pre>"}, {"location": "cloud/google/cloud_run/#deploying-the-application", "title": "Deploying the Application", "text": "<p>Once the Docker image is built and pushed to GCR, you can deploy the machine learning application on Cloud Run:</p> <ol> <li> <p>Using the Google Cloud Console, navigate to the Cloud Run section.</p> </li> <li> <p>Click on \"Create Service.\"</p> </li> <li> <p>Choose the Docker image you pushed to GCR.</p> </li> <li> <p>Configure the service settings, including the region, memory allocation,     and maximum number of container instances.</p> </li> <li> <p>Click on \"Create\" to deploy the application.</p> </li> </ol>"}, {"location": "cloud/google/cloud_run/#testing-the-application", "title": "Testing the Application", "text": "<p>After the application is deployed, Cloud Run generates a unique URL that you can use to test the machine learning application. You can send HTTP requests to this URL with the necessary input data to receive predictions from your model.</p> <p>Using Python, you can write a simple script to test the deployed ML application. Here's an example using the <code>requests</code> library:</p> <pre><code>import requests\nurl = \"https://YOUR_CLOUD_RUN_URL\"\ndata = {\n\"feature1\": 0.5,\n\"feature2\": 0.8,\n# Include other input features as required by your model\n}\nresponse = requests.post(url, json=data)\npredictions = response.json()\nprint(predictions)\n</code></pre> <p>Replace <code>YOUR_CLOUD_RUN_URL</code> with the URL generated by Cloud Run for your deployed ML application. The script sends a POST request with input data in JSON format and retrieves the predictions as the response.</p>"}, {"location": "cloud/google/cloud_run/#conclusion", "title": "Conclusion", "text": "<p>Google Cloud Run provides an excellent platform for deploying machine learning applications with ease. By leveraging the power of containerization, automatic scaling, and seamless integration with Google Cloud services, developers can efficiently deploy and scale their ML models. Whether you are deploying a simple predictive model or a complex deep learning network, Cloud Run offers the flexibility and scalability required to meet the demands of modern machine learning applications.</p>"}, {"location": "data_science/statistics/intro/", "title": "Introduction to statistics", "text": "In\u00a0[1]: Copied! <pre>2+2\n</pre> 2+2 Out[1]: <pre>4</pre> In\u00a0[2]: Copied! <pre># True\n</pre> # True"}, {"location": "data_science/statistics/intro/#introduction-to-statistics", "title": "Introduction to statistics\u00b6", "text": ""}, {"location": "data_science/statistics/intro/#hello-2", "title": "Hello 2\u00b6", "text": ""}, {"location": "mlops/continous_delivery/", "title": "Continous integration", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#continuous-integration", "title": "Continuous Integration", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#introduction", "title": "Introduction", "text": "<p>What is continuous integration? It is the ability to know whether your code  works.</p> <p>Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#benefits", "title": "Benefits", "text": "<p>CI provides numerous benefits, including early identification of integration issues, faster software release cycles, and improved code quality.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#principles", "title": "Principles", "text": "<p>The main principles of CI include:</p> <ol> <li>Maintain a Single Source Repository: All code and resources are stored    in a version-controlled source repository.</li> <li>Automate the Build: Building, testing, and packaging processes are    automated.</li> <li>Make Your Build Self-Testing: Every commit triggers a build and test    process.</li> <li>Every Commit Should Build the Main Branch: Developers integrate their    changes with the main branch regularly.</li> <li>Keep the Build Fast: The build process is designed to be fast to provide    quick feedback.</li> <li>Test in a Clone of the Production Environment: Use a copy of the    production environment for testing.</li> <li>Make it Easy to Get the Latest Deliverables: Builds are available for    testing as soon as they pass.</li> <li>Everyone can See What's Happening: Transparency on the build progress    and results is crucial.</li> <li>Automate Deployment: Deployment to production or staging environments    is automated.</li> </ol>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#tools", "title": "Tools", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#containers", "title": "Containers", "text": "<ul> <li>Docker</li> </ul>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#test", "title": "Test", "text": "<ul> <li>pytest</li> </ul>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#other-tools", "title": "Other tools", "text": "<ul> <li>Organize commands:<ul> <li>Makefile</li> </ul> </li> </ul> <p>Popular CI tools include Jenkins, Travis CI, CircleCI, and</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/mlops_intro/", "title": "Introduction", "text": "", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#intro-to-mlops", "title": "Intro to MLops", "text": "<p>MLOps, or Machine Learning Operations, is a discipline that merges the # MLOps:  Unifying Machine Learning System Development and Operation</p> <p>MLOps, or Machine Learning Operations, is a discipline that merges the  development (Dev) and operation (Ops) of machine learning (ML) systems. As   data science and ML continue to become key capabilities for solving complex    real-world problems, the practice of MLOps is gaining traction. MLOps aims     to promote automation and monitoring throughout the construction of ML      systems, including integration, testing, release, deployment, and       infrastructure management.</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-is-more-than-just-ml-code", "title": "MLOps is More than Just ML Code", "text": "<p>The complexity of real-world ML systems goes beyond the ML code. The required  elements surrounding the ML code comprise a vast and intricate system that   includes configuration, automation, data collection, data verification,    testing and debugging, resource management, model analysis, process and     metadata management, serving infrastructure, and monitoring. In other words,     only a small fraction of an ML system is composed of the ML code      itself.</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-a-marriage-of-devops-and-machine-learning", "title": "MLOps: A Marriage of DevOps and Machine Learning", "text": "<p>Just as DevOps principles have proved beneficial in the development and  operation of large-scale software systems, these principles are applicable to   ML systems as well. However, ML systems have distinct characteristics:</p> <ol> <li>Team skills: An ML project typically involves data scientists or ML     researchers who might not be experienced software engineers capable of      building production-class services.</li> <li>Development: ML is experimental in nature. Challenges arise in tracking     what worked and what didn't, and maintaining reproducibility while      maximizing code reusability.</li> <li>Testing: Testing an ML system is more complex than testing other    software systems. It requires data validation, trained model quality     evaluation, and model validation.</li> <li>Deployment: Deployment in ML systems often entails deploying a     multi-step pipeline to automatically retrain and deploy models.</li> <li>Production: ML models can experience performance degradation due to     constantly evolving data profiles, necessitating the tracking of summary      statistics of data and monitoring of the online performance of models.</li> </ol>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#developing-ml-models-steps-to-success", "title": "Developing ML Models: Steps to Success", "text": "<p>Although the precise steps may vary depending on the specific ML project, a  general process can be outlined for developing ML models:</p> <ol> <li>Understanding the problem: This involves defining the business problem,     identifying the ML task, and preparing the initial data.</li> <li>Data preparation: This step includes gathering, cleaning, and     transforming data for the ML model.</li> <li>Model building: This includes selecting the model, training it, and then     evaluating its performance.</li> <li>Model deployment: This involves deploying the model into a production environment.</li> <li>Monitoring and maintenance: This step involves monitoring the     performance of the model over time, retraining it as necessary, and      performing model updates.</li> </ol>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-hierarchy-of-needs", "title": "MLops Hierarchy of needs", "text": "<p>To reach the Mlops level you have to achieve a few steps before you can reach  the next level. You cannot, for example, have DataOps without first   implementing devops.</p> <p>You need to achieve one step of the bottom</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-maturity-levels", "title": "MLOps Maturity Levels", "text": "<p>there are several different phases of going from a crude working where you can   barely get things into production and things are error-prone and manual all    the way to a very sophisticated system that has really end-to-end automation     and uses the next-generation features.</p> <ol> <li>Initial step: Experimentation. Establish the experimentation environment.</li> <li>Repeatable: standardize your code, your repos, make sure that there's maybe     a platform that you're using that can actually deploy the solution.</li> <li>Reliable: Test, monitoring, data drift, model versions.</li> <li>Scalable: You're able to templatize and productionize a lot of different ML     solutions, not just one, but actually have a scalable system that you can      repeat over and over again</li> </ol> Level Description Highlights Technology 0 No MLOps <ul><li>Difficult to manage full machine learning model lifecycle</li><li>The teams are disparate and releases are painful</li><li>Most systems exist as 'black boxes,' little feedback during/post deployment</li><li>Manual builds and deployments</li><li>Manual testing of model and application</li><li>No centralized tracking of model performance</li><li>Training of model is manual</li></ul> 1 DevOps but no MLOps <ul><li>Releases are less painful than No MLOps, but rely on Data Team for every new model</li><li>Still limited feedback on how well a model performs in production</li><li>Difficult to trace/reproduce results</li></ul> <ul><li>Automated builds</li><li>Automated tests for application code</li></ul> 2 Automated Training <ul><li>Training environment is fully managed and traceable</li><li>Easy to reproduce model</li><li>Releases are manual, but low friction</li></ul> <ul><li>Automated model training</li><li>Centralized tracking of model training performance</li><li>Model management</li></ul> 3 Automated Model Deployment <ul><li>Releases are low friction and automatic</li><li>Full traceability from deployment back to original data</li><li>Entire environment managed: train &gt; test &gt; production</li></ul> <ul><li>Integrated A/B testing of model performance for deployment</li><li>Automated tests for all code</li><li>Centralized tracking of model training performance</li></ul> 4 Full MLOps Automated Operations <ul><li>Full system automated and easily monitored</li><li>Production systems are providing information on how to improve and, in some cases, automatically improve with new models</li><li>Approaching a zero-downtime system</li></ul> <ul><li>Automated model training and testing</li><li>Verbose, centralized metrics from deployed model</li></ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-0-no-mlops", "title": "Level 0: No MLOps", "text": "<ul> <li> <p>People: Data scientists, data engineers, and software engineers are    siloed and not in regular communications.</p> </li> <li> <p>Model Creation: Data is gathered manually, compute is likely not managed,    experiments aren't predictably tracked, and the end result may be a single     model file manually handed off.</p> </li> <li>Model Release: The release process is manual, and the scoring script may    be manually created well after experiments without version control.</li> <li>Application Integration: Heavily reliant on data scientist expertise to    implement and manual releases each time.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-1-devops-no-mlops", "title": "Level 1: DevOps no MLOps", "text": "<ul> <li> <p>People: Same as Level 0.</p> </li> <li> <p>Model Creation: Data pipeline gathers data automatically, but compute may    not be managed, and experiments aren't predictably tracked.</p> </li> <li>Model Release: Still a manual process, but the scoring script is likely    version controlled and is handed off to software engineers.</li> <li>Application Integration: Basic integration tests exist, but still heavily    reliant on data scientist expertise. However, releases are automated and     application code has unit tests.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-2-automated-training", "title": "Level 2: Automated Training", "text": "<ul> <li> <p>People: Data scientists work directly with data engineers to convert    experimentation code into repeatable scripts/jobs, while software engineers     remain siloed.</p> </li> <li> <p>Model Creation: Data pipeline gathers data automatically, compute is    managed, experiment results are tracked, and both training code and  resulting models are version controlled.</p> </li> <li>Model Release: Manual release, but the scoring script is version    controlled with tests and the release is managed by the software engineering     team.</li> <li>Application Integration: Same as Level 1.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-3-automated-model-deployment", "title": "Level 3: Automated Model Deployment", "text": "<ul> <li> <p>People: Data scientists and data engineers work together and also with    software engineers to manage inputs/outputs and automate model integration     into application code.</p> </li> <li> <p>Model Creation: Same as Level 2.</p> </li> <li>Model Release: Release is automatic and managed by a continuous delivery    (CI/CD) pipeline.</li> <li>Application Integration: Unit and integration tests exist for each model    release, and the process is less reliant on data scientist expertise.     Application code has unit/integration tests..</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-4-full-mlops-automated-retraining", "title": "Level 4: Full MLOps Automated Retraining", "text": "<ul> <li> <p>People: All roles work together, with software engineers implementing    post-deployment metrics gathering.</p> </li> <li> <p>Model Creation: Similar to Level 3, but retraining is triggered    automatically based on production metrics.</p> </li> <li>Model Release: Same as Level 3.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#references", "title": "References", "text": "<ul> <li>Google Mlops levels</li> <li>Microsoft Mlops levels</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/tools/docker_file/", "title": "Docker", "text": ""}, {"location": "mlops/tools/docker_file/#dockerizing-a-python-application", "title": "Dockerizing a Python Application", "text": "<p>This guide will walk you through the process of dockerizing a simple Python  application.</p>"}, {"location": "mlops/tools/docker_file/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>Docker installed on your machine.</li> <li>Basic knowledge of Python.</li> </ul>"}, {"location": "mlops/tools/docker_file/#step-1-create-a-python-application", "title": "Step 1: Create a Python Application", "text": "<p>First, let's create a simple Python application that we want to dockerize.  Let's call it <code>app.py</code>.</p> <pre><code>def main():\nprint('Hello, Docker')\nif __name__ == '__main__':\nmain()\n</code></pre> <p>This application simply prints out 'Hello, Docker!' when run.</p>"}, {"location": "mlops/tools/docker_file/#step-2-create-a-dockerfile", "title": "Step 2: Create a Dockerfile", "text": "<p>A Dockerfile is a script that contains collections of commands and instructions  to create a Docker image.</p> <p>In the same directory as your <code>app.py</code>, create a file named <code>Dockerfile</code> with the following content:</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9\n# Set the working directory in the container to /app\nWORKDIR /app\n# Add the current directory contents into the container at /app\nADD . /app\n\n# Run app.py when the container launches\nCMD ['python', 'app.py']\n</code></pre> <p>This Dockerfile starts with a Python 3.9 base image, sets the working directory  to /app, copies the current directory into the container, and finally runs the   <code>app.py</code> script.</p>"}, {"location": "mlops/tools/docker_file/#step-3-build-the-docker-image", "title": "Step 3: Build the Docker Image", "text": "<p>Now, you can build the Docker image from the Dockerfile. Run the following  command in the same directory as your Dockerfile:</p> <pre><code>docker build -t python-docker-demo .\n</code></pre> <p>This tells Docker to build an image from the Dockerfile and tag it with the  name <code>python-docker-demo</code>.</p>"}, {"location": "mlops/tools/docker_file/#step-4-run-the-docker-container", "title": "Step 4: Run the Docker Container", "text": "<p>After the Docker image is built, you can run the Docker container with the  following command:</p> <pre><code>docker run python-docker-demo\n</code></pre> <p>You should see 'Hello, Docker!' printed to your console.</p>"}, {"location": "mlops/tools/docker_file/#conclusion", "title": "Conclusion", "text": "<p>Congratulations! You have just dockerized a Python application. Docker allows  you to package your applications with all of their dependencies into a   standardized unit for software development, making your applications more    reliable and easier to share and deploy.</p>"}, {"location": "mlops/tools/github_actions/", "title": "Github actions", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#github-actions", "title": "GitHub Actions", "text": "<p>In this guide, we'll go through the steps to create a continuous integration  (CI) workflow for a Python package using GitHub Actions. This will   automatically test your Python package each time you push a commit to your repository.</p> <pre><code>.\n\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 workflows\n\u2502   \u2502   \u2514\u2500\u2500 workflow1.yml\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n</code></pre>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-1-create-a-workflow-file", "title": "Step 1: Create a Workflow File", "text": "<p>In your GitHub repository, create a new file in the <code>.github/workflows</code>  directory. You can name it anything you like, but it must end in <code>.yml</code>  or <code>.yaml</code>. For this example, let's name it <code>python-package.yml</code>.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-2-set-up-the-workflow", "title": "Step 2: Set up the Workflow", "text": "<p>Open <code>python-package.yml</code> and let's set up the workflow. Here's a simple  starting point:</p> <pre><code>name: Python package\non:\npush:\nbranches: [ master ]\npull_request:\nbranches: [ master ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\npython-version: [3.6, 3.7, 3.8, 3.9, 3.10]\n</code></pre> <p>This configures the workflow to run on <code>push</code> and <code>pull_request</code> events to the  <code>master</code> branch. It runs on the <code>ubuntu-latest</code> GitHub-hosted runner and tests   against Python 3.6, 3.7, 3.8, 3.9, and 3.10.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-3-configure-the-workflow-steps", "title": "Step 3: Configure the Workflow Steps", "text": "<p>We will now add the steps that the workflow will follow. Below the  <code>python-version</code> line, add:</p> <pre><code>    steps:\n- uses: actions/checkout@v2\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v2\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n- name: Lint with flake8\nrun: |\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127\nchars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n- name: Test with pytest\nrun: |\npytest\n</code></pre> <p>This checks out your repository, sets up Python, installs dependencies, and  then runs <code>flake8</code> for linting and <code>pytest</code> for running tests.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-4-commit-and-push", "title": "Step 4: Commit and Push", "text": "<p>Once you're done, commit the <code>python-package.yml</code> file and push it to your  GitHub repository. GitHub Actions will start running the workflow on your next   push.</p> <p>Remember, you can always modify and expand this workflow to suit your specific  needs. For more information, check out the GitHub Actions documentation.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/makefile/", "title": "Makefile", "text": ""}, {"location": "mlops/tools/makefile/#using-makefile-and-python-for-streamlined-development", "title": "Using Makefile and Python for Streamlined Development", "text": "<p>Makefiles are powerful tools for automating build processes and managing  dependencies in software development projects. When combined with Python, they   become even more versatile and efficient. In this article, we will explore    the benefits of using Makefiles and Python together and demonstrate how they     can streamline your development workflow.</p>"}, {"location": "mlops/tools/makefile/#what-is-a-makefile", "title": "What is a Makefile", "text": "<p>A Makefile is a simple text file that contains a set of rules and dependencies.  It is commonly used in Unix-based systems to automate the compilation and   execution of programs. Make, the build automation tool, reads the Makefile    and executes the necessary commands to build the target or targets specified.</p> <p>Makefiles are especially useful when working on projects with multiple source  files, libraries, or complex build processes. They provide a clear and concise   way to define the dependencies and actions needed to build and run your    project.</p>"}, {"location": "mlops/tools/makefile/#integrating-python-with-makefiles", "title": "Integrating Python with Makefiles", "text": "<p>Python is a popular programming language known for its simplicity and  readability. It offers a wide range of libraries and frameworks that   facilitate various tasks, from data analysis to web development. By combining    Python with Makefiles, you can leverage the strengths of both tools and     create a more efficient development workflow.</p> <p>One of the primary advantages of using Python in a Makefile is its ability to  automate repetitive tasks. Instead of manually executing commands or scripts,   you can define them as rules in the Makefile and let Make handle the    execution. This approach saves time and reduces the chance of errors.</p> <p>Let's look at an example to illustrate how Python and Makefiles work together . Suppose you are working on a Python project that requires installing  dependencies, running tests, and generating documentation. You can define the   following rules in your Makefile:</p> <pre><code>install:\npip install -r requirements.txt\n\ntest:\npytest tests/\n\ndocs:\npython generate_docs.py\n</code></pre> <p>In this example, the <code>install</code> rule installs the project dependencies by  running <code>pip install -r requirements.txt</code>. The <code>test</code> rule executes the test   suite using pytest, and the <code>docs</code> rule generates the project documentation    using a custom Python script.</p> <p>To run a specific rule, you simply type <code>make &lt;rule_name&gt;</code> in your terminal.  For example, <code>make test</code> will execute the tests defined in the <code>tests/</code> directory.</p>"}, {"location": "mlops/tools/makefile/#handling-dependencies-with-makefiles", "title": "Handling Dependencies with Makefiles", "text": "<p>One of the key features of Makefiles is their ability to handle dependencies  automatically. When a rule depends on certain files or other rules, Make   ensures that the dependencies are up to date before executing the rule. This    feature is invaluable in large projects with complex dependencies.</p> <p>Let's consider a scenario where your Python project depends on multiple source  files, and each file needs to be compiled before the final executable is   generated. You can define the following rule in your Makefile:</p> <pre><code>main: utils.o file1.o file2.o\ngcc -o main main.c utils.o file1.o file2.o\n\n%.o: %.c\ngcc -c $&lt;\n</code></pre> <p>In this example, the <code>main</code> rule depends on three object files: <code>utils.o</code>,  <code>file1.o</code>, and <code>file2.o</code>. The rule specifies the compilation command to   generate the <code>main</code> executable. The second rule, <code>%.o: %.c</code>, is a pattern    rule that compiles any <code>.c</code> file into an object file. The <code>$&lt;</code> placeholder     represents the source file name.</p> <p>When you execute <code>make main</code>, Make automatically checks if any of the object  files are missing or have been modified since the last build. If necessary, it   compiles the required source files and then proceeds to link them into the    <code>main</code> executable.</p> <p>By defining the dependencies accurately in your Makefile, you ensure that your  project is built correctly and efficiently, with only the necessary steps  being executed.</p>"}, {"location": "mlops/tools/makefile/#using-the-phony-target", "title": "Using the <code>.PHONY</code> Target", "text": "<p>In a Makefile, the <code>.PHONY</code> target is used to declare rules that do not  correspond to actual files. These rules are considered 'phony' because they   don't generate any output files with the same name. Instead, they execute a    series of commands or actions.</p> <p>The <code>.PHONY</code> target is typically used for defining rules that perform common  tasks such as cleaning the project directory, running tests, or building the   project. By declaring these rules as phony, you ensure that Make doesn't    confuse them with actual files and always executes the associated commands.</p> <p>For example, consider the following Makefile snippet:</p> <pre><code>.PHONY: clean test build\nclean:\nrm -rf build/\n\ntest:\npytest tests/\n\nbuild:\npython setup.py build\n</code></pre> <p>In this example, the <code>.PHONY</code> target is used to declare the rules <code>clean</code>,  <code>test</code>, and <code>build</code> as phony targets. When you run <code>make clean</code>, it executes   the command <code>rm -rf build/</code> to remove the build directory. Similarly,   <code>make test</code> runs the test suite using pytest, and <code>make build</code> builds the    project using the <code>setup.py</code> script.</p> <p>By using the <code>.PHONY</code> target, you ensure that Make always executes the  specified commands for these rules, regardless of whether there are files with   the same names.</p>"}, {"location": "mlops/tools/makefile/#including-environment-variables-in-makefile", "title": "Including Environment Variables in Makefile", "text": "<p>Makefiles allow you to include environment variables within their definitions,  which can be useful for setting configuration parameters or passing values to   the commands executed by Make.</p> <p>To define an environment variable in a Makefile, you can use the <code>export</code>  directive followed by the variable name and its value. Here's an example:</p> <pre><code>export MY_VARIABLE = my_value\n\nrule:\necho $(MY_VARIABLE)\n</code></pre> <p>In this example, the <code>MY_VARIABLE</code> environment variable is defined with the  value <code>'my_value'</code>. The <code>rule</code> target then uses the <code>echo</code> command to display   the value of <code>MY_VARIABLE</code>.</p> <p>Including a <code>.venv</code> File</p> <p>Sometimes, it is necessary to include environment variables defined in an  external file, such as a <code>.venv</code> file used for managing virtual environments.   You can achieve this by using the <code>include</code> directive in your Makefile.</p> <p>Assuming you have a <code>.venv</code> file with environment variable assignments like:</p> <pre><code>VAR1=value1\nVAR2=value2\n</code></pre> <p>You can include these variables in your Makefile as follows:</p> <pre><code>include .venv\nrule:\necho $(VAR1)\necho $(VAR2)\n</code></pre> <p>In this example, the <code>.venv</code> file is included in the Makefile, and its  variables (<code>VAR1</code> and <code>VAR2</code>) are accessible and can be used in the rules.</p>"}, {"location": "mlops/tools/makefile/#opening-a-browser-with-a-python-script", "title": "Opening a Browser with a Python Script", "text": "<p>If you want to open a browser from within a Python script, you can make use of  the <code>webbrowser</code> module. This module provides a high-level interface for   displaying web-based documents to users. Here's an example script that opens    a URL in the default web browser:</p> <pre><code>define BROWSER_PYSCRIPT\nimport os, webbrowser, sys\nfrom urllib.request import pathname2url\nwebbrowser.open('file://' + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\nBROWSER := poetry run python -c '$$BROWSER_PYSCRIPT'\n</code></pre> <p>In this script, the <code>webbrowser.open()</code> function is used to open the specified  URL in the default web browser. When you run the script, it will launch the   browser and navigate to the given URL.</p> <p>You can integrate this Python script into your Makefile by creating a rule that  executes the script. For example:</p> <pre><code>coverage:\npoetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage html --rcfile=pyproject.toml\n    $(BROWSER) htmlcov/index.html\n</code></pre> <p>Running <code>make open_browser</code> will execute the <code>open_browser.py</code></p>"}, {"location": "mlops/tools/makefile/#script-template", "title": "Script template", "text": "<pre><code>.PHONY: clean clean-build clean-pyc clean-test create-network db-start\nprecommit precommit_style pylint precommit_security test_check_coverage coverage\n install install_dev docs test_benchmark\n\ndefine BROWSER_PYSCRIPT # (1)\nimport os, webbrowser, sys\n\nfrom urllib.request import pathname2url\n\nwebbrowser.open('file://' + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\nBROWSER := poetry run python -c '$$BROWSER_PYSCRIPT'\nclean: clean-build clean-pyc clean-test\n\nclean-build:\n    rm -fr build/\n    rm -fr dist/\n    rm -fr .eggs/\n    find . -name '*.egg-info' -exec rm -fr {} +\n    find . -name '*.egg' -exec rm -f {} +\n\nclean-pyc:\n    find . -name '*.pyc' -exec rm -f {} +\n    find . -name '*.pyo' -exec rm -f {} +\n    find . -name '__pycache__' -exec rm -fr {} +\n\nclean-test:\n    rm -f .coverage\n    rm -fr htmlcov/\n    rm -fr .pytest_cache\n\ncompose-build:\n    docker compose -f docker-compose.yml build --no-cache\n\ncompose-up:\n    docker compose -f docker-compose.yml up\n\ncreate-network:\n    docker network create my-docker-network\n\ndb-start:\n    docker compose up -d postgres\n\ndocs:\n    poetry run mkdocs build\n\nclean-docs:\n    rm -fr site/\n\nprecommit:\n    poetry run pre-commit run -a\n\nprecommit_security:\n    SKIP=mypy,flakeheaven,black,isort,pycln,check-docstring-first,\n    check-case-conflict,trailing-whitespace,end-of-file-fixer,debug-statements,\n    check-ast,check-json,check-yaml,no-commit-to-branch poetry run pre-commit\n     run -a\n\npylint:\n    pylint --disable=R,C src/\n\nprecommit_style:\n    SKIP=detect-aws-credentials,detect-private-key,check-added-large-files,\n    bandit,no-commit-to-branch poetry run pre-commit run -a\n\ntest:\n    poetry run pytest -vvv --benchmark-skip\n\ntest_check_coverage:\n    # Check pyproject.toml configuration\npoetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage report --rcfile=pyproject.toml\n\ncoverage:\n    poetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage html --rcfile=pyproject.toml\n    $(BROWSER) htmlcov/index.html\n\ninstall: clean\n    poetry install --no-dev\n\ninstall_dev: clean\n    poetry install\n\ntest_benchmark:\n    poetry run pytest -vvv --benchmark-autosave --benchmark-only\n\ninit:\n    poetry install\n    poetry run ipython kernel install --name 'my_kernel_name' --user\n    dvc pull\n</code></pre> <ol> <li> This is the way to declare a function inside a Makefile</li> </ol>"}, {"location": "python/environments/", "title": "Virtual environments", "text": ""}, {"location": "python/environments/#virtualenv", "title": "virtualenv", "text": ""}, {"location": "python/environments/#create-an-environment", "title": "Create an environment", "text": "<pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>You can check the wheter the environment was activated or not doing:</p> <pre><code>which pip\n</code></pre> <p>And you should see something similar to:</p> <pre><code>usr/folder/.venv/bin/pip\n</code></pre>"}, {"location": "python/pytest/", "title": "Pytests", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#pytest", "title": "Pytest", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#test-layouts", "title": "Test layouts", "text": "<ul> <li>Directory layout starts with <code>tests</code></li> <li>From <code>tests</code> you can add anything like <code>unit</code>, <code>functional</code> or other meaningful   names like <code>database</code></li> <li>Files need to be pre-fixed with <code>test_</code></li> <li>Test functions need to be prefixed with <code>test_</code></li> <li>Test classes need to be prefixed with <code>Test</code></li> </ul>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#how-syntax-works", "title": "How syntax works", "text": "<p>Tests can be functions or classes</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#functions", "title": "Functions", "text": "<pre><code>def test_my_function():\nassert 1 == 1\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#classes", "title": "Classes", "text": "<p>Classes do not need inheritance:</p> <p></p> <pre><code># This function is here for convenience only, in a real-world scenario this function\n# would be elsewhere in a package\ndef str_to_int(string):\n'''\n    Parses a string number into an integer, optionally converting to a float\n    and rounding down.\n    You can pass '1.1' which returns 1\n    ['1'] -&gt; raises RuntimeError\n    '''\nerror_msg = 'Unable to convert to integer: '%s'' % str(string)\ntry:\ninteger = float(string.replace(',', '.'))\nexcept AttributeError:\n# this might be a integer already, so try to use it, otherwise raise\n# the original exception\nif isinstance(string, (int, float)):\ninteger = string\nelse:\nraise RuntimeError(error_msg)\nexcept (TypeError, ValueError):\nraise RuntimeError(error_msg)\nreturn int(integer)\n# When you create yout class test you have special methods\nclass TestStrToInt:\ndef setup_method(self):\nprint('\\nthis is setup')\ndef teardown_method(self):\nprint('\\nthis is teardown')\ndef setup_class(cls):\nprint('\\nthis is setup class')\ndef teardown_class(cls):\nprint('\\nthis is teardown class')\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\nassert result == 2\ndef test_round_down_lesser_half(self):\nresult = str_to_int('1.2')\nassert result == 2\n</code></pre> <p>That setup_class is executed before a test in a class and happens just once, and setup_method is executed before every test in the class.</p> <p>You can use these special methods to run code before all tests in a class or before each one.</p> <p>You can see the ouput here:</p> Ouptut example <pre><code>======================================= test session starts =======================================\nplatform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0\nrootdir: /home/coder/python-testing/notebooks/lesson2\ncollected 2 items\ntest-classes/test_classes.py FF                                                             [100%]\n============================================ FAILURES =============================================\n__________________________________ TestStrToInt.test_rounds_down __________________________________\nself = &lt;test_classes.TestStrToInt object at 0x7f9e8a8c8220&gt;\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest-classes/test_classes.py:44: AssertionError\n-------------------------------------- Captured stdout setup --------------------------------------\nthis is setup class\nthis is setup\n------------------------------------ Captured stdout teardown -------------------------------------\nthis is teardown\n____________________________ TestStrToInt.test_round_down_lesser_half _____________________________\nself = &lt;test_classes.TestStrToInt object at 0x7f9e8a8c8340&gt;\ndef test_round_down_lesser_half(self):\nresult = str_to_int('1.2')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest-classes/test_classes.py:48: AssertionError\n-------------------------------------- Captured stdout setup --------------------------------------\nthis is setup\n------------------------------------ Captured stdout teardown -------------------------------------\nthis is teardown\nthis is teardown class\n===================================== short test summary info =====================================\nFAILED test-classes/test_classes.py::TestStrToInt::test_rounds_down - assert 1 == 2\nFAILED test-classes/test_classes.py::TestStrToInt::test_round_down_lesser_half - assert 1 == 2\n======================================== 2 failed in 0.02s ========================================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#run-tests", "title": "Run tests", "text": "<p>In the test directory</p> <pre><code>pytest -vvvv tests/\n</code></pre> Ouptut example <pre><code>============================= test session starts ==============================\nplatform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\ncachedir: .pytest_cache\nrootdir: /content, inifile:\ncollecting 0 items\ncollecting 2 items\ncollecting 2 items\ncollecting 2 items\ncollected 2 items\ntest_util.py::TestFloats::test_rounds_down FAILED                        [ 50%]\ntest_util.py::TestFloats::test_round_down_lesser_half FAILED             [100%]\n=================================== FAILURES ===================================\n_________________________ TestFloats.test_rounds_down __________________________\nself = &lt;test_util.TestFloats instance at 0x7fbf26d90870&gt;\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest_util.py:42: AssertionError\nshow more (open the raw output data in a text editor) ...\nthis is teardown\nthis is teardown class\n=========================== 2 failed in 0.04 seconds ===========================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#testing-failures", "title": "Testing failures", "text": "<p>Enter to the python debugger where your code is failing:</p> <pre><code>pytest --pdb test_failure_output.py\n</code></pre> <p>Once entered in the debugger you can type <code>h</code> to see the commands that you can use.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#another-commands-for-pytest", "title": "Another commands for pytest", "text": "<ul> <li><code>--collect-only</code> -&gt; Only collect tests, don't execute them</li> <li><code>-x</code> -&gt; Stop at the first failure</li> </ul> <p>To see all type:</p> <pre><code>pytest --help\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#plugins", "title": "Plugins", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#pytest-xdist", "title": "pytest-xdist", "text": "<p>Gives you the ability to run instance for running your test using the <code>-n</code> cli parameter.</p> <pre><code>pytest -n 4 test/\n</code></pre> <p>Going to set 4 differents runner instances and run them at the same time.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#nbval", "title": "nbval", "text": "<p>The plugin adds functionality to py.test to recognise and collect Jupyter  notebooks. The intended purpose of the tests is to determine whether execution   of the stored inputs match the stored outputs of the .ipynb file. Whilst also    ensuring that the notebooks are running without errors.</p> <p>The tests were designed to ensure that Jupyter notebooks (especially those for  reference and documentation), are executing consistently.</p> <p>Each cell is taken as a test, a cell that doesn't reproduce the expected output  will fail.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#installation", "title": "Installation", "text": "<pre><code>pip install nbval\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#usage", "title": "Usage", "text": "<pre><code>python -m pytest --nbval notebooks/\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#other-functionalities", "title": "Other functionalities", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#parametrize-tests", "title": "Parametrize tests", "text": "<p>Parametrize tests it's like put a for loop over the tests that you want to expect  the same result using the same function. The problem with plain for loops it's  that the output it's a little bit confused. You don't know really where the error  is located and if the rest of the loop it's going to be cover. So a good  option it's parametrize tests.</p> str_to_bool function: <pre><code>  def str_to_bool(val):\n'''\n    Convert a string representation of truth to True or False\n    True values are 'y', 'yes', or ''; case-insensitive\n    False values are 'n', or 'no'; case-insensitive\n    Raises ValueError if 'val' is anything else.\n    '''\ntrue_vals = ['yes', 'y', '']\nfalse_vals = ['no', 'n']\ntry:\nval = val.lower()\nexcept AttributeError:\nval = str(val).lower()\nif val in true_vals:\nreturn True\nelif val in false_vals:\nreturn False\nelse:\nraise ValueError('Invalid input value: %s' % val)\n</code></pre> <pre><code> import pytest\nfrom src impport str_to_bool # function to convert string to bool\n@pytest.mark.parametrize('value', ['y', 'yes', ''])\ndef test_is_true(value):\nresult = str_to_bool(value)\nassert result is True\n</code></pre> Example output: <pre><code>======================================= test session starts =======================================\nplatform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0\nrootdir: /home/coder/python-testing/notebooks/lesson2\ncollected 3 items\nparametrize/test_utils.py ...                                                               [100%]\n======================================== 3 passed in 0.01s ========================================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixtures", "title": "Fixtures", "text": "<p>In pytest, fixtures are a way to provide data or test-doubles (mocks, stubs, etc) to your tests. They are created using the <code>@pytest.fixture</code> decorator and then injected into your tests as arguments. Fixtures are meant to simplify test setup and teardown code, and they help to make your tests more modular and scalable.</p> <p>Here's a basic example of how to use a fixture in pytest:</p> <pre><code>import pytest\n# Define a fixture\n@pytest.fixture\ndef my_fixture():\nreturn 'Hello, World!'\n# Use the fixture in a test\ndef test_hello(my_fixture):\nassert my_fixture == 'Hello, World!'\n</code></pre> <p>In this example, the <code>my_fixture</code> fixture is defined to return the string <code>'Hello, World!'</code>. Then, in the <code>test_hello</code> test, <code>my_fixture</code> is injected as an argument. When pytest runs this test, it first calls the my_fixture fixture function and then passes its return value to <code>test_hello</code>.</p> <p>Here's a more complex example where a fixture is used for setup and teardown:</p> <pre><code>import pytest\n# Define a fixture\n@pytest.fixture\ndef database():\ndb = setup_database()  # Setup code\nyield db  # This is what will be injected into your tests\nteardown_database(db)  # Teardown code\n# Use the fixture in a test\ndef test_db(database):\nassert database.is_connected()\n</code></pre> <p>In this example, the <code>database</code> fixture is used to manage a database connection. The setup_database function is called to establish the connection, and then the connection object is yielded to the test. After the test runs, the <code>teardown_database</code> function is called to clean up the connection.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixture-scopes", "title": "fixture scopes", "text": "<p>Fixture scope determines when a fixture is set up and torn down. The possible  scopes are function, class, module, package or session:</p> <ul> <li><code>function</code>: The default scope, the fixture is set up and torn down for each    test function.</li> <li><code>class</code>: The fixture is set up and torn down for each test class.</li> <li><code>module</code>: The fixture is set up and torn down once per test module.</li> <li><code>package</code>: The fixture is set up and torn down once per test package.</li> <li><code>session</code>: The fixture is set up once when the test session starts, and is    torn down once at the end of the test session.</li> </ul> <pre><code>import pytest\n@pytest.fixture(scope='module')\ndef module_fixture():\n# Setup code here\nyield 'Hello, Module!'\n# Teardown code here\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixture-dependencies", "title": "Fixture dependencies", "text": "<p>Fixtures can use other fixtures. This is often useful when you want to  modularize your fixtures for reuse and better organization.</p> <pre><code>import pytest\n@pytest.fixture\ndef order():\nreturn {'name': 'Burger', 'price': 7.99}\n@pytest.fixture\ndef cart(order):\nreturn [order]\ndef test_cart(cart):\nassert len(cart) == 1\n````\n#### conftest\nThe conftest.py file serves as a means of providing fixtures for an entire directory\nof tests. Any fixture defined in conftest.py will be automatically available to all\ntest files in the same directory and subdirectories.\n```python\n# conftest.py\nimport pytest\n@pytest.fixture\ndef my_fixture():\nreturn 'Available Everywhere'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#temporal-directories", "title": "temporal directories", "text": "<p>The tmpdir fixture is a built-in pytest fixture that creates a temporary directory unique to the test invocation, which is automatically cleaned up after the test.</p> <pre><code>class TestMyClass:\ndef test_write_Yes(self, tmpdir):\npath = str(tmpdir.join('test_value'))\nwrite_integer('Yes', path)\nwith open(path, 'r') as _f:\nvalue = _f.read()\nassert value == 'True'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#monkeypatch", "title": "Monkeypatch", "text": "<p>The monkeypatch fixture helps to safely set/delete an attribute, dictionary item or environment variable or to modify sys.path for importing.</p> <pre><code>def test_monkeypatch(monkeypatch):\nresult = {'HELLO': 'world'}\nmonkeypatch.setenv('HELLO', 'monkeypatched')\nassert result['HELLO'] == 'monkeypatched'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/apis/best_practices/", "title": "Best practices", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#api-best-practices-with-fastapi-in-python", "title": "API Best Practices with FastAPI in Python", "text": "<p>Building APIs is a common task for backend developers, and they serve as the backbone of many modern web and mobile applications. However, designing and building an API can be a complex process, and it's important to follow best practices to ensure the resulting API is robust, reliable, and easy to use. This article presents API best practices with specific examples using FastAPI, a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#1-design-first-code-later", "title": "1. Design First, Code Later", "text": "<p>It's important to have a clear plan before starting to code your API. This means having a detailed specification of your API, including the routes, methods, parameters, and expected responses. You can create such a specification using the OpenAPI standard, which FastAPI supports out of the box.</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n@app.get('/items/{item_id}')\nasync def read_item(item_id: int):\nreturn {'item_id': item_id}\n</code></pre> <p>In the above example, the OpenAPI schema is automatically generated and can be accessed at the <code>/docs</code> endpoint.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#2-consistent-and-restful-routing", "title": "2. Consistent and RESTful Routing", "text": "<p>Make sure your API routes are consistent and follow RESTful principles. This means using the correct HTTP methods (GET, POST, PUT, DELETE, etc.) and having meaningful, predictable URLs. FastAPI makes it easy to define these with Python decorators.</p> <pre><code>@app.get('/users/{user_id}')\nasync def read_user(user_id: int):\n# code to get user\n@app.post('/users/')\nasync def create_user(user: User):\n# code to create user\n</code></pre> HTTP Method Description Idempotent Safe GET Retrieves the current state of a resource. Read Only Yes Yes POST Creates a new resource. Write Only No No PUT Replaces the current state of a resource with a new state. Update existing Yes No PATCH Applies partial modifications to a resource. No No DELETE Deletes a resource. Yes No HEAD Similar to GET but only retrieves the headers of a response. DOes it exist? Yes Yes OPTIONS Returns the HTTP methods that the server supports for the specified URL. Yes Yes <ul> <li>Idempotent means that multiple identical requests should have the same effect    as a single request.</li> <li>Safe means that the method only retrieves data and does not modify it.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#3-use-request-validation", "title": "3. Use Request Validation", "text": "<p>FastAPI supports request validation using Pydantic models, which allow you to define the expected shape of the data using Python type hints. This can significantly reduce the amount of boilerplate validation code you need to write.</p> <pre><code>from pydantic import BaseModel\nclass Item(BaseModel):\nname: str\ndescription: str\nprice: float\ntax: float = None\n@app.post('/items/')\nasync def create_item(item: Item):\n# code to create item\n</code></pre> <p>In this example, FastAPI will automatically validate that the incoming request data matches the <code>Item</code> model, and if it doesn't, it will return a helpful error message.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#4-error-handling", "title": "4. Error Handling", "text": "<p>You should anticipate and handle errors gracefully in your API. FastAPI provides the HTTPException class which you can raise to return a specific HTTP status code and message.</p> <pre><code>from fastapi import HTTPException\n@app.get('/items/{item_id}')\nasync def read_item(item_id: int):\nitem = get_item(item_id)\nif not item:\nraise HTTPException(status_code=404, detail='Item not found')\nreturn item\n</code></pre> Error Type HTTP Status Code Description Bad Request 400 The server could not understand the request due to invalid syntax. Unauthorized 401 The client must authenticate itself to get the requested response. Forbidden 403 The client does not have access rights to the content; that is, it is unauthorized, so server is rejecting to give proper response. Not Found 404 The server can not find the requested resource. Method Not Allowed 405 The method specified in the request is not allowed for the resource identified by the request URI. Conflict 409 This response is sent when a request conflicts with the current state of the server. Internal Server Error 500 The server has encountered a situation it doesn't know how to handle. Service Unavailable 503 The server is not ready to handle the request. Common causes are a server that is down for maintenance or that is overloaded.", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#5-rate-limiting", "title": "5. Rate Limiting", "text": "<p>Protect your API from abuse and overuse by implementing rate limiting. While  FastAPI doesn't have built-in support for rate limiting, you can use   third-party libraries such as SlowApi.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#6-use-asynchronous-code", "title": "6. Use Asynchronous Code", "text": "<p>FastAPI supports asynchronous request handling using Python's async and await keywords. This can improve the performance of your API by allowing it to handle other requests while waiting for IO-bound tasks (like database queries) to complete.</p> <pre><code>@app.get('/items/')\nasync def read_items():\nitems = await get_all_items()  # (1)\nreturn items\n</code></pre> <ol> <li>An async function that gets all items from the database</li> </ol>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#7-logging-and-monitoring", "title": "7. Logging and Monitoring", "text": "<p>An essential aspect of maintaining and troubleshooting APIs is having robust logging and monitoring in place. Here are some best practices for logging and monitoring in FastAPI:</p> <ol> <li> <p>Use the standard library logging module: The logging module is a built-in    Python library that provides a flexible and powerful way to log messages from    your application. FastAPI has built-in support for the logging module, so    you can easily integrate it into your application\u200b1\u200b.</p> <pre><code>import logging\nlogger = logging.getLogger(__name__)\n@app.post('/items/')\nasync def create_item(item: Item):\nlogger.info(f'Creating item: {item.name}')\n# code to create item\n</code></pre> </li> <li> <p>Avoid using print() to log messages: print() does not provide the same level     of control and flexibility as other logging methods. Instead, use FastAPI\u2019s     built-in logger for more control over how messages are logged, including setting     log levels, adding contextual information, and formatting logs for easier readability\u200b1\u200b.</p> </li> <li> <p>Log as much information as possible: By logging detailed information, such as     the request URL, query parameters, headers, body, response status code, and more,     developers can easily pinpoint where an issue occurred and what caused it.     Use structured loggers to log data in a consistent format that can be easily     parsed by other tools\u200b1\u200b.</p> </li> <li> <p>Log exceptions: Logging exceptions allows developers to quickly identify and     debug errors in their applications. By logging exceptions, developers can     easily pinpoint where an issue occurred and what caused it. To log exceptions     with FastAPI, use a library like Python\u2019s built-in logging module\u200b1\u200b.</p> </li> <li> <p>Add context to your logs: Adding context to your logs helps you quickly identify     the source of an issue. By adding contextual information such as request and     response data, user IDs, or other relevant details, you can easily pinpoint     where the issue originated from\u200b1\u200b.</p> </li> <li> <p>Use a structured logger: Structured logging is a way of formatting log messages     so that they are easier to read and parse. FastAPI provides built-in support     for structured logging via its Logging middleware\u200b1\u200b.</p> </li> <li> <p>Configure your logger for production: Configuring your logger for production     ensures that the right information is being logged at the right time. This     includes setting up log levels so that only important messages are recorded,     and configuring the format of the logs so they are easy to read and interpret.     Additionally, ensure that sensitive data is not included in the logs, and     that access to the logs is restricted to authorized personnel\u200b1\u200b.</p> </li> <li> <p>Use an external service to store and analyze logs: Using an external service     to store logs is beneficial because it allows for more efficient log     management. Additionally, these services typically offer a range of features     such as real-time monitoring, alerting, and reporting capabilities\u200b1\u200b.</p> </li> </ol>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/", "title": "FastAPI", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#fastapi", "title": "FastApi", "text": "<p>FastAPI is a modern, fast (high-performance), web framework for building APIs with  Python 3.6+ based on standard Python type hints.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#installation", "title": "Installation", "text": "<p>To install FastAPI, you'll need a Python version of 3.6 or greater. You can install  it using pip:</p> <pre><code>pip install fastapi\npip install uvicorn\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#baseic-example", "title": "Baseic example", "text": "<p>Here's a basic example of a FastAPI application:</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n@app.get('/')\ndef read_root():\nreturn {'Hello': 'World'}\n</code></pre> <p>You can run the application using Uvicorn:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>This command refers to:</p> <ul> <li><code>uvicorn</code>: Python framework that allows us to run a python application.</li> <li><code>main</code>: the file main.py (the Python 'module').</li> <li><code>app</code>: the object created inside of main.py with the line app = FastAPI().</li> <li><code>--reload</code>: make the server restart after code changes. Only do this for development.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#path-parameters", "title": "Path parameters", "text": "<p>You can define path parameters by putting them in curly braces {} in the path  of the  route decorator:</p> <pre><code>@app.get('/items/{item_id}')\ndef read_item(item_id: int):\nreturn {'item_id': item_id}\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#query-parameters", "title": "Query Parameters", "text": "<p>If you want the client to send additional data, but not in the path, you can use  query parameters:</p> <pre><code>from typing import Optional\n@app.get('/items/')\ndef read_items(q: Optional[str] = None):\nif q:\nreturn {'item': q}\nreturn {'item': 'not found'}\n</code></pre> <p>In this case, <code>q</code> is an optional string query parameter.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#request-body", "title": "Request Body", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#type-hints", "title": "Type hints", "text": "<p>FastAPI automatically recognizes Python type hints in the function parameters:</p> <pre><code>from pydantic import BaseModel\nclass Item(BaseModel):\nname: str\ndescription: Optional[str] = None\nprice: float\ntax: Optional[float] = None\n@app.post('/items/')\ndef create_item(item: Item):\nreturn item\n</code></pre> <p>In this example, the item body request parameter is declared to be of type Item.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#example-script", "title": "Example script", "text": "<pre><code>from datetime import datetime\nfrom os.path import dirname, abspath, join\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse # for serving files\nfrom fastapi.staticfiles import StaticFiles # for serving static files\nfrom pydantic import BaseModel\ncurrent_dir = dirname(abspath(__file__)) # get the path of the current script\nstatic_path = join(current_dir, 'static')\u00a0\napp = FastAPI()\napp.mount('/ui', StaticFiles(directory=static_path), name='ui')\nclass Body(BaseModel):\nstrftime: str\n@app.get('/')\ndef root():\nhtml_path = join(static_path, 'index.html')\nreturn FileResponse(html_path)\n@app.post('/generate')\ndef generate(body: Body):\n'''\n    Generate the current time given a strftime template. For example:\n    '%Y-%m-%dT%H:%M:%S.%f'\n    '''\ntmpl = body.strftime or '%Y-%m-%dT%H:%M:%S.%f'\nreturn {'date': datetime.now().strftime(tmpl)}\n@app.post('/azure_cognitive')\ndef azure_cognitive(body: Body):\n'''\n    Put here your code to create an Azure Cognitive service endpoint!\n    '''\nreturn {'result': None} # Change None\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#why-use-uvicorn", "title": "Why use uvicorn", "text": "<p>Uvicorn is an ASGI (Asynchronous Server Gateway Interface) server that is used to  run your FastAPI application. The ASGI specification fills in the gap left by the     traditional WSGI servers used for synchronous Python web applications, and allows    for greater concurrency and the use of long-lived connections, which are required     for modern web applications that need to handle things like WebSockets and HTTP/2.</p> <p>Here are some of the features that Uvicorn provides when used with FastAPI:</p> <ul> <li> <p>Performance: Uvicorn is one of the fastest ASGI servers due to its minimal and   highly optimized code base. It's built on uvloop and httptools, which are   themselves very fast asynchronous I/O and HTTP parsing libraries, respectively.</p> </li> <li> <p>Concurrency: By supporting the ASGI specification, Uvicorn allows FastAPI applications to handle many connections concurrently. This is great for applications that need to handle long-lived connections, such as WebSocket connections, in addition to regular HTTP requests.</p> </li> <li> <p>Hot Reload: Uvicorn supports hot reloading, which means it can automatically restart the server whenever it detects changes to your source code. This is extremely useful during development.</p> </li> <li> <p>WebSockets and HTTP/2 Support: ASGI servers like Uvicorn can handle long-lived  connections, such as WebSocket connections, which can be used for real-time   communication between the server and the client. They also support HTTP/2,   which can provide performance benefits over HTTP/1.</p> </li> <li> <p>Integration with FastAPI: FastAPI is built to work seamlessly with Uvicorn and other ASGI servers. This means you can take full advantage of all the features provided by FastAPI and ASGI, while still getting the performance benefits of Uvicorn.</p> </li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/", "title": "Flask", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#flask", "title": "Flask", "text": "<p>Flask is a web framework for Python, it's lightweight and easy to understand.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#setup", "title": "Setup", "text": "<p>First, we need to install Flask. We can do this with pip:</p> <pre><code>pip install flask\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#you-first-flask-application", "title": "You first Flask application", "text": "<p>Here's a basic Flask application:</p> <pre><code>from flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello_world():\nreturn 'Hello, World!'\nif __name__ == '__main__':\napp.run()\n</code></pre> <p>Here's what this code does:</p> <ul> <li><code>from flask import Flask</code> imports the Flask module.</li> <li><code>app = Flask(__name__)</code> creates an instance of the Flask class for our application.</li> <li><code>@app.route('/')</code> is a decorator that tells Flask what URL should trigger the   function that follows.</li> <li><code>def hello_world():</code> defines a function that returns the string 'Hello, World!'</li> <li><code>if __name__ == '__main__': app.run()</code> runs the application on the local   development server.</li> </ul> <p>To run the application, save the above code in a file called app.py, then run  python app.py from your terminal. You should see output indicating that the  server is running, and you can visit http://localhost:5000 in your web browser   to view your application.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#run-flask-api", "title": "Run flask api", "text": "<p>You can now run your Flask app by using the following command in your terminal:</p> <pre><code>python app.py\n</code></pre> <p>Then, open your web browser and navigate to <code>http://127.0.0.1:5000/</code>. You should see the text 'Hello, World!' displayed.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#add-more-routes", "title": "Add more routes", "text": "<p>Flask allows you to add more decorators to create more routes. Here's how you can  create a new route:</p> <pre><code>@app.route('/about')\ndef about():\nreturn 'About Page'\n</code></pre> <p>Now, if you navigate to <code>http://127.0.0.1:5000/about</code>, you will see the text 'About Page'.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#script-example", "title": "Script example", "text": "<pre><code>from flask import Flask, abort\napp = Flask(__name__)\n@app.route('/') # / equals to the root of the website\ndef hello_world():\nreturn 'Hello, World!'\n@app.route('/error')\ndef error():\nabort(500, 'oooh some error!')\nif __name__ == '__main__':\napp.run(debug=True, port=8000, host='0.0.0.0')\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#llm-model-with-flask-roberta", "title": "LLM model with flask (RoBERTa)", "text": "<pre><code>from flask import Flask, request, jsonify\nimport torch\nimport numpy as np\nfrom transformers import RobertaTokenizer\nimport onnxruntime\napp = Flask(__name__)\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nsession = onnxruntime.InferenceSession('roberta-sequence-classification-9.onnx')\ndef to_numpy(tensor):\nreturn (\ntensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n)\n@app.route('/')\ndef home():\nreturn '&lt;h2&gt;RoBERTa sentiment analysis&lt;/h2&gt;'\n@app.route('/predict', methods=['POST'])\ndef predict():\ninput_ids = torch.tensor(\ntokenizer.encode(request.json[0], add_special_tokens=True)\n).unsqueeze(\n0\n)\ninputs = {session.get_inputs()[0].name: to_numpy(input_ids)}\nout = session.run(None, inputs)\nresult = np.argmax(out)\nreturn jsonify({'positive': bool(result)})\nif __name__ == '__main__':\napp.run(host='0.0.0.0', port=5000, debug=True)\n</code></pre> <ul> <li><code>roberta-sequence-classification-9.onnx</code> The model was donwloaded.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#send-a-post", "title": "Send a post", "text": "<pre><code>curl -X POST --header 'Content-Type: application/json'\\\n--data '['using curl is not to my liking']'\\\nhttp:/127.0.0.1:5000/predict\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/cli/argparse/", "title": "Argparse", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#argparse", "title": "Argparse", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#introduction-to-argparse-in-python", "title": "Introduction to <code>argparse</code> in Python", "text": "<p><code>argparse</code> is a powerful module in Python that provides a convenient way to parse command-line arguments and options. It simplifies the process of building command-line interfaces (CLIs) for your Python scripts or applications. In this tutorial, we will explore the basics of <code>argparse</code> and learn how to use it effectively.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#installation-of-argparse", "title": "Installation of argparse", "text": "<p><code>argparse</code> is included in the Python standard library, so there is no need for  additional installation.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#getting-started", "title": "Getting Started", "text": "<p>To start using <code>argparse</code>, you need to import the module:</p> <pre><code>import argparse\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#creating-a-parser", "title": "Creating a Parser", "text": "<p>The first step in using <code>argparse</code> is to create an ArgumentParser object, which will  handle the parsing of command-line arguments. You can create a parser by invoking  the <code>argparse.ArgumentParser()</code> constructor:</p> <pre><code>parser = argparse.ArgumentParser()\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#adding-arguments", "title": "Adding Arguments", "text": "<p>Once you have a parser, you can add arguments to it. An argument is defined by invoking the <code>add_argument()</code> method on the parser object. Here\\'s an example that adds a positional argument:</p> <pre><code>parser.add_argument('filename', help='name of the file to process')\n</code></pre> <p>In the example above, we added a positional argument called <code>filename</code> that represents the name of the file to be processed. The <code>help</code> parameter provides a description of the argument, which is displayed when the user requests help information.</p> <p>You can also add optional arguments using the <code>add_argument()</code> method. Here's an example that adds an optional argument called <code>--verbose</code>:</p> <pre><code>parser.add_argument(\n'--verbose', action='store_true', help='increase output verbosity'\n)\n</code></pre> <p>In the example above, we added a positional argument called filename that represents the name of the file to be processed. The help parameter provides a description of the argument, which is displayed when the user requests help information.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#parsing-arguments", "title": "Parsing Arguments", "text": "<p>After you have added the desired arguments, you need to parse the command-line arguments provided by the user. This is done by invoking the <code>parse_args()</code> method on the parser object:</p> <pre><code>args = parser.parse_args()\n</code></pre> <p>The <code>parse_args()</code> method returns an object containing the values of the parsed  arguments. You can access the values by using dot notation on the <code>args</code> object.   For example, to access the value of the <code>filename</code> argument:</p> <pre><code>print(args.filename)\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#putting-it-all-together", "title": "Putting It All Together", "text": "<p>Here's an example that combines the concepts discussed above:</p> <pre><code>import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('filename', help='name of the file to process')\nparser.add_argument('--verbose', action='store_true', help='increase output verbosity')\nargs = parser.parse_args()\nprint('Processing file:', args.filename)\nif args.verbose:\nprint('Verbose mode enabled.')\n</code></pre> <p>ssuming this script is saved as <code>script.py</code>, you can run it from the command line as follows:</p> <pre><code>python script.py myfile.txt\n</code></pre> <p>If you include the <code>--verbose</code> flag:</p> <pre><code>python script.py myfile.txt --verbose\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#example-script-of-argparse", "title": "Example script of argparse", "text": "<pre><code>import argparse\ndef greet_user(name, age=None, greeting='Hello', uppercase=False):\n# Modify greeting based on options\nif uppercase:\ngreeting = greeting.upper()\n# Greet the user\noutput = f'{greeting}, {name}!'\nif age:\noutput += f' You are {age} years old.'\nreturn output\nif __name__ == '__main__':\n# Create a parser\nparser = argparse.ArgumentParser(description='Script to greet a user')\n# Add arguments\nparser.add_argument('name', help='name of the user')  # positional argument\nparser.add_argument('--age', type=int, help='age of the user')  # optional argument with type validation\nparser.add_argument('--greeting', choices=['Hello', 'Hi', 'Hola'], default='Hello',\nhelp='choose a greeting from the given options')  # optional argument with choices and default value\nparser.add_argument('--uppercase', action='store_true', help='convert greeting to uppercase')  # optional argument with flag\n# Parse the arguments\nargs = parser.parse_args()\n# Call the greet_user function with the provided arguments\noutput = greet_user(args.name, args.age, args.greeting, args.uppercase)\n# Print the greeting\nprint(output)\n</code></pre> <p>Clarifications:     * When an argument has a action, by default is <code>False</code> until you put it in the     call to enable it, in this case it is <code>--sort</code>argument.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/", "title": "Click", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#click-command-line-python-framework-tutorial", "title": "Click Command Line Python Framework Tutorial", "text": "<p>Click is a Python package that allows for the creation of beautiful command line interfaces (CLI) in a composable way. It's perfect for building command line applications and supports lazy argument parsing.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#installation", "title": "Installation", "text": "<p>First, we need to install Click. You can do this using pip:</p> <pre><code>pip install click\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#a-basic-click-command", "title": "A Basic Click Command", "text": "<p>Here is an example of a simple Click command:</p> <pre><code>import click\n@click.command()\ndef hello():\nclick.echo('Hello Click!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this script,<code>@click.command()</code> is a decorator which tells Click that this +\u00a1 function is a command line command. <code>click.echo</code> is a function that prints text to the console.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#command-with-arguments", "title": "Command with Arguments", "text": "<p>We can add arguments to our Click commands as follows:</p> <pre><code>import click\n@click.command()\n@click.argument('name')\ndef hello(name):\nclick.echo(f'Hello {name}!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this example, <code>@click.argument('name')</code> is another decorator which adds an argument to our command. Now, when you run the command, you need to provide an additional piece of information.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#command-with-options", "title": "Command with Options", "text": "<p>Options are like arguments, but they are optional and have default values:</p> <pre><code>import click\n@click.command()\n@click.option('--greeting', default='Hello', help='Change the greeting.')\n@click.argument('name')\ndef hello(greeting, name):\nclick.echo(f'{greeting} {name}!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this example, <code>@click.option('--greeting', default='Hello', help='Change the greeting.')</code> is an option. You can provide it when you run the command, or you can leave it out to use the default value.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#grouping-commands", "title": "Grouping commands", "text": "<p>You can group multiple commands together:</p> <pre><code>import click\n@click.group()\ndef cli():\npass\n@cli.command()\ndef initdb():\nclick.echo('Initialized the database')\n@cli.command()\ndef dropdb():\nclick.echo('Dropped the database')\nif __name__ == '__main__':\ncli()\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#example-click-script", "title": "Example click script", "text": "<pre><code>import click\n@click.group()\ndef cli():\n'Example CLI group. This is a Click command-line interface.'\npass\n@cli.command()\n@click.argument('name', nargs=1)\ndef greet(name):\n'This command greets NAME.'\nclick.echo(f'Hello, {name}!')\n@cli.command()\n@click.argument('name', nargs=1)\n@click.option('--count', default=1, help='Number of greetings.')\n@click.option('--greeting', '-g', default='Hello', help='Customize the greeting.')\ndef repeat(name, count, greeting):\n'This command repeats a greeting to NAME, COUNT times.'\nfor _ in range(count):\nclick.echo(f'{greeting}, {name}!')\n@cli.group()\ndef maths():\n'Maths commands group.'\npass\n@maths.command()\n@click.argument('numbers', nargs=-1, type=int)\ndef sum(numbers):\n'This command sums up NUMBERS.'\nclick.echo(f'Sum: {sum(numbers)}')\n@maths.command()\n@click.argument('numbers', nargs=-1, type=int)\ndef multiply(numbers):\n'This command multiplies NUMBERS.'\nresult = 1\nfor num in numbers:\nresult *= num\nclick.echo(f'Product: {result}')\n@cli.command()\n@click.argument('filepath', type=click.Path(exists=True))\ndef showfile(filepath):\n'This command shows the content of a FILEPATH.'\nwith open(filepath, 'r') as file:\ncontent = file.read()\nclick.echo(content)\nif __name__ == '__main__':\ncli()\n</code></pre> <p>This script provides a command-line interface with a <code>greet</code> command that accepts one argument (a name to greet), a <code>repeat</code> command that accepts a name and optional count and greeting options (to repeat the greeting a certain number of times), and a <code>maths</code> group with <code>sum</code> and <code>multiply</code> commands that perform mathematical operations on a list of numbers.</p> <p>You can run these commands like so:</p> <pre><code>python myscript.py greet Bob\npython myscript.py repeat Alice --count 3 --greeting Hi\npython myscript.py maths sum 1 2 3 4 5\npython myscript.py maths multiply 1 2 3 4 5\npython myscript.py showfile /path/to/file\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#argument-vs-option", "title": "Argument vs Option", "text": "<p>In the context of command-line interfaces, the terms 'argument' and 'option' have specific meanings:</p> <ul> <li> <p>Argument: An argument is a value that is passed directly to a command. Arguments   are positional, meaning that their order matters. They are typically mandatory,    although some commands may allow optional arguments. For example, in the     command <code>cp source.txt dest.txt</code>, <code>source.txt</code> and <code>dest.txt</code> are arguments.</p> </li> <li> <p>Option: An option modifies the behavior of a command. It is usually prefixed    by a - (single dash for single-character options) or -- (double dash for    multi-character options). Options may or may not require a value to be provided.     For example, in the command <code>ls -l</code>, <code>-l</code> is an option that modifies the behavior     of the ls command.</p> </li> </ul> <p>In the context of the Click library in Python:</p> <ul> <li> <p>Argument: Click treats arguments similarly to how they are treated in     command-line interfaces. They are positional and are defined using the     <code>@click.argument()</code> decorator.</p> </li> <li> <p>Option: Click treats options as modifiers of commands. They are defined using     the <code>@click.option()</code> decorator, and can have default values. Options in Click     are always optional, and if not provided in the command line, the default value     is used.</p> </li> </ul> <p>Here's an example of a command in Click that uses both arguments and options:</p> <pre><code>@click.command()\n@click.argument('filename')\n@click.option('--verbose', is_flag=True, help='Enable verbose mode.')\ndef process(filename, verbose):\n'''Process a file. If --verbose is provided, print detailed information.'''\nif verbose:\nclick.echo(f'Processing file in verbose mode: {filename}')\nelse:\nclick.echo(f'Processing file: {filename}')\n</code></pre> <p>In this example, filename is an argument, and <code>--verbose</code> is an option. The <code>is_flag=True</code> argument to <code>@click.option()</code> means that <code>--verbose</code> is a flag that does not take a value; its presence in the command line sets verbose to True.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#create-an-unique-unique-entrypoint-in-click", "title": "Create an unique unique entrypoint in click", "text": "<p>If you have multiple command groups defined across different files, you can import them into a main file and create an entry point for your application. This way, you can keep your command groups logically separated, which can be beneficial as your application grows.</p> <p>Here's an example:</p> <p>Let's say you have two Python files, <code>file1.py</code> and <code>file2.py</code>, with different c ommand groups.</p> <p><code>file1.py</code>:</p> <pre><code>import click\n@click.group()\ndef group1():\n'''This is group1 commands.'''\npass\n@group1.command()\ndef command1():\nclick.echo('Executing command1 from group1.')\n@group1.command()\ndef command2():\nclick.echo('Executing command2 from group1.')\n</code></pre> <p><code>file2.py</code>:</p> <pre><code>import click\n@click.group()\ndef group2():\n'''This is group2 commands.'''\npass\n@group2.command()\ndef command1():\nclick.echo('Executing command1 from group2.')\n@group2.command()\ndef command2():\nclick.echo('Executing command2 from group2.')\n</code></pre> <p>You can then create an entry point in a main file (for example, main.py) that imports these groups and adds them to the main command group:</p> <p><code>main.py</code>:</p> <pre><code>import click\nfrom file1 import group1\nfrom file2 import group2\n@click.group()\ndef cli():\n'''This is the main entry point.'''\npass\ncli.add_command(group1)\ncli.add_command(group2)\nif __name__ == '__main__':\ncli()\n</code></pre> <p>Now, when you run <code>python main.py</code>, you'll have access to both command groups, <code>group1</code> and <code>group2</code>, and all their respective commands. The commands can be accessed like this:</p> <pre><code>python main.py group1 command1\n</code></pre> <p>Please note that the import statements in <code>main.py</code> assume that <code>file1.py</code> and <code>file2.py</code> are in the same directory as <code>main.py</code>. If they're not, you'll need to adjust the import statements to match your directory structure.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/nlp/huggingface/", "title": "HuggingFace", "text": ""}, {"location": "python/nlp/huggingface/#hugginface", "title": "Hugginface", "text": ""}, {"location": "python/nlp/huggingface/#hugginface-cli", "title": "Hugginface CLI", "text": "<p>Hugging Face, primarily known for its advancements in natural language  processing, offers a robust Command Line Interface (CLI) that streamlines the   usage of its models and datasets. This guide provides a comprehensive    overview of installing the Hugging Face CLI and creating a project, drawing     parallels to GitHub's functionality. Link</p>"}, {"location": "python/nlp/huggingface/#cli-installation", "title": "CLI Installation", "text": "<p>To begin, ensure you have Python installed on your system. Hugging Face CLI  requires Python 3.6 or later.</p> <p>Step 1: Install the Hugging Face Hub</p> <p>Use pip to install the Hugging Face Hub:</p> <pre><code>pip install huggingface_hub\n</code></pre> <p>Step 2: Verify Installation Confirm the installation by checking the version:</p> <pre><code>huggingface-cli --version\n</code></pre>"}, {"location": "python/nlp/huggingface/#creating-a-project", "title": "Creating a Project", "text": "<p>Similar to GitHub, Hugging Face allows users to create and manage projects.  Here's how you can create a new project.</p> <p>Step 1: Log In First, log in to your Hugging Face account via the CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Enter your Hugging Face credentials when prompted.</p> <p>Step 2: Create a New Repository To create a new repository, use:</p> <pre><code>huggingface-cli repo create your-repo-name\n</code></pre> <p>Replace your-repo-name with the desired name for your repository.</p> <p>Step 3: Clone the Repository Clone your newly created repository:</p> <pre><code>git clone &lt;https://huggingface.co/username/your-repo-name&gt;\n</code></pre> <p>Replace username with your Hugging Face username and your-repo-name with the  repository name.</p>"}, {"location": "python/nlp/huggingface/#transformers", "title": "\ud83e\udd17 Transformers", "text": "<p>Load State-of-the-art Machine Learning models for PyTorch, TensorFlow, and JAX dynamically.</p> <p>\ud83e\udd17 Transformers provides APIs and tools to easily download and train state-of-the-art  pretrained models. Using pretrained models can reduce your compute costs, carbon   footprint, and save you the time and resources required to train a model from   scratch. These models support common tasks in different modalities, such as:</p> <p>\ud83d\udcdd Natural Language Processing: text classification, named entity recognition,  question answering, language modeling, summarization, translation, multiple  choice, and text generation. \ud83d\uddbc\ufe0f Computer Vision: image classification, object detection, and segmentation. \ud83d\udde3\ufe0f Audio: automatic speech recognition and audio classification. \ud83d\udc19 Multimodal: table question answering, optical character recognition, information  extraction from scanned documents, video classification, and visual question answering.</p> <p>\ud83e\udd17 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage  of a model\u2019s life; train a model in three lines of code in one framework, and  load it for inference in another. Models can also be exported to a format like  ONNX and TorchScript for deployment in production environments.</p>"}, {"location": "python/nlp/huggingface/#install-hugginface-tansformers", "title": "Install Hugginface tansformers", "text": "<p>Installation</p> Normal instalationpytorchtensorflow <pre><code>pip install transformers\n</code></pre> <pre><code>pip install 'transformers[torch]'\n</code></pre> <pre><code>pip install 'transformers[tf-cpu]'\n</code></pre>"}, {"location": "python/nlp/huggingface/#pipeline", "title": "Pipeline", "text": ""}, {"location": "python/nlp/huggingface/#datasets", "title": "Datasets", "text": "<p>\ud83e\udd17 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.</p> <p>Load a dataset in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature  a deep integration with the Hugging Face Hub, allowing you to easily load and   share a dataset with the wider machine learning community.</p> <p>Find your dataset today on the Hugging Face Hub,  and take an in-depth look inside of it with the live viewer.</p>"}, {"location": "python/nlp/huggingface/#installation", "title": "Installation", "text": "<pre><code>pip install datasets\n</code></pre>"}, {"location": "python/nlp/huggingface/#load-datasets", "title": "Load datasets", "text": "<pre><code>from datasets import load_dataset, list_datasets\navailable = list_datasets()\n# load the dataset dynamically\nmovie_rationales = load_dataset('movie_rationales)\n# the object is a dict-like mapping of actual datasets\ntrain = movie_rationales['train]\ndf = train.to_pandas()\n</code></pre>"}, {"location": "python/nlp/huggingface/#resources", "title": "Resources", "text": "<ul> <li>Fine tuning hugginface model</li> <li>MLops codespace template -&gt;    Create a github codespace with some utilities around cuda and hugginface.</li> </ul>"}, {"location": "python/packaging/setuptools/", "title": "Setuptools", "text": "", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#the-setuppy-python-file", "title": "The setup.py python file", "text": "<p><code>setup.py</code> is a Python file that provides information about a module/package that you have created. It's the build script for setuptools. It tells setuptools about your package (such as the name and version) as well as files to include.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#creating-a-basic-setuppy-file", "title": "Creating a Basic <code>setup.py</code> File", "text": "<p>Here's an example of a basic setup.py file:</p> <pre><code>from setuptools import setup, find_packages\nsetup(\nname='MyPackage',\nversion='0.1',\npackages=find_packages(),\n)\n</code></pre> <p>In this example, we're importing <code>setup</code> and <code>find_packages</code> from setuptools.  <code>setup</code> is the function that sets up your package, and <code>find_packages</code>  automatically discovers all packages and subpackages.</p> <p>The <code>name</code> argument is the name of your package, and <code>version</code> is the current version of your package.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#adding-more-information", "title": "Adding More Information", "text": "<p>We can add more information about our package in the setup.py file:</p> <pre><code>setup(\nname='MyPackage',\nversion='0.1',\npackages=find_packages(),\ndescription='A sample Python package',\n# you cand add this using a function from the requierements.txt\ninstall_requires = ['click==7.3.0', 'colorama'],\nentry_points='''\n    [console_scripts]\n    command1=src.main:command1\n    '''\nlong_description=open('README.txt').read(),\nauthor='Your Name',\nauthor_email='your.email@example.com',\nurl='http://example.com/MyPackage/',\nlicense='LICENSE.txt',\n)\n</code></pre> <p>Here, description provides a short description of the package. <code>long_description</code>  can be a detailed description, which we are reading from a file named <code>README.txt</code>.  <code>author</code> and <code>author_email</code> are self-explanatory. <code>url</code> is the URL for the homepage  of the package. <code>license</code> specifies the license under which the package is released.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#including-additional-files-manifestin", "title": "Including Additional Files - `MANIFEST.in", "text": "<p>To include additional files such as the README, you can use the <code>MANIFEST.in</code> file. Create a file named <code>MANIFEST.in</code> in the same directory as setup.py and list any additional files you want to include in the package:</p> <pre><code>include README.txt\ninclude LICENSE.txt\n</code></pre> <p>You can also specify this directly in the setup.py file:</p> <pre><code>setup(\n# \u2026\ninclude_package_data=True,\npackage_data={\n'': ['README.txt', 'LICENSE.txt'],\n},\n)\n</code></pre> <p>Here, <code>include_package_data=True</code> tells setuptools to include any data files specified in package_data or <code>MANIFEST.in.</code></p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#installing-the-package", "title": "Installing the Package", "text": "<p>Once you've written your <code>setup.py</code>, you can install your package using <code>pip</code>:</p> <pre><code>pip install .\n</code></pre>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#installing-editable", "title": "Installing editable", "text": "<p>This command installs the package in the current directory. If you want to install  the package in editable mode (i.e., changes to the source code are immediately  available without needing to reinstall the package), you can use:</p> PipPyhton <pre><code>pip install -e .\n</code></pre> <pre><code>python setup.py develop\n</code></pre>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#tutorials", "title": "Tutorials", "text": "<ul> <li>Setuptools official documentaion</li> </ul>", "tags": ["Development", "Packaging"]}]}