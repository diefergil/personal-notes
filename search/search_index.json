{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "Home", "text": ""}, {"location": "#welcome-to-mkdocs", "title": "Welcome to MkDocs", "text": "<p>For full documentation visit mkdocs.org.</p>"}, {"location": "#commands", "title": "Commands", "text": "<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"}, {"location": "#project-layout", "title": "Project layout", "text": "<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"}, {"location": "about/", "title": "About", "text": ""}, {"location": "about/#pendet-illud-mutilatae-certa-urgetque-populi-radiis", "title": "Pendet illud mutilatae certa urgetque populi radiis", "text": ""}, {"location": "about/#et-nostra-quod-quamvis-tamen", "title": "Et nostra quod quamvis tamen", "text": "<p>Lorem markdownum erat perque, colebat dedit; collo habuit relictis falcato et fide mente iugulatus intrat. Tenet orbe ignoscite Saturnia valeant ulla neque orant positus mea aspicit aegide loquentem animae postquam cecidere aras. Prosilit quod ignavo in crinis metus unda, di Stygias florebat lacrimantem vellent nunc, et undis moenia mecum?</p>"}, {"location": "about/#ferat-teli-exitio-acrisius-et-modo-veteres", "title": "Ferat teli exitio Acrisius et modo veteres", "text": "<p>Idas feram an esse paruerit feres; cadet tonitruque nostra femur ipse. Ut errat tenet magni ultra n\u00e9 signa, sub, obstantis legit non, auctor.</p> <ul> <li>Haec quondam relaxant litora auxiliaria ferro Ampycides</li> <li>Corda sceleri</li> <li>Furiisque stimulis domos quod per palla</li> <li>Ire responderat legit i qua frugum fuit</li> <li>Ignarus nepotem do gravis</li> </ul>"}, {"location": "about/#et-averna-cernimus-adsuetos-aiax-interea-perque", "title": "Et Averna cernimus adsuetos Aiax interea perque", "text": "<p>Mihi aversa ignisque flumina: miscet: ab deo sive avidisque. Ad veni deponendique pars interdum Byblis noctem, sed nostro, nec satis ignotissima. Subitus longis, faciemque amorem. Nube ilia opus vulnere mentis mihi sorores referam sperato, hos ignis possedit et invenit, mens saecula aetas comitesque.</p>"}, {"location": "about/#montibus-aurora-barba-achaide", "title": "Montibus Aurora barba Achaide", "text": "<p>Tutaque verumque monimenta clamata et pretium gemellos latratu Minoa aequore; puerum. Tartareas priori inscripta spretae sua, iactat adspice peregrinosque metallis expellitur. Duobus sed vigoris illa mutatus, multicavo animosa!</p> <ul> <li>Abdita laticesque lepores ferro sibi suam per</li> <li>Litora sub Cecropide</li> <li>Me vincula quod dabat flumen mensuraque secura</li> <li>Pulsavere cantus redeuntem peritura</li> <li>Et luctatusque aequantia caedis praesagia montis certamina</li> <li>Suos est lucem fine velox ubi nam</li> </ul> <p>Fessos animis custodit cumque, Priamidas, lucem mihi Pyrrha; namque. Ille nihil: illo ultor nisi materque sit sensit, Cyllenaeo opus.</p>"}, {"location": "hooks/", "title": "Hooks", "text": "In\u00a0[\u00a0]: Copied! <pre>import os\nimport shutil\n</pre> import os import shutil In\u00a0[\u00a0]: Copied! <pre>def copy_ads_txt(config, **kwargs):\n    site_dir = config['site_dir']\n    shutil.copy('theme/assets/ads.txt', os.path.join(site_dir, 'ads.txt'))\n</pre> def copy_ads_txt(config, **kwargs):     site_dir = config['site_dir']     shutil.copy('theme/assets/ads.txt', os.path.join(site_dir, 'ads.txt'))"}, {"location": "cloud/aws/app_runner/", "title": "App Runner", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#aws-app-runner", "title": "AWS App Runner", "text": "<p>App Runner is a fully managed service that makes it easy for developers to  quickly build, deploy, and run containerized applications. It was announced by   AWS (Amazon Web Services) in May 2021.</p> <p>Amazon App Runner is designed to simplify the process of deploying applications  in containers, making it easier for developers to focus on writing code rather   than managing infrastructure. The service automatically handles all the    operational aspects like building and running containers, scaling up or down     based on traffic, and monitoring application health.</p> <p>This guide will walk you through the process of deploying a containerized  FastAPI application using AWS App Runner.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#deploy-a-fastapi-app-in-app-runner", "title": "Deploy A FastApi app in App Runner", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>An AWS account.</li> <li>Docker installed on your machine.</li> <li>A FastAPI application to deploy.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-1-containerizing-your-fastapi-application", "title": "Step 1: Containerizing your FastAPI Application", "text": "<p>Once you have your Fastapi app builded, in this  example we are going to use a simple summatory function:</p> <pre><code># main.py\nfrom fastapi import FastAPI\nimport uvicorn\napp = FastAPI()\n@app.get('/')\nasync def root():\nreturn {'message': 'Hello Duke'}\n@app.get('/add/{num1}/{num2}')\nasync def add(num1: int, num2: int):\n'''Add two numbers together'''\ntotal = num1 + num2\nreturn {'total': total}\nif __name__ == '__main__':\nuvicorn.run(app, port=8080, host='0.0.0.0')\n</code></pre> <p>You can curl your app in local doing:</p> <pre><code>curl http:/0.0.0.0:8080/add/2/2\n</code></pre> <p>You'll need to create a Dockerfile in the root directory of your FastAPI  application. This file will instruct Docker on how to build a container for   your app.</p> <p>Here's a simple Dockerfile for a FastAPI app:</p> <pre><code>FROM public.ecr.aws/lambda/python:3.8 # (1)\nRUN mkdir -p /app\nCOPY . main.py /app/\nWORKDIR /app\nRUN pip install -r requirements.txt\nEXPOSE 8080\nCMD [ 'main.py' ]\nENTRYPOINT [ 'python', 'main.py' ]\n</code></pre> <ol> <li>The container that you need to run app runner.</li> </ol> <p>Then, build the Docker image with the following command:</p> <pre><code>docker build -t your-image-name .\n</code></pre>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-2-push-your-docker-image-to-amazon-ecr", "title": "Step 2: Push your Docker Image to Amazon ECR", "text": "<p>AWS App Runner needs to pull your Docker image from a registry. We'll use  Amazon's Elastic Container Registry (ECR).</p> <p>First, create a new repository in ECR:</p> <pre><code>aws ecr create-repository --repository-name your-repo-name\n</code></pre> <p>Next, authenticate Docker to your ECR registry:</p> <pre><code>aws ecr get-login-password --region your-region | docker login --username AWS\n--password-stdin your-ecr-url\n</code></pre> <p>Then, tag your image with the ECR repository:</p> <pre><code>docker tag your-image-name:latest your-ecr-url/your-repo-name:latest\n</code></pre> <p>Finally, push your image to ECR:</p> <pre><code>docker push your-ecr-url/your-repo-name:latest\n</code></pre>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#step-3-deploy-your-fastapi-app-using-aws-app-runner", "title": "Step 3: Deploy your FastAPI App using AWS App Runner", "text": "<p>Navigate to the AWS App Runner console and follow these steps:</p> <ol> <li>Click 'Create an App Runner service'.</li> <li>Select 'Source' as 'Container registry'.</li> <li>Enter the ECR image URI from the previous step.</li> <li>Configure the build settings and deployment settings as per your requirements.</li> <li>Click 'Create and Deploy'.</li> </ol> <p>You should now have your FastAPI application running on AWS App Runner!</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#conclusion", "title": "Conclusion", "text": "<p>AWS App Runner is a powerful service for deploying containerized applications.  With it, you can focus more on developing your FastAPI applications and less   on managing infrastructure.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/app_runner/#references", "title": "References", "text": "<ul> <li>Fastapi</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "App Runner"]}, {"location": "cloud/aws/batch/", "title": "Batch", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#aws-batch", "title": "AWS Batch", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#introduction", "title": "Introduction", "text": "<p>AWS Batch is a cloud service provided by Amazon Web Services (AWS) that enables  developers and scientists to easily and efficiently run hundreds of thousands   of batch computing jobs. AWS Batch dynamically provisions the optimal    quantity and type of compute resources based on the volume and specific     resource requirements of the batch jobs submitted.</p> <p>In the context of machine learning (ML), AWS Batch can be a powerful tool for  training models.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#what-is-aws-batch", "title": "What is AWS Batch?", "text": "<p>AWS Batch is a set of batch management capabilities that enables developers,  scientists, and engineers to easily and efficiently run hundreds of thousands   of batch computing jobs on AWS. AWS Batch dynamically provisions the optimal    quantity and type of compute resources (e.g., CPU or memory-optimized     instances) based on the volume and specific resource requirements of the      batch jobs submitted.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#why-use-aws-batch-for-machine-learning", "title": "Why Use AWS Batch for Machine Learning?", "text": "<p>Training machine learning models often involves running large-scale compute  jobs. These jobs can take a long time to complete and require significant   compute resources. AWS Batch is designed to handle this kind of workload    efficiently. Here are some reasons why AWS Batch is a good fit for ML model     training:</p> <ul> <li> <p>Scalability: AWS Batch can automatically scale up to handle large jobs    and scale down when resources are not needed, helping you to use resources     efficiently.</p> </li> <li> <p>Cost-Effectiveness: With AWS Batch, you pay only for the compute time you    consume. It also integrates with Spot Instances, allowing you to take     advantage of unused EC2 capacity at a significant discount.</p> </li> <li> <p>Integration with AWS Services: AWS Batch integrates with other AWS    services like Amazon S3, Amazon EC2, and AWS IAM, making it easier to set up     and manage your ML training jobs.</p> </li> <li> <p>Simplified Operations: AWS Batch removes the need to install and manage    batch computing software, allowing you to focus on analyzing results and     solving problems.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/batch/#getting-started-with-aws-batch-for-machine-learning", "title": "Getting Started with AWS Batch for Machine Learning", "text": "<p>Here are the general steps to use AWS Batch for training ML models:</p> <ol> <li> <p>Prepare Your Training Code: Write your ML model training code and     package it into a Docker container. This container will be the job that AWS      Batch runs.</p> </li> <li> <p>Upload Your Data: Upload your training data to a storage service like     Amazon S3.</p> </li> <li> <p>Create a Compute Environment: In the AWS Batch console, create a     compute environment that specifies the type of instances that you want to      use for your jobs.</p> </li> <li> <p>Create a Job Queue: Create a job queue that is associated with the     compute environment you created.</p> </li> <li> <p>Submit a Job: Submit a job to the job queue. In the job definition,     specify the Docker container with your training code and the location of      your training data.</p> </li> <li> <p>Monitor Your Job: Use the AWS Batch console or CloudWatch Logs to     monitor the progress of your job.</p> </li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Batch"]}, {"location": "cloud/aws/lambda/", "title": "Lambda functions", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#using-python-with-aws-lambda-a-comprehensive-guide", "title": "Using Python with AWS Lambda: A Comprehensive Guide", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#introduction", "title": "Introduction", "text": "<p>Amazon Web Services (AWS) Lambda is a serverless computing service that allows  you to run your applications without having to manage servers. It executes   your code only when required and scales automatically, from a few requests    per day to thousands per second. You only pay for the compute time you     consume - there is no charge when your code is not running.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#setting-up-aws-lambda", "title": "Setting Up AWS Lambda", "text": "<p>Before you can start using AWS Lambda with Python, you need to set up your AWS  environment. Here are the steps to do so:</p> <ol> <li>Create an IAM Role: AWS Identity and Access Management (IAM) roles are     used to grant permissions to your Lambda function. You can create an IAM      role from the AWS Management Console following the instructions in the       official AWS IAM User Guide.</li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#writing-python-code-for-aws-lambda", "title": "Writing Python Code for AWS Lambda", "text": "<p>Once you've set up your AWS environment, you can start writing Python code for  AWS Lambda. Here's a simple example of a Lambda function written in Python:</p> <pre><code>def lambda_handler(event, context):\n# print the event details\nprint('Received event: ' + str(event))\n# return a response\nreturn {\n'statusCode': 200,\n'body': 'Hello from Lambda!'\n}\n</code></pre> <p>In this example, <code>lambda_handler</code> is the entry point to your Lambda function.  AWS Lambda passes event data to this handler as the first parameter, and   context information as the second parameter.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#deploying-your-lambda-function", "title": "Deploying Your Lambda Function", "text": "<p>After writing your Python code, you need to deploy your Lambda function to the  AWS environment. Here are the steps to do so:</p> <ol> <li> <p>Package Your Code: Zip your code and any dependencies into a deployment     package. For Python, your deployment package can be as simple as a .zip     file containing your .py files.</p> </li> <li> <p>Create a Lambda Function: Go to the AWS Management Console, navigate to     the Lambda service, and click on 'Create function'. You can then provide      your function name, select Python as your runtime, and upload your .zip file.</p> </li> <li> <p>Test Your Function: After creating your function, you can test it by     clicking on 'Test' in the AWS Management Console. You can define a test      event and see the result of your function execution.</p> </li> </ol>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#monitoring-and-debugging", "title": "Monitoring and Debugging", "text": "<p>AWS provides several tools for monitoring and debugging your Lambda functions:</p> <ul> <li> <p>AWS CloudWatch: AWS CloudWatch allows you to collect and track metrics,    collect and monitor log files, and set alarms. You can use CloudWatch to     gain system-wide visibility into resource utilization, application      performance, and operational health.</p> </li> <li> <p>AWS X-Ray: AWS X-Ray helps you debug and analyze your microservices    applications with request tracing. You can use X-Ray to trace requests from     start to end and get a detailed view of the entire request path.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#handling-events-in-aws-lambda-functions-with-python", "title": "Handling Events in AWS Lambda Functions with Python", "text": "<p>AWS Lambda is an event-driven computing service that executes your code in  response to events. These events can come from a variety of sources, such as   HTTP requests via Amazon API Gateway, modifications to objects in Amazon S3    buckets, table updates in Amazon DynamoDB, and state transitions in AWS Step Functions.</p> <p>In this section, we will discuss how to handle events in AWS Lambda functions  using Python.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#understanding-the-event-object", "title": "Understanding the Event Object", "text": "<p>When AWS Lambda executes your function, it passes an event object to the  handler. This object contains information about the event that triggered the   function. The structure of the event object varies depending on the event    source. For example, an event from Amazon S3 might look like this:</p> <pre><code>{\n\"Records\": [\n{\n\"eventVersion\": \"2.1\",\n\"eventSource\": \"aws:s3\",\n\"awsRegion\": \"us-west-2\",\n\"eventTime\": \"2021-05-22T00:17:44.695Z\",\n\"eventName\": \"ObjectCreated:Put\",\n\"s3\": {\n\"s3SchemaVersion\": \"1.0\",\n\"configurationId\": \"testConfigRule\",\n\"bucket\": {\n\"name\": \"mybucket\",\n\"ownerIdentity\": {\n\"principalId\": \"EXAMPLE\"\n},\n\"arn\": \"arn:aws:s3:::mybucket\"\n},\n\"object\": {\n\"key\": \"HappyFace.jpg\",\n\"size\": 1024,\n\"eTag\": \"d41d8cd98f00b204e9800998ecf8427e\",\n\"sequencer\": \"0A1B2C3D4E5F678901\"\n}\n}\n}\n]\n}\n</code></pre> <p>In this case, the event object contains information about the S3 bucket and the  object that was created.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#accessing-event-data", "title": "Accessing Event Data", "text": "<p>You can access the data in the event object just like you would with any Python  dictionary. Here's an example of a Lambda function that prints the name of the   S3 bucket and the key of the object:</p> <pre><code>def lambda_handler(event, context):\n# get the bucket name\nbucket = event['Records'][0]['s3']['bucket']['name']\n# get the object key\nkey = event['Records'][0]['s3']['object']['key']\n# print the bucket name and object key\nprint(f'Bucket: {bucket}, Key: {key}')\nreturn {\n'statusCode': 200,\n'body': f'Bucket: {bucket}, Key: {key}'\n}\n</code></pre> <p>In this example, <code>event['Records'][0]['s3']['bucket']['name']</code> accesses the  name of the S3 bucket, and <code>event['Records'][0]['s3']['object']['key']</code>   accesses the key of the object.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#conclusion", "title": "Conclusion", "text": "<p>Handling events in AWS Lambda functions with Python involves understanding the  structure of the event object and accessing its data. The event object   provides valuable information about the event that triggered the function,    allowing you to write code that responds appropriately to the event.</p> <p>Remember, the structure of the event object depends on the event source, so be  sure to check the AWS Lambda documentation   for details about the event object structure for different event sources.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#understanding-the-context-object-in-aws-lambda-functions-with-python", "title": "Understanding the Context Object in AWS Lambda Functions with Python", "text": "<p>In addition to the event object, AWS Lambda also passes a context object to  your function. This object provides methods and properties that provide   information about the invocation, function, and execution environment.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#properties-of-the-context-object", "title": "Properties of the Context Object", "text": "<p>Here are some of the properties provided by the context object:</p> <ul> <li><code>aws_request_id</code>: The identifier of the invocation request.</li> <li><code>log_group_name</code>: The log group for the function.</li> <li><code>log_stream_name</code>: The log stream for the function instance.</li> <li><code>function_name</code>: The name of the Lambda function.</li> <li><code>memory_limit_in_mb</code>: The amount of memory available to the function in MB.</li> <li><code>function_version</code>: The version of the function.</li> <li><code>invoked_function_arn</code>: The Amazon Resource Name (ARN) used to invoke the    function. It can be function ARN or alias ARN. An unqualified ARN executes     the <code>$LATEST</code> version and aliases execute the function version it is      pointing to.</li> <li><code>identity</code> and <code>client_context</code>: For AWS Mobile SDK invocations, these    provide information about the client application and device.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#methods-of-the-context-object", "title": "Methods of the Context Object", "text": "<p>The context object also provides the following methods:</p> <ul> <li><code>get_remaining_time_in_millis()</code>: Returns the number of milliseconds left    before the execution times out.</li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#using-the-context-object", "title": "Using the Context Object", "text": "<p>Here's an example of how to use some of the properties and methods of the  context object:</p> <pre><code>def lambda_handler(event, context):\n# print the AWS request ID and memory limit\nprint(f'AWS Request ID: {context.aws_request_id}')\nprint(f'Memory Limit: {context.memory_limit_in_mb}')\n# get the remaining execution time\nremaining_time = context.get_remaining_time_in_millis()\nprint(f'Remaining Time: {remaining_time}ms')\nreturn {\n'statusCode': 200,\n'body': 'Hello from Lambda!'\n}\n</code></pre> <p>In this example, <code>context.aws_request_id</code> and <code>context.memory_limit_in_mb</code> are  used to print the AWS request ID and memory limit, respectively. The <code>context.  get_remaining_time_in_millis()</code> method is used to get the remaining execution time.</p> <p>The context object is a powerful tool that provides valuable information about  the invocation and execution environment of your AWS Lambda function. By   understanding and utilizing the properties and methods of the context object,    you can write more robust and efficient Lambda functions.</p> <p>For more information about the context object, check out the AWS Lambda documentation.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#chaining-aws-lambda-functions-with-python", "title": "Chaining AWS Lambda Functions with Python", "text": "<p>In AWS Lambda, you can create a sequence of Lambda functions where the output  of one function becomes the input of the next. This is often referred to as   \"chaining\" Lambda functions. Chaining can be useful when you need to create a    pipeline of processing steps, each handled by a separate Lambda function.</p> <p>There are several ways to chain Lambda functions, but one of the most common  methods is using AWS Step Functions.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#aws-step-functions", "title": "AWS Step Functions", "text": "<p>AWS Step Functions is a serverless  workflow service that lets you coordinate multiple AWS services into   serverless workflows. You can design and run workflows that stitch together    services, such as AWS Lambda, AWS Fargate, and Amazon SageMaker, into     feature-rich applications.</p> <p>Here's a basic example of how you can use AWS Step Functions to chain two  Lambda functions:</p> <ol> <li> <p>Create Your Lambda Functions: First, you need to create your Lambda     functions. For example, you might have a function <code>functionA</code> that      processes an input and produces an output, and a function <code>functionB</code> that       takes the output of <code>functionA</code> as its input.</p> <pre><code># functionA\ndef lambda_handler(event, context):\n# process the event\nprocessed_event = process_event(event)\nreturn processed_event\n# functionB\ndef lambda_handler(event, context):\n# the event is the output of functionA\nresult = do_something_with(event)\nreturn result\n</code></pre> </li> <li> <p>Define Your Step Functions State Machine: A state machine in AWS Step     Functions is a JSON-based, visual workflow of your application's steps.      Here's an example of a state machine that chains <code>functionA</code> and <code>functionB</code>:</p> </li> </ol> <pre><code>{\n\"Comment\": \"A simple AWS Step Functions state machine that chains two Lambda functions.\",\n\"StartAt\": \"functionA\",\n\"States\": {\n\"functionA\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:functionA\",\n\"Next\": \"functionB\"\n},\n\"functionB\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:functionB\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this state machine, the <code>StartAt</code> field specifies the first state to run  (<code>functionA</code>). The <code>Next</code> field in the <code>functionA</code> state specifies the next   state to run after <code>functionA</code> (<code>functionB</code>). The <code>End</code> field in the    <code>functionB</code> state indicates that <code>functionB</code> is the final state.</p> <ol> <li>Create Your State Machine: After defining your state machine, you can     create it in the AWS Step Functions console. You can then start an      execution of your state machine, providing an initial JSON input. AWS Step       Functions will run your Lambda functions in the order defined by your        state machine, passing the output of one function as the input to the         next.</li> </ol> <p>Remember, error handling and retry policies are important considerations when  chaining Lambda functions. AWS Step Functions provides built-in support for   error handling and retries, which you can customize in your state machine definition.</p> <p>For more information about AWS Step Functions, check out the  official AWS Step Functions Developer Guide.</p> <p>You can find the section about AWS steps function here</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/lambda/#conclusion_1", "title": "Conclusion", "text": "<p>AWS Lambda and Python are a powerful combination for serverless computing. With  AWS Lambda, you can focus on writing code without having to worry about   managing servers. And with Python, you can write readable and maintainable    code that can be easily deployed to AWS Lambda.</p> <p>This guide has covered the basics of using Python with AWS Lambda, but there's  much more to learn. Be sure to check out the official AWS Lambda Developer   Guide and the    official Python documentation for more     information. Happy coding!</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Lambda"]}, {"location": "cloud/aws/steps/", "title": "Steps", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#aws-step-functions", "title": "AWS Step Functions", "text": "", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#introduction", "title": "Introduction", "text": "<p>AWS Step Functions is a serverless workflow service provided by Amazon Web  Services. It allows developers to design and execute workflows that coordinate   between multiple AWS services such as AWS Lambda, Amazon SNS, and Amazon    DynamoDB. These workflows, known as state machines, are defined using a     JSON-based, Amazon States Language (ASL).</p> <p>In this article, we will explore the basics of AWS Step Functions, how to  create a state machine, and how to integrate it with other AWS services.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#what-are-aws-step-functions", "title": "What are AWS Step Functions?", "text": "<p>AWS Step Functions is a service that helps you coordinate multiple AWS services  into serverless workflows so you can build and update apps quickly. Using Step   Functions, you can design and run workflows that stitch together services    such as AWS Lambda and Amazon ECS into feature-rich applications.</p> <p>Workflows are made up of a series of steps, with the output of one step acting  as input into the next. AWS Step Functions is fully managed, so it scales,   operates, and ensures the reliability of your operational tasks so you can    focus on your application.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#components-of-aws-step-functions", "title": "Components of AWS Step Functions", "text": "<p>The primary components of AWS Step Functions are:</p> <ul> <li> <p>State Machine: A state machine is the core component that you interact    with. It defines the workflow of the application and is described using the     Amazon States Language.</p> </li> <li> <p>States: Each step in the workflow is represented as a state. There are    various types of states like Task, Choice, Wait, Succeed, Fail, Parallel,     and Map.</p> </li> <li> <p>Transitions: Transitions are the movement between states in a state    machine.</p> </li> <li> <p>Tasks: Tasks represent a single unit of work that the state machine needs    to perform.</p> </li> </ul>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#creating-a-state-machine", "title": "Creating a State Machine", "text": "<p>Creating a state machine involves defining the state machine structure using  the Amazon States Language in a JSON format. Here's an example of a simple   state machine:</p> <pre><code>{\n\"Comment\": \"A Hello World example of the Amazon States Language using a Pass state\",\n\"StartAt\": \"HelloWorld\",\n\"States\": {\n\"HelloWorld\": {\n\"Type\": \"Pass\",\n\"Result\": \"Hello, World!\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this example, there is a single state named <code>HelloWorld</code>. The <code>Type</code> field  indicates that this is a <code>Pass</code> state, which is a state that does nothing and   passes its input to its output. The <code>Result</code> field contains a static string    that is the output of the state. The <code>End</code> field indicates that this is the     end of the execution.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#integrating-with-other-aws-services", "title": "Integrating with Other AWS Services", "text": "<p>AWS Step Functions can integrate with various AWS services. For example, you  can use AWS Lambda functions as tasks within your state machine. Here's an   example of a state machine that uses a Lambda function:</p> <pre><code>{\n\"Comment\": \"A simple AWS Step Functions state machine that executes a Lambda function\",\n\"StartAt\": \"InvokeLambdaFunction\",\n\"States\": {\n\"InvokeLambdaFunction\": {\n\"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:REGION:ACCOUNT_ID:function:FUNCTION_NAME\",\n\"End\": true\n}\n}\n}\n</code></pre> <p>In this example, the <code>Resource</code> field contains the ARN of the Lambda function  to invoke.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#monitoring-and-debugging", "title": "Monitoring and Debugging", "text": "<p>AWS Step Functions provides detailed logging for each step of your execution in  CloudWatch Logs. You can use these logs to monitor executions and to   troubleshoot issues. AWS Step Functions also provides visual workflows in the    AWS Management Console, which allows you to see the path that your execution     took through the state machine.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/aws/steps/#conclusion", "title": "Conclusion", "text": "<p>AWS Step Functions is a powerful service for orchestrating multi-step workflows  in a reliable and scalable manner. By defining workflows as state machines,   you can simplify complex processes and coordinate between multiple AWS services.</p> <p>For more information about AWS Step Functions, check out the  official AWS Step Functions Developer Guide.</p>", "tags": ["Mlops", "Devops", "AWS", "Microservices", "AWS Steps"]}, {"location": "cloud/github/codespaces/", "title": "Codespaces", "text": ""}, {"location": "cloud/github/codespaces/#github-codespaces", "title": "Github codespaces", "text": ""}, {"location": "cloud/github/codespaces/#resources", "title": "Resources", "text": "<ul> <li>Fine tuning hugginface model</li> </ul>"}, {"location": "cloud/google/app_engine/", "title": "App Engine", "text": "", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#google-app-engine", "title": "Google App Engine", "text": "", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#deploy-a-fastapi-app-in-google-app-engine", "title": "Deploy a Fastapi App in Google App engine", "text": "<p>This guide will help you deploy a FastAPI application on Google App Engine.</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>A Google Cloud account</li> <li>Google Cloud SDK installed on your machine</li> <li>A FastAPI application</li> </ul>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-1-prepare-your-fastapi-application", "title": "Step 1: Prepare Your FastAPI Application", "text": "<p>Your FastAPI application is ready to go. Here's the <code>main.py</code> for reference:</p> <pre><code>from fastapi import FastAPI\nimport uvicorn\napp = FastAPI()\n@app.get('/')\nasync def root():\nreturn {'message': 'Hello Duke'}\n@app.get('/add/{num1}/{num2}')\nasync def add(num1: int, num2: int):\n'''Add two numbers together'''\ntotal = num1 + num2\nreturn {'total': total}\nif __name__ == '__main__':\nuvicorn.run(app, port=8080, host='0.0.0.0')\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-2-create-appyaml-file", "title": "Step 2: Create app.yaml File", "text": "<p>Next, you'll need to create an <code>app.yaml</code> file in the root directory of your  project. This file configures your App Engine application's settings.</p> <p>Here's an example <code>app.yaml</code> file:</p> <pre><code>runtime: python39  # Use the Python 3.9 runtime\ninstance_class: F2  # Choose a class with at least 256MB to run FastAPI and Uvicorn\nentrypoint: uvicorn main:app --host 0.0.0.0 --port $PORT\nautomatic_scaling:\ntarget_cpu_utilization: 0.65\nmin_instances: 1\nmax_instances: 10\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-3-create-a-requirementstxt-file", "title": "Step 3: Create a requirements.txt File", "text": "<p>Create a <code>requirements.txt</code> file in the root directory of your project and add  the necessary dependencies:</p> <pre><code>fastapi==0.68.1\nuvicorn[standard]==0.15.0\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#step-4-deploy-your-application", "title": "Step 4: Deploy Your Application", "text": "<p>Before you deploy, make sure you're authenticated to Google Cloud:</p> <pre><code>gcloud auth login\n</code></pre> <p>Next, set your project ID:</p> <pre><code>gcloud config set project your-project-id\n</code></pre> <p>Finally, deploy your app:</p> <pre><code>gcloud app deploy\n</code></pre> <p>Your FastAPI application should now be deployed to Google App Engine!</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#bonus-deploy-with-github-actions", "title": "Bonus: Deploy with github actions", "text": "<p>To deploy your FastAPI application on Google App Engine using GitHub Actions,  you'll first need to create a service account in your Google Cloud Project.   This service account should have the 'App Engine Deployer' and 'Storage    Admin' roles to allow it to deploy applications and upload to the Cloud     Storage bucket. Download the JSON key file for this service account and      store it as a secret (let's say GCP_SA_KEY) in your GitHub repository.</p> <p>Here's a simple GitHub Actions workflow that can be used for deployment. Create  a new file under .github/workflows in your repository named deploy.yml and add   the following content:</p> <pre><code>name: Deploy to Google App Engine\non:\npush:\nbranches:\n- main  # Trigger the workflow on push to main branch\njobs:\ndeploy:\nruns-on: ubuntu-latest\nsteps:\n- name: Checkout code\nuses: actions/checkout@v2\n- name: Set up Python 3.9\nuses: actions/setup-python@v2\nwith:\npython-version: 3.9\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install -r requirements.txt\n- name: Setup gcloud CLI\nuses: google-github-actions/setup-gcloud@master\nwith:\nservice_account_key: ${{ secrets.GCP_SA_KEY }}\nproject_id: your-gcp-project-id\nexport_default_credentials: true\n- name: Deploy to App Engine\nrun: gcloud app deploy --quiet\n</code></pre>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#conclusion", "title": "Conclusion", "text": "<p>Google App Engine is a powerful platform for deploying Python web applications.  With it, you can focus on building your FastAPI application and leave the  infrastructure management to Google.</p>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/app_engine/#references", "title": "References", "text": "<ul> <li>Fastapi</li> <li>Github Actions</li> </ul>", "tags": ["Mlops", "Continuous Integration", "FastApi", "Devops", "Google Cloud", "Microservices"]}, {"location": "cloud/google/cloud_functions/", "title": "Cloud Functions", "text": ""}, {"location": "cloud/google/cloud_functions/#google-cloud-functions-serverless-computing-made-easy", "title": "Google Cloud Functions: Serverless Computing Made Easy", "text": "<p>In today's fast-paced digital landscape, businesses are increasingly looking  for ways to optimize their application development and deployment processes.   Serverless computing has emerged as a popular solution, offering scalability,    cost-effectiveness, and simplified management. Among the various serverless     platforms available, Google Cloud Functions stands out as a powerful and      user-friendly option. In this article, we will explore the features and       benefits of Google Cloud Functions and how it can help developers build        and deploy applications effortlessly.</p>"}, {"location": "cloud/google/cloud_functions/#what-are-google-cloud-functions", "title": "What are Google Cloud Functions", "text": "<p>Google Cloud Functions is a serverless computing platform that enables developers to build and run applications without worrying about infrastructure  management. With Cloud Functions, developers can write and deploy code in a   serverless environment, where the cloud provider handles all the operational    aspects such as scaling, patching, and monitoring. This allows developers to     focus solely on writing the application logic, resulting in faster      development cycles and increased productivity.</p>"}, {"location": "cloud/google/cloud_functions/#key-features", "title": "Key Features", "text": ""}, {"location": "cloud/google/cloud_functions/#1-event-driven-computing", "title": "1. Event-Driven Computing", "text": "<p>Google Cloud Functions is designed around the concept of event-driven  computing. Developers can write functions that respond to various types of   events, such as changes in data, incoming HTTP requests, or messages from a    messaging system. This event-driven model allows applications to be highly     reactive and responsive, triggering functions only when needed, reducing      costs and optimizing resource utilization.</p>"}, {"location": "cloud/google/cloud_functions/#2-language-support", "title": "2. Language Support", "text": "<p>Cloud Functions supports a wide range of programming languages, including  JavaScript (Node.js), Python, Go, and more. This flexibility allows developers   to use their preferred language and leverage existing code and libraries.    Whether you're building a web application, processing data, or creating a     microservice, you can find the right language for your needs.</p>"}, {"location": "cloud/google/cloud_functions/#3-automatic-scaling", "title": "3. Automatic Scaling", "text": "<p>One of the significant advantages of serverless computing is automatic scaling.  With Cloud Functions, you don't have to worry about provisioning or managing   resources based on anticipated traffic. The platform automatically scales the    resources up or down based on the incoming request rate, ensuring optimal     performance and cost efficiency. You pay only for the actual execution time      of your functions, making it a highly cost-effective solution.</p>"}, {"location": "cloud/google/cloud_functions/#4-seamless-integration-with-google-cloud-services", "title": "4. Seamless Integration with Google Cloud Services", "text": "<p>Google Cloud Functions seamlessly integrates with other services provided by  Google Cloud, such as Cloud Storage, Cloud Pub/Sub, Cloud Firestore, and more.   This tight integration allows you to create powerful applications that    leverage the full potential of Google Cloud's ecosystem. Whether you need to     process incoming data, trigger actions based on file uploads, or perform      real-time analytics, Cloud Functions provides the necessary tools for       seamless integration.</p>"}, {"location": "cloud/google/cloud_functions/#5-monitoring-and-logging", "title": "5. Monitoring and Logging", "text": "<p>Google Cloud Functions offers robust monitoring and logging capabilities,  allowing developers to gain insights into their applications' performance and   troubleshoot issues effectively. You can monitor the execution metrics, view    logs, and set up alerts to ensure your functions are running smoothly.</p>"}, {"location": "cloud/google/cloud_functions/#writing-and-triggering-a-google-cloud-function", "title": "Writing and Triggering a Google Cloud Function", "text": "<p>Let's take a look at an example of writing and triggering a Google Cloud  Function using Python.</p>"}, {"location": "cloud/google/cloud_functions/#writing-the-function", "title": "Writing the Function", "text": "<pre><code>def hello_world(request):\n\"\"\"HTTP Cloud Function.\n    Args:\n        request (flask.Request): The request object.\n        &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Request&gt;\n    Returns:\n        The response text, or any set of values that can be turned into a\n        Response object using `make_response`\n        &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response&gt;.\n    \"\"\"\nreturn 'Hello, World!'\n</code></pre> <p>In this example, we have a simple function named <code>hello_world</code> that takes a  Flask <code>request</code> object as an argument and returns the string \"Hello, World!\".</p>"}, {"location": "cloud/google/cloud_functions/#deploying-the-function", "title": "Deploying the Function", "text": "<p>To deploy the function to Google Cloud Functions, follow these steps:</p> <ol> <li> <p>Make sure you have the Google Cloud SDK     installed and configured on your machine.</p> </li> <li> <p>Open a terminal or command prompt and navigate to the directory containing     your Python function code.</p> </li> <li> <p>Run the following command to deploy the function:</p> </li> </ol> <pre><code>gcloud functions deploy hello_world \\\n--runtime python39 \\\n--trigger-http \\\n--allow-unauthenticated\n</code></pre> <p>This command deploys the function with the name <code>hello_world</code>, specifies the  Python 3.9 runtime, and sets the trigger type to HTTP. The   <code>--allow-unauthenticated</code> flag allows the function to be triggered without authentication.</p>"}, {"location": "cloud/google/cloud_functions/#triggering-and-testing-the-function", "title": "Triggering and Testing the Function", "text": "<p>Once the function is deployed, you can trigger it by sending an HTTP request to  its URL. You can use tools like <code>curl</code> or <code>httpie</code> to test the function   locally or use services like Postman for more    advanced testing.</p> <p>Here's an example using <code>curl</code> to send a GET request to the function's URL:</p> <pre><code>curl https://REGION-PROJECT_ID.cloudfunctions.net/hello_world\n</code></pre> <p>Replace <code>REGION</code> with the region where the function is deployed (e.g., <code>us-central1</code>) and <code>PROJECT_ID</code> with your Google Cloud project ID.</p> <p>After triggering the function, you should receive the response \"Hello, World!\".</p>"}, {"location": "cloud/google/cloud_functions/#conclusion", "title": "Conclusion", "text": "<p>Google Cloud Functions offers a powerful and convenient platform for building  and deploying serverless applications. With its event-driven model, support   for multiple programming languages, automatic scaling, seamless integration    with Google Cloud services, and robust monitoring capabilities, developers     can focus on writing application logic without worrying about      infrastructure management. By leveraging the power of serverless       computing, businesses can optimize resource utilization, reduce costs,        and accelerate their application development process.</p>"}, {"location": "cloud/google/cloud_run/", "title": "Cloud Run", "text": ""}, {"location": "cloud/google/cloud_run/#cloud-run-scaling-machine-learning-applications-with-ease", "title": "Cloud Run: Scaling Machine Learning Applications with Ease", "text": "<p>As machine learning continues to revolutionize various industries, the need for scalable and efficient deployment solutions for ML applications becomes crucial. Google Cloud Run offers a powerful and flexible platform for running containerized applications, making it an ideal choice for deploying machine learning models. In this article, we will explore the features and benefits of Google Cloud Run and how it can be leveraged to deploy a machine learning application using Python.</p>"}, {"location": "cloud/google/cloud_run/#what-is-google-cloud-run", "title": "What is Google Cloud Run", "text": "<p>Google Cloud Run is a fully managed serverless platform that allows developers to deploy containerized applications quickly and easily. It provides automatic scaling, networking, and infrastructure management, enabling developers to focus on building and deploying applications without worrying about the underlying infrastructure. With Cloud Run, you can deploy stateless HTTP services and take advantage of on-demand scaling, cost-efficiency, and ease of management.</p>"}, {"location": "cloud/google/cloud_run/#key-features", "title": "Key Features", "text": ""}, {"location": "cloud/google/cloud_run/#1-serverless-scalability", "title": "1. Serverless Scalability", "text": "<p>Cloud Run offers automatic scaling based on the incoming request rate, allowing your application to handle any level of traffic without manual intervention. It scales containers up and down quickly, ensuring that your machine learning application can handle peak loads efficiently. You only pay for the resources consumed during execution, making it a cost-effective solution for running ML workloads.</p>"}, {"location": "cloud/google/cloud_run/#2-container-compatibility", "title": "2. Container Compatibility", "text": "<p>Cloud Run supports containerized applications, allowing you to package your machine learning model and dependencies into a Docker container. This flexibility enables you to use any programming language, library, or framework that can be containerized. With Python being a popular choice for machine learning, you can easily deploy Python-based ML applications on Cloud Run.</p>"}, {"location": "cloud/google/cloud_run/#3-rapid-deployment", "title": "3. Rapid Deployment", "text": "<p>Deploying a machine learning application on Cloud Run is straightforward. You can use the command-line interface (CLI) or integrate with continuous integration and deployment (CI/CD) pipelines to automate the deployment process. Cloud Run provides a seamless experience for deploying new versions or rolling back to previous versions, enabling rapid iteration and deployment cycles.</p>"}, {"location": "cloud/google/cloud_run/#4-easy-integration", "title": "4. Easy Integration", "text": "<p>Cloud Run seamlessly integrates with other Google Cloud services, such as Cloud Storage, BigQuery, and Pub/Sub. This integration allows you to utilize additional services for data storage, data processing, and event-driven architectures. For example, you can trigger your ML application based on new data arriving in Cloud Storage or process predictions asynchronously using Pub/Sub.</p>"}, {"location": "cloud/google/cloud_run/#deploying-a-machine-learning-application-with-cloud-run", "title": "Deploying a Machine Learning Application with Cloud Run", "text": "<p>Let's explore how to deploy a machine learning application using Python and Cloud Run.</p>"}, {"location": "cloud/google/cloud_run/#building-the-docker-container", "title": "Building the Docker Container", "text": "<p>To deploy a machine learning application on Cloud Run, you need to package your application and its dependencies into a Docker container. Here are the steps to build the container:</p> <ol> <li> <p>Create a <code>Dockerfile</code> in your project directory with the following content:</p> <pre><code># Use the official Python runtime as the base image\nFROM python:3.9-slim\n# Set the working directory in the container\nWORKDIR /app\n# Copy the requirements file and install dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy the rest of the application code\nCOPY . .\n\n# Define the command to run your application\nCMD [ \"python\", \"app.py\" ]\n</code></pre> </li> <li> <p>Create a <code>requirements.txt</code> file listing the Python dependencies required     by your machine learning application.</p> </li> <li> <p>Build the Docker image by running the following command in the project     directory:</p> </li> </ol> <pre><code>docker build -t gcr.io/PROJECT_ID/IMAGE_NAME .\n</code></pre> <p>Replace <code>PROJECT_ID</code> with your Google Cloud project ID and <code>IMAGE_NAME</code> with the desired name for your Docker image.</p> <ol> <li>Push the Docker image to the Google Container Registry (GCR):</li> </ol> <pre><code>docker push gcr.io/PROJECT_ID/IMAGE_NAME\n</code></pre>"}, {"location": "cloud/google/cloud_run/#deploying-the-application", "title": "Deploying the Application", "text": "<p>Once the Docker image is built and pushed to GCR, you can deploy the machine learning application on Cloud Run:</p> <ol> <li> <p>Using the Google Cloud Console, navigate to the Cloud Run section.</p> </li> <li> <p>Click on \"Create Service.\"</p> </li> <li> <p>Choose the Docker image you pushed to GCR.</p> </li> <li> <p>Configure the service settings, including the region, memory allocation,     and maximum number of container instances.</p> </li> <li> <p>Click on \"Create\" to deploy the application.</p> </li> </ol>"}, {"location": "cloud/google/cloud_run/#testing-the-application", "title": "Testing the Application", "text": "<p>After the application is deployed, Cloud Run generates a unique URL that you can use to test the machine learning application. You can send HTTP requests to this URL with the necessary input data to receive predictions from your model.</p> <p>Using Python, you can write a simple script to test the deployed ML application. Here's an example using the <code>requests</code> library:</p> <pre><code>import requests\nurl = \"https://YOUR_CLOUD_RUN_URL\"\ndata = {\n\"feature1\": 0.5,\n\"feature2\": 0.8,\n# Include other input features as required by your model\n}\nresponse = requests.post(url, json=data)\npredictions = response.json()\nprint(predictions)\n</code></pre> <p>Replace <code>YOUR_CLOUD_RUN_URL</code> with the URL generated by Cloud Run for your deployed ML application. The script sends a POST request with input data in JSON format and retrieves the predictions as the response.</p>"}, {"location": "cloud/google/cloud_run/#conclusion", "title": "Conclusion", "text": "<p>Google Cloud Run provides an excellent platform for deploying machine learning applications with ease. By leveraging the power of containerization, automatic scaling, and seamless integration with Google Cloud services, developers can efficiently deploy and scale their ML models. Whether you are deploying a simple predictive model or a complex deep learning network, Cloud Run offers the flexibility and scalability required to meet the demands of modern machine learning applications.</p>"}, {"location": "data_science/statistics/intro/", "title": "Introduction to statistics", "text": "In\u00a0[1]: Copied! <pre>2+2\n</pre> 2+2 Out[1]: <pre>4</pre> In\u00a0[2]: Copied! <pre># True\n</pre> # True"}, {"location": "data_science/statistics/intro/#introduction-to-statistics", "title": "Introduction to statistics\u00b6", "text": ""}, {"location": "data_science/statistics/intro/#hello-2", "title": "Hello 2\u00b6", "text": ""}, {"location": "generative_ai/adapting_models/", "title": "Introduction", "text": ""}, {"location": "generative_ai/adapting_models/#adaptation", "title": "Adaptation", "text": "<p>Adaptation in AI is a crucial step to enhance the capabilities of foundation models, allowing them to cater to specific tasks and domains. This process is about tailoring pre-trained AI systems with new data, ensuring they perform optimally in specialized applications and respect privacy constraints. Reaping the benefits of adaptation leads to AI models that are not only versatile but also more aligned with the unique needs of organizations and industries.</p> <p>Adapting foundation models is essential due to their limitations in specific areas despite their extensive training on large datasets. Although they excel at many tasks, these models can sometimes misconstrue questions or lack up-to-date information, which highlights the need for fine-tuning. By addressing these weaknesses through additional training or other techniques, the performance of foundation models can be significantly improved.</p> <ul> <li>Prompting</li> <li> <p>Probing: Using probing to train a classifier is a  powerful approach to   tailor generative AI foundation models, like BERT, for specific   applications. By adding a modestly-sized neural network, known as a   classification head, to a foundation model, one can specialize in   particular tasks such as sentiment analysis. This technique involves   freezing the original model's parameters and only adjusting the   classification head through training with labeled data. Ultimately, this   process simplifies adapting sophisticated AI systems to our needs,   providing a practical tool for developing efficient and targeted machine   learning solutions.</p> <ul> <li> <p>Linear Probing: A simple form of probing that involves attaching a  linear classifier to a pre-trained model to adapt it to a new task   without modifying the original model.</p> </li> <li> <p>Classification Head: It is the part of a neural network that is  tailored to classify input data into defined categories.</p> </li> <li>Transfer learning</li> <li> <p>Fine Tuning: Fine-tuning is an important phase in enhancing the abilities of generative AI models, making them adept at specific tasks. By introducing additional data to these powerful models, they can be tailored to meet particular requirements, which is invaluable in making AI more effective and efficient. Although this process comes with its challenges, such as the need for significant computational resources and data, the outcome is a more specialized and capable AI system that can bring value to a wide range of applications.</p> </li> <li> <p>Fine tuning is a type of transfer learning.</p> </li> <li>Traditional fine tuning consist in update all the weigths of a  training model. (Need more resource a much more data)</li> </ul> </li> </ul>"}, {"location": "generative_ai/adapting_models/#resources", "title": "Resources", "text": "<ul> <li>Create a sentiment classifier with Bert</li> </ul>"}, {"location": "generative_ai/build_applications/", "title": "Build applications", "text": ""}, {"location": "generative_ai/build_applications/#build-llm-powered-applications", "title": "Build LLM-powered applications", "text": ""}, {"location": "generative_ai/build_applications/#chain-of-thought", "title": "Chain of thought", "text": "<p>At its core, CoT prompting spurs reasoning in LLMs via decomposition. When we tackle a complicated enough math or logic question, we often can\u2019t help but break  the larger problem into a series of intermediate steps that help us arrive at a   final answer.</p>"}, {"location": "generative_ai/build_applications/#program-aided-laguage-models", "title": "Program-aided laguage models", "text": "<p>The Program-Aided Language Model (PAL) method uses LLMs to read natural language problems and generate programs as reasoning steps. The code is executed by a  interpreter to produce the answer.</p>"}, {"location": "generative_ai/build_applications/#react-synergizing-reasonning-and-actions-in-llms", "title": "ReAct: Synergizing Reasonning and Actions in LLMs", "text": "<p>ReAct enables LLMs to generate reasoning traces and task-specific actions,  leveraging the synergy between them. The approach demonstrates superior  performance over baselines in various tasks, overcoming issues like hallucination  and error propagation. ReAct outperforms imitation and reinforcement learning  methods in interactive decision making, even with minimal context examples.  It not only enhances performance but also improves interpretability,  trustworthiness, and diagnosability by allowing humans to distinguish between  internal knowledge and external information.</p> <p>In summary, ReAct bridges the gap between reasoning and acting in LLMs, yielding remarkable results across language reasoning and decision making tasks. By interleaving reasoning traces and actions, ReAct overcomes limitations and outperforms baselines, not only enhancing model performance but also providing interpretability and trustworthiness, empowering users to understand the model's decision-making process.</p> <p></p> <p>The figure provides a comprehensive visual comparison of different prompting methods in two distinct domains. The first part of the figure (1a) presents a comparison of four prompting methods: Standard, Chain-of-thought (CoT, Reason Only), Act-only, and ReAct (Reason+Act) for solving a HotpotQA question. Each method's approach is demonstrated through task-solving trajectories generated by the model (Act, Thought) and the environment (Obs). The second part of the figure (1b) focuses on a comparison between Act-only and ReAct prompting methods to solve an AlfWorld game. In both domains, in-context examples are omitted from the prompt, highlighting the generated trajectories as a result of the model's actions and thoughts and the observations made in the environment. This visual representation enables a clear understanding of the differences and advantages offered by the ReAct paradigm compared to other prompting methods in diverse task-solving scenarios.</p> <p>Paper Link</p>"}, {"location": "generative_ai/build_applications/#resources", "title": "Resources", "text": "<ul> <li> <p>Chain-of-thought Prompting Elicits Reasoning in Large Language Models Paper by researchers at Google exploring how chain-of-thought prompting improves the ability of LLMs to perform complex reasoning.</p> </li> <li> <p>PAL: Program-aided Language Models: This paper proposes an approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps.</p> </li> <li> <p>ReAct: Synergizing Reasoning and Acting in Language Models: This paper presents an advanced prompting technique that allows an LLM to make decisions about how to interact with external applications.</p> </li> </ul>"}, {"location": "generative_ai/deployment/", "title": "Deployment", "text": ""}, {"location": "generative_ai/deployment/#deploymennt-llm", "title": "Deploymennt LLM", "text": ""}, {"location": "generative_ai/deployment/#reduce-the-size-of-the-model-in-deployment", "title": "Reduce the size of the model in deployment", "text": ""}, {"location": "generative_ai/deployment/#pruning", "title": "Pruning", "text": "<p>Deep model pruning involves identifying and removing unnecessary connections,  weights, or even entire neurons from a trained deep learning model. By  eliminating these redundant components, the model can become more compact,  faster, and more memory-efficient, while still maintaining a high level of accuracy.</p>"}, {"location": "generative_ai/deployment/#distilling", "title": "Distilling", "text": "<p>The key idea of distilling step-by-step is to extract informative natural language  rationales (i.e., intermediate reasoning steps) from LLMs, which can in turn be   used to train small models in a more data-efficient way.</p>"}, {"location": "generative_ai/deployment/#peft", "title": "PEFT", "text": "<p>TBD</p>"}, {"location": "generative_ai/deployment/#resources", "title": "Resources", "text": "<ul> <li>How to Forget Jenny's Phone Number or: Model Pruning, Distillation, and Quantization</li> <li>Distilling step-by-step</li> </ul>"}, {"location": "generative_ai/evaluation/", "title": "Evaluation", "text": ""}, {"location": "generative_ai/evaluation/#evaluating-llms", "title": "Evaluating LLMS", "text": ""}, {"location": "generative_ai/evaluation/#metrics", "title": "Metrics", "text": ""}, {"location": "generative_ai/evaluation/#rouge", "title": "ROUGE", "text": "<p>ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is primarily used for text summarization. It compares a summary to one or more human-generated reference summaries.</p>"}, {"location": "generative_ai/evaluation/#bleu", "title": "BLEU", "text": "<p>BLEU (BiLingual Evaluation Understudy) is primarily used for machine translation. It compares a translation to human-generation translations. The score is computed using the average precision over multiple n-gram sizes. It is similar to the ROUGE score but is calculated over a range of n-gram sizes and then averaged.</p>"}, {"location": "generative_ai/evaluation/#benchmark", "title": "Benchmark", "text": "<p>Benchmarks matter because they are the standards that help us measure and accelerate progress in AI. They offer a common ground for comparing different AI models and encouraging innovation, providing important stepping stones on the path to more advanced AI technologies.</p>"}, {"location": "generative_ai/evaluation/#glue", "title": "GLUE", "text": "<p>The GLUE benchmarks serve as an essential tool to assess an AI's grasp of human language, covering diverse tasks, from grammar checking to complex sentence relationship analysis. By putting AI models through these varied linguistic challenges, we can gauge their readiness for real-world tasks and uncover any potential weaknesses.</p> <ul> <li> <p>Short Name: CoLA</p> <ul> <li>Full Name: Corpus of Linguistic Acceptability</li> <li>Description: Measures the ability to determine if an English  sentence is linguistically acceptable.</li> </ul> </li> <li> <p>Short Name: SST-2</p> <ul> <li>Full Name: Stanford Sentiment Treebank</li> <li>Description: Consists of sentences from movie reviews and human  annotations about their sentiment.</li> </ul> </li> <li> <p>Short Name: MRPC</p> <ul> <li>Full Name: Microsoft Research Paraphrase Corpus</li> <li>Description: Focuses on identifying whether two sentences are paraphrases of each other.</li> </ul> </li> <li> <p>Short Name: STS-B</p> <ul> <li>Full Name: Semantic Textual Similarity Benchmark</li> <li>Description: Involves determining how similar two sentences are  in terms of semantic content.</li> </ul> </li> <li> <p>Short Name: QQP</p> <ul> <li>Full Name: Quora Question Pairs</li> <li>Description: Aims to identify whether two questions asked on  Quora are semantically equivalent.</li> </ul> </li> <li> <p>Short Name: MNLI</p> <ul> <li>Full Name: Multi-Genre Natural Language Inference</li> <li>Description: Consists of sentence pairs labeled for textual  entailment across multiple genres of text.</li> </ul> </li> <li> <p>Short Name: QNLI</p> <ul> <li>Full Name: Question Natural Language Inference</li> <li>Description: Involves determining whether the content of a  paragraph contains the answer to a question.</li> </ul> </li> <li> <p>Short Name: RTE</p> <ul> <li>Full Name: Recognizing Textual Entailment</li> <li>Description: Requires understanding whether one sentence entails another.</li> </ul> </li> <li> <p>Short Name: WNLI</p> <ul> <li>Full Name: Winograd Natural Language Inference</li> <li>Description: Tests a system's reading comprehension by having it  determine the correct referent of a pronoun in a sentence, where  understanding depends on contextual information provided by specific   words or phrases.</li> </ul> </li> </ul>"}, {"location": "generative_ai/evaluation/#superglue", "title": "SuperGLUE", "text": "<p>SuperGlue is designed as a successor to the original GLUE benchmark. It's a more advanced benchmark aimed at presenting even more challenging language understanding tasks for AI models. Created to push the boundaries of what AI can understand and process in natural language, SuperGlue emerged as models began to achieve human parity on the GLUE benchmark. It also features a public leaderboard, facilitating the direct comparison of models and enabling the tracking of progress over time.</p> <ul> <li> <p>Short Name: BoolQ</p> <ul> <li>Full Name: Boolean Questions</li> <li>Description: Involves answering a yes/no question based on a short passage.</li> </ul> </li> <li> <p>Short Name: CB</p> <ul> <li>Full Name: CommitmentBank</li> <li>Description: Tests understanding of entailment and contradiction in a three-sentence format.</li> </ul> </li> <li> <p>Short Name: COPA</p> <ul> <li>Full Name: Choice of Plausible Alternatives</li> <li>Description: Measures causal reasoning by asking for the cause effect of a given sentence.</li> </ul> </li> <li> <p>Short Name: MultiRC</p> <ul> <li>Full Name: Multi-Sentence Reading Comprehension</li> <li>Description: Involves answering questions about a paragraph  where each question may have multiple correct answers.</li> </ul> </li> <li> <p>Short Name: ReCoRD</p> <ul> <li>Full Name: Reading Comprehension with Commonsense Reasoning</li> <li>Description: Requires selecting the correct named entity from a  passage to fill in the blank of a question.</li> </ul> </li> <li> <p>Short Name: RTE</p> <ul> <li>Full Name: Recognizing Textual Entailment</li> <li>Description: Involves identifying whether a sentence entails,  contradicts, or is neutral towards another sentence.</li> </ul> </li> <li> <p>Short Name: WiC</p> <ul> <li>Full Name: Words in Context</li> <li>Description: Tests understanding of word sense disambiguation in  different contexts.</li> </ul> </li> <li> <p>Short Name: WSC</p> <ul> <li>Full Name: Winograd Schema Challenge</li> <li>Description: Focuses on resolving coreference resolution within  a sentence, often requiring commonsense reasoning.</li> </ul> </li> <li> <p>Short Name: AX-b</p> <ul> <li>Full Name: Broad Coverage Diagnostic</li> <li>Description: A diagnostic set to evaluate model performance on a  broad range of linguistic phenomena.</li> </ul> </li> <li> <p>Short Name: AX-g</p> <ul> <li>Full Name: Winogender Schema Diagnostics</li> <li>Description: Tests for the presence of gender bias in automated  coreference resolution systems.</li> </ul> </li> </ul>"}, {"location": "generative_ai/evaluation/#others", "title": "Others", "text": "<ul> <li>MMLU</li> <li>BIG-Bench</li> <li>HELM</li> </ul>"}, {"location": "generative_ai/huggingface/", "title": "Overview", "text": ""}, {"location": "generative_ai/huggingface/#hugginface", "title": "Hugginface", "text": ""}, {"location": "generative_ai/huggingface/#hugginface-cli", "title": "Hugginface CLI", "text": "<p>Hugging Face, primarily known for its advancements in natural language  processing, offers a robust Command Line Interface (CLI) that streamlines the   usage of its models and datasets. This guide provides a comprehensive    overview of installing the Hugging Face CLI and creating a project, drawing     parallels to GitHub's functionality. Link</p>"}, {"location": "generative_ai/huggingface/#cli-installation", "title": "CLI Installation", "text": "<p>To begin, ensure you have Python installed on your system. Hugging Face CLI  requires Python 3.6 or later.</p> <p>Step 1: Install the Hugging Face Hub</p> <p>Use pip to install the Hugging Face Hub:</p> <pre><code>pip install huggingface_hub\n</code></pre> <p>Step 2: Verify Installation Confirm the installation by checking the version:</p> <pre><code>huggingface-cli --version\n</code></pre>"}, {"location": "generative_ai/huggingface/#creating-a-project", "title": "Creating a Project", "text": "<p>Similar to GitHub, Hugging Face allows users to create and manage projects.  Here's how you can create a new project.</p> <p>Step 1: Log In First, log in to your Hugging Face account via the CLI:</p> <pre><code>huggingface-cli login\n</code></pre> <p>Enter your Hugging Face credentials when prompted.</p> <p>Step 2: Create a New Repository To create a new repository, use:</p> <pre><code>huggingface-cli repo create your-repo-name\n</code></pre> <p>Replace your-repo-name with the desired name for your repository.</p> <p>Step 3: Clone the Repository Clone your newly created repository:</p> <pre><code>git clone &lt;https://huggingface.co/username/your-repo-name&gt;\n</code></pre> <p>Replace username with your Hugging Face username and your-repo-name with the  repository name.</p>"}, {"location": "generative_ai/huggingface/#transformers", "title": "\ud83e\udd17 Transformers", "text": "<p>Load State-of-the-art Machine Learning models for PyTorch, TensorFlow, and JAX dynamically.</p> <p>\ud83e\udd17 Transformers provides APIs and tools to easily download and train state-of-the-art  pretrained models. Using pretrained models can reduce your compute costs, carbon   footprint, and save you the time and resources required to train a model from   scratch. These models support common tasks in different modalities, such as:</p> <p>\ud83d\udcdd Natural Language Processing: text classification, named entity recognition,  question answering, language modeling, summarization, translation, multiple  choice, and text generation. \ud83d\uddbc\ufe0f Computer Vision: image classification, object detection, and segmentation. \ud83d\udde3\ufe0f Audio: automatic speech recognition and audio classification. \ud83d\udc19 Multimodal: table question answering, optical character recognition, information  extraction from scanned documents, video classification, and visual question answering.</p> <p>\ud83e\udd17 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage  of a model\u2019s life; train a model in three lines of code in one framework, and  load it for inference in another. Models can also be exported to a format like  ONNX and TorchScript for deployment in production environments.</p>"}, {"location": "generative_ai/huggingface/#install-hugginface-tansformers", "title": "Install Hugginface tansformers", "text": "<p>Installation</p> Normal instalationpytorchtensorflow <pre><code>pip install transformers\n</code></pre> <pre><code>pip install 'transformers[torch]'\n</code></pre> <pre><code>pip install 'transformers[tf-cpu]'\n</code></pre>"}, {"location": "generative_ai/huggingface/#pipeline", "title": "Pipeline", "text": ""}, {"location": "generative_ai/huggingface/#datasets", "title": "Datasets", "text": "<p>\ud83e\udd17 Datasets is a library for easily accessing and sharing datasets for Audio, Computer Vision, and Natural Language Processing (NLP) tasks.</p> <p>Load a dataset in a single line of code, and use our powerful data processing methods to quickly get your dataset ready for training in a deep learning model. Backed by the Apache Arrow format, process large datasets with zero-copy reads without any memory constraints for optimal speed and efficiency. We also feature  a deep integration with the Hugging Face Hub, allowing you to easily load and   share a dataset with the wider machine learning community.</p> <p>Find your dataset today on the Hugging Face Hub,  and take an in-depth look inside of it with the live viewer.</p>"}, {"location": "generative_ai/huggingface/#example-of-a-dataset-generator-class", "title": "Example of a dataset generator class", "text": "<p>The GeneratorBasedBuilder class in the datasets library (often used with  Hugging Face's datasets module) is designed to create and manage datasets for   machine learning tasks, particularly in natural language processing. The    example demonstrates how to create a custom dataset class     WineRatings using this framework. Here's an overview of what the class does:</p> <p>Class Definition: WineRatings is a subclass of GeneratorBasedBuilder, meaning  it inherits functionalities for generating and processing datasets.</p> <p>Metadata: The class includes metadata such as _CITATION,_DESCRIPTION,  _HOMEPAGE, and_LICENSE, which provide information about the dataset, its   source, and usage terms.</p> <p>Dataset Version: VERSION is defined to keep track of different versions of the  dataset. This is useful for maintaining consistency and reproducibility in experiments.</p> <p>Dataset Information (_info method): This method defines the structure of the  dataset, specifying features like \"name\", \"region\", \"variety\", \"rating\", and   \"notes\". Each of these features is assigned a data type (e.g., string, float).</p> <p>Dataset Splits (_split_generators method): This method is used to define how  the dataset is split into different sets, typically \"train\", \"validation\", and   \"test\". It also specifies the file paths for each split.</p> <p>Data Generation (_generate_examples method): This method is crucial for  processing the data. It reads from the specified CSV files and yields   individual examples. Each example is a dictionary with keys corresponding to    the features defined in _info. The method iterates over the dataset,     converting each row of the CSV file into the structured format required by      the datasets library.</p> <p>In summary, the WineRatings class defines how to structure, split, and process  a dataset about wine ratings. It's a template for creating a custom dataset   that can be easily integrated with tools and models in the Hugging Face    ecosystem. This kind of setup is particularly useful for training and     evaluating machine learning models, as it standardizes data handling and      makes it easier to work with various datasets.</p> <pre><code>'''A wine-ratings dataset'''\nimport csv\nimport datasets\n_CITATION = \"\"\"\\\n@InProceedings{huggingface:dataset,\ntitle = {A wine ratings dataset from regions around the world},\nauthor={Alfredo Deza\n},\nyear={2022}\n}\n\"\"\"\n_DESCRIPTION = \"\"\"\\\nThis is a dataset for wines in various regions around the world with names,\n regions, ratings and descriptions\n\"\"\"\n_HOMEPAGE = \"https://github.com/paiml/wine-ratings\"\n_LICENSE = \"MIT\"\nclass WineRatings(datasets.GeneratorBasedBuilder):\nVERSION = datasets.Version(\"0.0.1\")\ndef _info(self):\nfeatures = datasets.Features(\n{\n\"name\": datasets.Value(\"string\"),\n\"region\": datasets.Value(\"string\"),\n\"variety\": datasets.Value(\"string\"),\n\"rating\": datasets.Value(\"float\"),\n\"notes\": datasets.Value(\"string\"),\n}\n)\nreturn datasets.DatasetInfo(\ndescription=_DESCRIPTION,\nfeatures=features,\nhomepage=_HOMEPAGE,\nlicense=_LICENSE,\ncitation=_CITATION,\n)\ndef _split_generators(self, dl_manager):\nreturn [\ndatasets.SplitGenerator(\nname=datasets.Split.TRAIN,\ngen_kwargs={\n\"filepath\": \"train.csv\",\n\"split\": \"train\",\n},\n),\ndatasets.SplitGenerator(\nname=datasets.Split.VALIDATION,\ngen_kwargs={\n\"filepath\": \"validation.csv\",\n\"split\": \"validation\",\n},\n),\ndatasets.SplitGenerator(\nname=datasets.Split.TEST,\ngen_kwargs={\n\"filepath\": \"test.csv\",\n\"split\": \"test\"\n},\n),\n]\n# method parameters are unpacked from `gen_kwargs` as given in `_split_generators`\ndef _generate_examples(self, filepath, split):\nwith open(filepath, encoding=\"utf-8\") as f:\ncsv_reader = csv.reader(f, delimiter=\",\")\nnext(csv_reader)\nfor id_, row in enumerate(csv_reader):\nyield id_, {\n\"name\": row[0],\n\"region\": row[1],\n\"variety\": row[2],\n\"rating\": row[3],\n\"notes\": row[4],\n}\n</code></pre> <p>Here there is  another example.</p> <p>With this class you can deal with data without uploading to hugginface like for  example here</p>"}, {"location": "generative_ai/huggingface/#installation", "title": "Installation", "text": "<pre><code>pip install datasets\n</code></pre>"}, {"location": "generative_ai/huggingface/#load-datasets", "title": "Load datasets", "text": "<pre><code>from datasets import load_dataset, list_datasets\navailable = list_datasets()\n# load the dataset dynamically\nmovie_rationales = load_dataset('movie_rationales)\n# the object is a dict-like mapping of actual datasets\ntrain = movie_rationales['train]\ndf = train.to_pandas()\n</code></pre>"}, {"location": "generative_ai/huggingface/#key-components", "title": "Key Components", "text": ""}, {"location": "generative_ai/huggingface/#tokenizers", "title": "Tokenizers", "text": "<p>These work like a translator, converting the words we use into smaller parts   and creating a secret code that computers can understand and work with.</p> <p>HuggingFace tokenizers help us break down text into smaller, manageable pieces  called tokens. These tokenizers are easy to use and also remarkably fast due to   their use of the Rust programming language.</p> <p>Hugging Face provides pretrained tokenizers through its flexible API as part of  the `transformers`` Python library.</p> <p>You can:</p> <pre><code>* Use a tokenizer off the shelf. (Don't need to modify it at all).\n* Fine tuning with your own data.\n* Train your tokenizer from scratch.\n</code></pre>"}, {"location": "generative_ai/huggingface/#encoding-text", "title": "Encoding text", "text": "<p>To tokenize text with Hugging Face, instantiate a tokenizer object with the  `AutoTokenizer.from_pretrained`` method. Pass in the name of the model as a  string value.</p> <pre><code># 'bert-base-cased' can be replaced with a different model as needed\nmy_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n</code></pre> <p>Then you can use the tokenizer object to generate either string tokens or integer  ID tokens.</p> <p>To generate string tokens, including special tokens:</p> <pre><code>tokens = my_tokenizer(raw_text).tokens()\n</code></pre> <p>To generate integer ID tokens you can use the <code>.encode</code> method on raw text, or the <code>.convert_tokens_to_ids</code> method on string tokens.</p> <pre><code># Option for raw text\ntoken_ids = my_tokenizer.encode(raw_text)\n# Option for string tokens\ntoken_ids = my_tokenizer.convert_tokens_to_ids(tokens)\n</code></pre>"}, {"location": "generative_ai/huggingface/#decoding-text", "title": "Decoding text", "text": "<p>Integer ID tokens can be converted back to text using the <code>.decode</code> method:</p> <pre><code>decoded_text = my_tokenizer.decode(token_ids)\n</code></pre>"}, {"location": "generative_ai/huggingface/#unknown-tokens", "title": "Unknown tokens", "text": "<p>Pretrained tokenizers have a predetermined vocabulary. If a token is not in the tokenizer's vocabulary, it will be lost in the encoding + decoding process. In this example, unknown tokens were replaced with <code>[UNK]</code>, but this behavior will vary depending on the tokenizer.</p>"}, {"location": "generative_ai/huggingface/#documentation", "title": "Documentation", "text": "<ul> <li>Hugging Face's PreTrainedTokenizer</li> <li>Hugging Face's AutoTokenizer</li> </ul>"}, {"location": "generative_ai/huggingface/#tutorial", "title": "Tutorial", "text": "<ul> <li>Tokenizer intro</li> </ul>"}, {"location": "generative_ai/huggingface/#models", "title": "Models", "text": "<p>These are like the brain for computers, allowing them to learn and make  decisions based on information they've been fed.</p>"}, {"location": "generative_ai/huggingface/#datasets_1", "title": "Datasets", "text": "<p>Think of datasets as textbooks for computer models. They are collections of   information that models study to learn and improve.</p>"}, {"location": "generative_ai/huggingface/#trainers", "title": "Trainers", "text": "<p>Trainers are the coaches for computer models. They help these models get better  at their tasks by practicing and providing guidance. HuggingFace Trainers implement   the PyTorch training loop for you, so you can focus instead on other aspects   of working on the model.</p>"}, {"location": "generative_ai/huggingface/#resources", "title": "Resources", "text": "<ul> <li>Fine tuning hugginface model</li> <li>MLops codespace template -&gt;    Create a github codespace with some utilities around cuda and hugginface.</li> <li>Example Notebooks</li> <li>Train Bert for classification</li> </ul>"}, {"location": "generative_ai/instruction_fine_tuning/", "title": "Instruction Fine-Tuning", "text": ""}, {"location": "generative_ai/instruction_fine_tuning/#instruction-fine-tuning", "title": "Instruction Fine-Tuning", "text": "<p>Fine-tuning is the process of using labelled data to adapt a pre-trained model to a specific task or tasks. The data consists of prompt-completion pairs. Note that fine-tuning is applied on a pre-trained model and is supervised, as opposed to self-supervised.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#limitations-of-icl", "title": "Limitations of ICL", "text": "<p>We saw how some models are capable of identifying instructions contained in a prompt and correctly carrying out zero-shot inference. On the other hand, we also saw smaller models which fail to do so. In such cases, we use In-Context Learning (ICL) to make the model follow our instructions. There are some disadvantages to this:</p> <ul> <li>ICL may not always work for smaller models.</li> <li>Examples take up space in the context window, reducing the space available</li> </ul> <p>to add useful information in the prompt. To combat these disadvantages while having a model that can follow instructions, we can use instruction fine-tuning.</p> <p>Instruction fine-tuning is a fine-tuning technique used to improve a model\u2019s performance on a variety of tasks. Here, the training samples are prompts containing instructions while the labels are the expected response of the model in order to follow that instruction:</p> <ul> <li>Example: If we want to fine-tune a model to improve its summarization ability,     the dataset will contain prompts which look like as     follows:</li> <li>Prompt: Summarize the following text (EXAMPLE TEXT)</li> <li>Completion: Summarize the following text (EXAMPLE TEXT)     (EXAMPLE COMPLETION)</li> </ul> <p>Instruction fine-tuning where all of the model\u2019s weights are updated is called full fine-tuning. This results in a new version of the model with updated weights. Note that full fine-tuning requires enough memory and compute budget to store all the gradients, optimizer states and other components updated during training (see Efficient Multi-GPU Compute Strategies).</p>"}, {"location": "generative_ai/instruction_fine_tuning/#common-steps-involved-in-instruction-fine-tuning", "title": "Common Steps Involved in Instruction Fine-Tuning", "text": ""}, {"location": "generative_ai/instruction_fine_tuning/#prepare-the-dataset", "title": "Prepare the Dataset", "text": "<p>There are many publicly available datasets that have been used to train previous generations of LLMs. Most of these datasets are not formatted as instructions. Developers have built prompt template libraries that can be used to take existing datasets (for example, Amazon product reviews) and turn them into instruction prompt datasets for fine-tuning. Prompt template libraries include many templates for different tasks. For example:</p> <p></p> <p>Notice how each of the templates has an instruction in it: predict the associated rating, generate an x-star review and give a short sentence describing the following product review. The result is a prompt with an instruction and the example from the original dataset.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#split-dataset", "title": "Split Dataset", "text": "<p>After the dataset is prepared, like any supervised problem, we split the dataset into training, validation and test sets.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#training", "title": "Training", "text": "<p>The fine-tuning training loop is similar to any other supervised training loop:</p> <ul> <li>Pass the training data in batches to the model and obtain predictions.</li> <li>Calculate the loss. The output of an LLM is a probability distribution over the tokens available in the dataset. Thus, we can compare the probability distribution of the prediction with that of the label and use the standard cross-entropy loss to calculate the loss.</li> <li>Calculate some evaluation metric.</li> <li>Pass the validation data to the model and obtain predictions.</li> <li>Calculate the loss (optional) and the same evaluation metric.</li> <li>Backpropagate the loss to update the weights and repeat from the beginning as the next epoch. After training is done, we can evaluate the final performance of the model by passing it the test data and measuring the evaluation metric on model predictions. This process leads to a new version of the model, often called an Instruct Model. It tends to perform better at the tasks we have fine-tuned it for.</li> </ul>"}, {"location": "generative_ai/instruction_fine_tuning/#fine-tuning-on-a-single-task", "title": "Fine-Tuning On a Single Task", "text": "<p>Fine-tuning on a single task can be done by simply using a single-task dataset. That is, all prompt-completion pairs in the dataset have the same basic instruction in them. Example: Summarize the following text: (EXAMPLE TEXT) (EXAMPLE COMPLETION) In most cases, only a small dataset (500-1000 examples) is required to achieve good performance on a single-task.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#catastrophic-forgetting", "title": "Catastrophic Forgetting", "text": "<p>Fine-tuning on a single task can lead to a problem called catastrophic forgetting. This happens since full fine-tuning changes the weights of the original LLM. This leads to great performance on the task we are fine-tuning for but can degrade performance on other tasks. For example, a model fine-tuned for sentiment analysis might become very good at the task, but might fail on something like named entity recognition despite being performant on it before fine-tuning.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#avoiding-catastrophic-forgetting", "title": "Avoiding Catastrophic Forgetting", "text": "<p>First, we have to figure out whether our model is actually impacted by the problem. For example, if we require reliable performance only on the single task we are fine-tuning for, we do not need to worry about catastrophic forgetting. But, if we want the model to maintain its multi-task performance, we can perform fine-tuning on multiple tasks at the same time. This generally requires 50,000-100,000 examples across many tasks. Another alternative is Parameter Efficient Fine-Tuning (PEFT). PEFT preserves the weights of the original LLM and trains only a small number of task-specific adapter layers and parameters (see Parameter Efficient Fine-Tuning (PEFT)).</p>"}, {"location": "generative_ai/instruction_fine_tuning/#fine-tuning-on-multiple-tasks", "title": "Fine-Tuning On Multiple Tasks", "text": "<p>In case of multiple tasks, the dataset contains prompt-completion pairs related to multiple tasks. Example:</p> <ul> <li>Summarize the following text:</li> <li>Rate this review:</li> <li>Translate into Python code:</li> <li>Identify the places:</li> </ul> <p>The model is trained on this mixed dataset to fine-tune on multiple tasks simultaneously and remove the risk of catastrophic forgetting.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#case-study-flan", "title": "Case Study - FLAN", "text": "<p>FLAN (Fine-tuned Language Net) is a family of models fine-tuned on multiple tasks. FLAN models refer to a specific set of instructions used to perform instruction fine-tuning.</p> <p>FLAN-T5 is the FLAN instruct version of the T5 foundation model while FLAN-PALM is the FLAN instruct version of the PALM foundation model.</p> <p>FLAN-T5 is general purpose instruct model. It is fine-tuned on 473 datasets across 146 task categories. These datasets are chosen from other models and papers.</p> <p></p> <p>For example, the SAMSum dataset is a text summarization dataset. SAMSum has 16,000 messenger-like conversations with their summaries. They were crafted by linguists for the express purpose of training LLMs.</p> <p>Note that while FLAN models are general-purpose, we might still need Domain Adaptation for it to make it work well for our application.</p>"}, {"location": "generative_ai/instruction_fine_tuning/#resources", "title": "Resources", "text": "<ul> <li>Create a sentiment classifier with Bert</li> </ul>"}, {"location": "generative_ai/introduction/", "title": "Introduction", "text": ""}, {"location": "generative_ai/introduction/#generative-ai", "title": "Generative AI", "text": "<p>It is a subset of traditional ML. The ML algorithms that work behind generative AI do so by exploiting the statistical patterns present in the massive datasets of content that was originally generated by humans.</p>"}, {"location": "generative_ai/introduction/#terminology", "title": "Terminology", "text": "<ul> <li>Prompt: The input given to an LLM is called the prompt.</li> <li>Context Window: The space/memory that is available to the prompt is called the  context window. It is essentially the maximum size of the prompt that the model  can handle before it performs poorly. It is limited to a few thousand of words  but also varies model to model.</li> <li>Completion: The output of the an LLM when given a prompt is called the completion.     Generally, the completion consists of the prompt and the text generated by     the model by repeatedly generating the next token or word, though almost all     applications omit the prompt from the model\u2019s output when showing it to users</li> </ul>"}, {"location": "generative_ai/introduction/#llms", "title": "LLMs", "text": ""}, {"location": "generative_ai/introduction/#definition", "title": "Definition", "text": "<p>LLMs (Large Language Models) are generative AI models specifically designed to understand text. All LLMs are powered by the Transformer (Google, 2017) architecture. They are designed to take in input text and repeatedly generate the next token or word that appropriately \u201ccompletes\u201d the input text. For example, an LLM can be given the input:     Where is Ganymede located in the solar system? In response, the LLM might generate the following output:     Ganymede is a moon of Jupiter and is located in the solar system     within Jupiter\u2019s orbit. Here, the model essentially completed the given input by repeatedly generating the next word or token that fits appropriately. These models have abilities beyond just language and are capable of breaking down complex tasks, reasoning and problem solving. It is commonly accepted that as the size (in terms of number of parameters) of an LLM increases, so does its understanding of language. At the same time, it is also true the smaller models can be fine-tuned to perform well on specific tasks.</p>"}, {"location": "generative_ai/introduction/#foundation-model", "title": "Foundation Model", "text": "<p>A foundation model is a powerful AI tool that can do many different things after being trained on lots of diverse data. These models are incredibly versatile and provide a solid base for creating various AI applications, like a strong foundation holds up different kind of buildings. By using a foundation model, we have a strong starting point for building specialized AI tasks.</p> <p>Foundation Models (GPT, BERT) and Traditional Models (Linear regression, SVM) are two distinct approaches in the field of artificial intelligence with different strengths. Foundation Models, which are built on large, diverse datasets, have the incredible ability to adapt and perform well on many different tasks. In contrast, Traditional Models specialize in specific tasks by learning from smaller, focused datasets, making them more straightforward and efficient for targeted applications.</p>"}, {"location": "generative_ai/introduction/#pre-training-large-language-models", "title": "Pre-training Large Language Models", "text": ""}, {"location": "generative_ai/introduction/#initial-training-process-pre-training", "title": "Initial Training Process (Pre-training)", "text": "<p>The initial training process of an LLM is called as pre-training. LLMs work by learning a deep statistical representation of language and this deep representation is developed during pre-training. At a high-level, during pre-training, the model is fed large amounts of unstructured  textual data, ranging from gigabytes to petabytes in size. The data is pulled from many sources such as web crawling and corpora of text compiled specifically to train LLMs. The pre-training process is self-supervised. The model internalizes the patterns and structures present in the language. These patterns then unable the model to complete its training objective, which depends on the architecture of the model. In other words, during pre-training, the model weights get updated to minimize the loss of training objective. Clearly, this step requires a lot of compute and the use of GPUs.</p> <p>Additionally, since the data is coming from public sources such as the internet, there is often a data quality filter applied before feeding the data to the LLM so that the training data is of high quality, has low bias and does not have harmful content. Due to this, only about 1-3% of the original tokens are used for pre-training.</p>"}, {"location": "generative_ai/introduction/#training-objectives-for-transformer-variants", "title": "Training Objectives for Transformer Variants", "text": "<p>The three configurations of a Transformer are trained with different training objectives and thus, learn to perform different tasks.</p> <p>Popular transformer-based models differ in not only architecture but also pre-training objectives.</p> <ul> <li>Autoregressive (AR): Predicting the next token using its own last output</li> <li>Denoising autoencoder: Predicting tokens based on the pretext that the data has been corrupted</li> <li>Contrastive: Aligning different inputs or views of the same input and constructing (positive, negative) pairs.</li> </ul> <p>Some pre-training objectives are better than others for self-supervised learning; this depends on whether the ground truth can be constructed within the data structure, or whether it requires manual annotation.</p>"}, {"location": "generative_ai/introduction/#transformers", "title": "Transformers", "text": "<p>In mid-2017, Google published Attention Is All You Need  and introduced the Transformer model to the world. There are many wonderful works   explaining the Transformer model, in particular the Illustrated   Transformer (a wonderful   introduction) and the   Annotated Transformer    (with a line-by-line implementation in Python).</p> <p>Transformer architectures consist of building block of self-attention layers.</p> <p>Keep in mind that the original transformer from Vaswani et al. (2017) had both  an encoder and a decoder, but that newer transformer-based models often use   just the encoder or just the decoder portion.</p> <p></p>"}, {"location": "generative_ai/introduction/#encoder-only-models", "title": "Encoder-only Models", "text": "<p>The first LLM to gain broad adoption was BERT (Bidirectional  Encoder Representations from Transformers), an encoder-only model. Encoder-only  models are most commonly used as base models for subsequent fine-tuning with a  distinct objective, e.g. for the inference-time task of binary classification of  movie reviews. (Mask some words)</p> <p>The encoder-only variants of Transformers are also called autoencoding models. They are pre-trained using Masked Language Modeling (MLM). In MLM, tokens in the input sequence are randomly masked and the training objective is to predict the masked tokens in order to reconstruct the original input sequence. This is also called a denoising objective since the masking of the tokens can be thought of as adding noise to the input sequence and then predicting the masked tokens can be thought of as removing that noise from the input sequence. Autoencoding models build bidirectional context representations of the input sequence, meaning that model has an understanding of the full context of the token rather than just the tokens that come before it.</p> <p></p> <p>These models are usually suited to tasks that benefit from this bidirectional context such as sentiment analysis, named entity recognition and word classification, etc. Examples: BERT, ROBERTA.</p> <p>BERT has a denoising autoencoder objective. Specifically, it uses masked  language modeling.</p> <p>This means that a fraction of the tokens are masked, and the task for the  encoder is to predict those masked words.</p> <p>BERT also optimizes for next sentence prediction. This is fairly different  from a next token prediction\u2014BERT is not generating the next sentence.  It is performing a binary classification of whether or not the second   sentence belongs after the first.</p>"}, {"location": "generative_ai/introduction/#decoder-only-models", "title": "Decoder-only Models", "text": "<p>However, before BERT was released, the first GPT  (Generative Pre-Trained Transformer) model, a decoder-only model, was released  by OpenAI. Decoder-only models are most commonly used for the inference-time  task of text generation. In distinction to encoder-only models, the  Transformer's pre-training objective of next token prediction is very similar  to the decoder-only model's inference-time task of text generation.</p> <p>The decoder-only variants of Transformers are also called autoregressive models. They are pre-trained using Causal Language Modeling (CLM). In CLM, the training objective is to predict the next token based on the previous sequence of tokens. The tokens of the input sequence are masked and the model can only see the input tokens leading up to the token being predicted at the moment. The model has no knowledge of the tokens that come after this token. The model then iterates over the input sequence one-by-one to predict the next token. Thus, in contrast to autoencoding models, the model builds a unidirectional context for each token.</p> <p></p> <p>By learning to predict the next token from a vast number of examples, the model builds a statistical representation of the language. Predicting the next token is sometimes called full language modeling by researchers. These mode,ls are most suitable for text generation but large autoregressive models also show strong zero-shot inference ability and can perform a variety of tasks. Examples: GPT, BLOOM.</p> <p>GPT has an autoregressive objective. It assumes that there is some kind of continuity or dependency between a value and its predecessors.</p> <p>The attention scores for future tokens are set to negative infinity to prevent  \"cheating\", and then the model proceeds to pick the highest probability   candidate for the next token.</p> <p>A technique called \"teacher forcing\"\u2014that has been in use since the 1980s\u2014can  be used to prevent the model from accumulating mistakes and continuing on a   vicious feedback loop during training.</p>"}, {"location": "generative_ai/introduction/#encoder-decoder-models-sequence-to-sequence-models", "title": "Encoder-Decoder Models (Sequence-to-Sequence Models)", "text": "<p>Encoder-Decoder Models (Sequence-to-Sequence Models) The encoder-decoder variants of Transformers are also called sequence-tosequence  models. The exact details of pre-training objective vary from model to model.  For example, FLAN-T5 is trained using span corruption. In span corruption, a part of the input sequence is masked and replaced by a sentinel token. These sentinel tokens are special tokens added to the vocabulary that to do not correspond to any actual word from the dataset. The decoder then has to reconstruct the sentence autoregressively. The output is the sentinel token followed by the predicted tokens.</p> <p></p> <p>We can use such models for tasks such as translation, summarization and question answering. They are most useful where the input and output both are bodies of text. Examples: FLAN-T5, BART.</p>"}, {"location": "generative_ai/introduction/#attention-scores", "title": "Attention Scores", "text": "<p>The attention calculation computes the similarity matrix between queries and keys \\(QK^{T}\\) then uses a softmax function to convert the scores into a probability distribution, which is multiplied by the values embeddings \\(V\\) to produce the output vector \\(Z\\)</p> <p>Before applying the softmax function, \\(QK^{T}\\)  is divided by \\(\\sqrt{d_{k}}\\) (the square root of the dimension of \\(K\\)) in order to avoid tiny gradients at  extreme values.</p> <p>Attention scores describes the mathematical definition of attention. Attenntion mechanisms are about how mathematical operations are applied to different set of queries, kyes and values.</p> <p>This creates the following complete formula for scaled multiplicative attention:</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\]"}, {"location": "generative_ai/introduction/#types", "title": "Types", "text": ""}, {"location": "generative_ai/introduction/#multiplicative-attention-dot-product-attention", "title": "Multiplicative Attention (Dot-Product Attention)", "text": "<p>Multiplicative attention, also known as dot-product attention, calculates the attention scores by performing a dot product between the query and the key. It is a simple and efficient way to measure the similarity between the query and the key.</p> <p>\\(Attention(Q, K, V) = softmax(QK^{T})V\\)</p> <p>In this formula, \\(Q\\) represents the query matrix, \\(K\\) the key matrix, and \\(V\\) the value matrix. The attention scores are obtained by first calculating the dot product between \\(Q\\) and \\(K^{T}\\) (the transpose of \\(K\\)), followed by applying the softmax function to ensure the scores are normalized to sum up to 1.</p>"}, {"location": "generative_ai/introduction/#additive-attention", "title": "Additive Attention", "text": "<p>Additive attention, also known as content-based attention, computes the attention scores by adding the query and the key together, usually followed by a non-linear activation function such as tanh, and then projecting the result through a learnable weight matrix to produce the scores.</p> <p>\\(Attention(Q, K, V) = softmax(W[(Q + K)^{T}])V\\)</p> <p>Here, \\(W\\) is a weight matrix that is learned during training. The addition of \\(Q\\) and \\(K\\) allows for a more flexible interaction between the query and the keys, potentially capturing more complex dependencies.</p>"}, {"location": "generative_ai/introduction/#general-attention", "title": "General Attention", "text": "<p>General attention is a variant of multiplicative attention where the similarity  between the query and the key is calculated using a learnable weight matrix.  This allows the model to learn an optimal way of computing attention scores.</p> <p>\\(Attention(Q, K, V) = softmax(QW_kK^{T})V\\)</p> <p>In this case, \\(W_{k}\\) is a learnable weight matrix that transforms the key before  computing the dot product with the query. This adds an additional level of   flexibility compared to the standard dot-product attention, as the model can   learn the most effective way to compare queries and keys.</p> <p>Each of these attention mechanisms provides a different way to calculate how much focus or \"attention\" should be given to different parts of the input when  performing a task, allowing models to dynamically weigh the importance of   different elements in the data.</p>"}, {"location": "generative_ai/introduction/#attention-mechanisms", "title": "Attention Mechanisms", "text": "<p>Attention scores describes the mathematical definition of attention. Attenntion mechanisms are about how mathematical operations are applied to different set of queries, kyes and values.</p> <p>when some part of a neural network applies an attention score or mechanism to some information, you might think that the standard phrasing would be \"A pays  attention to B\". Instead, we typically say that \"A attends to B\". This standard   terminology is more concise and less anthropomorphizing.</p>"}, {"location": "generative_ai/introduction/#self-attention", "title": "Self-Attention", "text": "<p>This was the original attention mechanism described by Vaswani et al. (2017).</p> \\[Q = K = V\\] <p>Each position in the sequence attends to all positions in the same sequence.</p> <p>Self-attention has an interaction distance of \\(O(1)\\), which is a major improvement over the \\(O(n)\\) interaction distance of an RNN. The drawback is that it has \\(O(n^2)\\) computation.</p>"}, {"location": "generative_ai/introduction/#multi-head-attention", "title": "Multi-Head Attention", "text": "<p>Multiple Q, K, V heads</p> <p>Each head has its own set of weights, and attention is distributed, with each head focusing on a different specialized feature of the input.</p>"}, {"location": "generative_ai/introduction/#multi-query-attention", "title": "Multi-Query Attention", "text": "<p>Multiple Q heads, 1 K, V head</p> <p>Each query is distinct and shares the same key and value. Compared to multi-head  attention, it is more efficient and can significantly increase the training  batch size through parameter sharing.</p> <p>Billion-parameter LLMs like Falcon and LLaMA 2 use multi-query attention.</p>"}, {"location": "generative_ai/introduction/#cross-attention", "title": "Cross-Attention", "text": "<p>Q \u2260 K, V</p> <p>Used to \"cross-reference\" between different modalities or sequences. This mechanism is especially useful for tasks where different data types interact,  e.g., image captioning.</p>"}, {"location": "generative_ai/introduction/#attention-problems-and-solutions", "title": "Attention problems and solutions", "text": "<p>If self-attention is so powerful, can it replace RNNs altogether? Vaswani et al.  (2017) argue that it can, with the right solutions in place to address its limitations.</p> <ul> <li> <p>Problem: lack of input order</p> <ul> <li>As previously described with ELMo, context is important for understanding  the meaning of words.</li> <li>Self-attention doesn't understand this by default, so we add positional  encodings as part of the input embeddings.</li> </ul> </li> <li> <p>Problem: no nonlinearity between repeated self-attention layers</p> <ul> <li>The reason we typically use an activation function like ReLU in a neural network layer, rather than just a linear output, is to enable the model to capture more complexity. Linear outputs can be reduced to a simple \\((y = mx + b)\\) style formula.</li> <li>Self-attention layers don't have this nonlinearity by default, so we add a feed-forward network for each processed token afterward.</li> </ul> </li> <li> <p>Problem: \"cheating\" when predicting a sequence</p> <ul> <li>The goal of a deep learning model is to be able to predict some unknown information given some known information. If all of the information is known,  the model can't learn the relationships properly.</li> <li>By default, self-attention can look at all of the data, including the \"future\" that it is trying to predict. To prevent this, we mask attention on future words during decoding.</li> </ul> </li> <li> <p>Explainable AI: Visualizing Attention in Transformers</p> </li> </ul>"}, {"location": "generative_ai/introduction/#positional-embeddings", "title": "Positional Embeddings", "text": "<p>Because attention layers don't understand order by default, we encode the  ordering of the tokens in an embedding. Algorithms for positional embeddings include:</p> <ul> <li> <p>Absolute positional embeddings:</p> <ul> <li>Sinusoidal (original)</li> <li>Learned (BERT, GPT)</li> </ul> </li> <li> <p>Relative positional embeddings</p> <ul> <li>TransformerXL</li> </ul> </li> <li> <p>Rotary positional embeddings</p> </li> </ul>"}, {"location": "generative_ai/introduction/#residual-connections", "title": "Residual Connections", "text": "<p>A residual connection is a type of skipped layer, designed to address the  vanishing gradient problem. It originated from the 2015 ResNets paper by He   et al. and was originally used for computer vision tasks.</p>"}, {"location": "generative_ai/introduction/#layer-normalization", "title": "Layer Normalization", "text": "<p>Layer normalization is the process of subtracting the mean and dividing by the  standard deviation of the inputs for each sample. This stabilizes and speeds   up the model training.</p> <p>Similar to the scaled attention score described previously, this step is  helpful because of how it impacts the gradients that neural networks use for backpropagation.</p>"}, {"location": "generative_ai/introduction/#prompting-and-prompt-engineering", "title": "Prompting and Prompt Engineering", "text": "<p>We define a prompt as:</p> <ul> <li>\\(X \\rightarrow\\) The user prompt, is like the features in ML.</li> <li>\\(Y \\rightarrow\\) Masked (Next token)/Response(Next Token) Like Y label in ML.</li> <li>\\(\\theta \\rightarrow\\)  Model weights</li> </ul>"}, {"location": "generative_ai/introduction/#prompting", "title": "Prompting", "text": "<p>The text that is fed to LLMs as input is called the prompt and the act of providing the input is called prompting.</p>"}, {"location": "generative_ai/introduction/#prompt-engineering", "title": "Prompt Engineering", "text": "<p>The process of tweaking the prompt provided to an LLM so that it gives the best possible result is called prompt engineering. Some common techniques are given below.</p>"}, {"location": "generative_ai/introduction/#in-context-learning-icl", "title": "In-Context Learning (ICL)", "text": "<p>In ICL, we add examples of the task we are doing in the prompt. This adds more context for the model in the prompt, allowing the model to \u201clearn\u201d more about the task detailed in the prompt.</p>"}, {"location": "generative_ai/introduction/#zero-shot-inference", "title": "Zero-Shot Inference", "text": "<p>For example, we might be doing semantic classification using our LLM. In that case, a prompt could be:</p> <pre><code>Classify this review: I loved this movie!\nSentiment:\n</code></pre> <p>This prompt works well with large LLMs but smaller LLMs might fail to follow the instruction due to their size and fewer number of features. This is also called zero-shot inference since our prompt has zero examples regarding what the model is expected to output.</p>"}, {"location": "generative_ai/introduction/#few-shot-inference", "title": "Few-Shot Inference", "text": "<p>This is where ICL comes into play. By adding examples to the prompt, even a smaller LLM might be able to follow the instruction and figure out the correct output. An example of such a prompt is shown below. This is also called one-shot inference since we are providing a single example in the prompt:</p> <pre><code>Classify this review: I loved this movie!\nSentiment: Positive\nClassify this review: I don\u2019t like this chair.\nSentiment:\n</code></pre> <p>Here, we first provide an example to the model and then ask it to figure out the output for the I don\u2019t like this chair review. Sometimes, a single example won\u2019t be enough for the model, for example when the model is even smaller. We\u2019d then add multiple examples in the prompt. This is called few-shot inference.</p> <p>In other words:</p> <ul> <li>Larger models are good at zero-shot inference.</li> <li>For smaller models, we might need to add examples to the prompt, for few-shot inference.</li> </ul>"}, {"location": "generative_ai/introduction/#inference-configuration-parameters", "title": "Inference Configuration Parameters", "text": "<ul> <li>Max New Tokens: This is used to limit the maximum number of new tokens that     should be generated by the model in its output. The model might output     fewer tokens (for example,it predicts  before reaching the limit)      but not more than this number. <li>Greedy vs Random Sampling: Some models also give the user control over whether  the model should use greedy or random sampling.</li> <li> <p>Sample Top-K and Sample Top-P: Sample Top-K and Sample Top-P are used to limit  the random sampling of a model. A top-K value instructs the model to only consider K words with the highest probabilities in its random sampling. Consider the following softmax output:</p> <pre><code>Probability Word\n0.20 cake\n0.10 donut\n0.02 banana\n0.01 apple\n. . . . . .\n</code></pre> <p>If K = 3, the model will select one of cake, donut or banana. This allows the model to have variability while preventing the selection of some highly improbable words in its output. The top-P value instructs the model to only consider words with the highest probabilities such that their cumulative probability, p1 + p2 + \u00b7 \u00b7 \u00b7 + pK \u2264 P. For example, considering the above output, if we set P = 0.30, the model will only consider the words cake and donut since 0.20 + 0.10 \u2264 0.30.</p> </li> <li> <p>Temperature: Temperature is also another parameter used to control random sampling. It determines the shape of the probability distribution that the model  calculates for the next word.     Intuitively, a higher temperature increases the randomness of the model while     a lower temperature decreases the randomness of the model. This temperature is     passed as a scaling factor to the final softmax layer of the decoder.     If we pick a cooler temperature (T &lt; 1), the probability distribution is strongly     peaked. In other words, one (or a few more) words have very high probabilities     while the rest of the words have very low probabilities:</p> <pre><code>Probability Word\n0.001 apple\n0.002 banana\n0.400 cake\n0.012 donut\n. . . . . .\n</code></pre> <p>Notice how cake has a 40% chance of being picked while other words have very small chances of being picked. The resulting text will be less random. On the other hand, if we pick a warmer temperature (T &gt; 1), the probability distribution is broader, flatter and more evenly spread over the tokens:</p> <pre><code>```bash\nProbability Word\n0.040 apple\n0.080 banana\n0.150 cake\n0.120 donut\n. . . . . .\n```\n</code></pre> <p>Notice how none of the words have a clear advantage over the other words. The model generates text with a higher amount of randomness and has more variability in its output. Clearly, when T = 1, the model uses the softmax output as is for random sampling.</p> <p>You can find more information in how the different decoders works Link</p> </li>"}, {"location": "generative_ai/introduction/#virtuous-feedback-loops", "title": "Virtuous Feedback Loops", "text": "<p>Setting aside the model weights, we can see how the LLM can enable virtuous feedback loops (as opposed to the vicious feedback loops seen earlier) by considering how the LLM can aid in the generation of helpful prompt context.</p> <p>Recall for the prediction of \\(Y\\), given \\(X\\), \\(P(Y|X)\\), that the LLM generates each token in Y, one at a time, appending the previously generated token to \\(X\\). In the case of a prompt, \\(n\\) tokens long, and a complete prompt + LLM response \\(k\\) tokens long, then the tokens in \\(X\\) from \\(n+1\\) to \\(k-1\\) came from the LLM. This can be represented mathematically below:</p> <p>\\(P(Y_{n+1} | X_{0,1..n}) \\rightarrow\\) \\(P(Y_{n+2} | X_{0,1..n+1}) \\rightarrow\\) \\(P(Y_{n+3} | X_{0,1..n+2}) \\rightarrow\\) \\(\\dots \\rightarrow P(Y_k | X_{0,1..k-1})\\)</p> <p>Techniques that guide the LLM toward generating assistive tokens at the start of the LLM response can aid in answering a question at the end of the prompt,  effectively helping the LLM write its own features.</p> <p>An example of a virtuous feedback loop is provided by \u201cChain of Thought\u201d</p>"}, {"location": "generative_ai/introduction/#workflow-to-try-prompts", "title": "Workflow to try prompts", "text": "<ul> <li>Notebook</li> </ul>"}, {"location": "generative_ai/introduction/#data", "title": "Data", "text": ""}, {"location": "generative_ai/introduction/#most-common-sources", "title": "Most common sources", "text": "<ul> <li>Common crawl</li> <li>Github dataset</li> <li>Wikipedia</li> <li>Gutenberg</li> <li>Stanford Question Answering Dataset</li> <li>HotPotQA</li> <li>SubjQA</li> </ul>"}, {"location": "generative_ai/introduction/#generating-traing-data-with-llms", "title": "Generating traing data with LLMs", "text": "<p>LLMs can be used for the generation of training data along multiple dimensions, including:</p> <ul> <li> <p>Generation of responses from pre-existing queries_</p> <p>Enabling instruction fine-tuning dataset pairs:</p> <pre><code>{\"prompt\": &lt;existing_instruction&gt;, \"completion\": &lt;LLM_generated_response&gt;}\n</code></pre> <p>This is probably the most straightforward usage of LLMs for synthetic dataset generation. Ensure LLM's license supports such usage.</p> </li> <li> <p>Generation of instructions from pre-existing documents.     Enabling instruction fine-tuning dataset pairs:</p> <pre><code>{\"prompt\": &lt;LLM_generated_instruction&gt;, \"completion\": &lt;existing_document_chunk&gt;}\n</code></pre> <p>This method, known as back-translation from the development of translation systems for low-resource languages, has been given this modern spin in \"Self-Alignment with Instruction Backtranslation\"</p> </li> <li> <p>Generation of preference data from existing prompt/response LLM pairs:     For example, the comparison of:</p> <pre><code>{{\"prompt\": &lt;prompt_0&gt;, \"completion\": &lt;completion_a&gt;}, \"pref\": LLM_generated_preference_1}}\n</code></pre> <p>vs.</p> <pre><code>{{\"prompt\": &lt;prompt_0&gt;, \"completion\": &lt;completion_a&gt;}, \"pref\": LLM_generated_preference_-1}}\n</code></pre> <p>This can be used for Reinforcement Learning from Human Feedback (despite the feedback being non-human in this case), as explored more in \"Constitutional AI: Harmlessness from AI Feedback\".</p> </li> </ul>"}, {"location": "generative_ai/introduction/#resources", "title": "Resources", "text": "<ul> <li>Explainable AI: Visualizing Attention in Transformers</li> <li>Illustrated   Transformer (a wonderful   introduction) and the</li> <li>Annotated Transformer    (with a line-by-line implementation in Python).</li> <li>How decoder works</li> </ul>"}, {"location": "generative_ai/lora/", "title": "LoRa", "text": ""}, {"location": "generative_ai/lora/#introduction", "title": "Introduction", "text": "<p>LoRA is a PEFT technique based on reparameterization. The encoder and decoder blocks of a Transformer consist of self-attention (in the form of Multi-Headed Attention) layers. Weights are applied to the input embedding vectors to obtain an attention map for the input prompt. In full fine-tuning, every weight in these layers is updated. In LoRA:</p> <ul> <li>All the model parameters are frozen.</li> <li>Two (smaller) rank decomposition matrices A and B are injected with the original weights. The dimensions of the matrices are such that their product has the same dimension as that of the original weight matrices.</li> <li> <p>The weights in the smaller matrices are trained via fine-tuning. For inference:</p> </li> <li> <p>We multiply the two low rank matrices to obtain B \u00d7 A, which has the same dimensions as the frozen weights of the model.</p> </li> <li>We add B \u00d7 A to the original frozen weights.</li> <li>The model weights are replaced with these new weights.</li> </ul> <p>We now have a fine-tuned model which can carry out the task(s) we have finetuned it for. Since the model has the same number of parameters as original, there is little to no impact on inference latency. Researchers have found that applying LoRA just to the self-attention layers is often enough to fine-tune for a task and achieve performance gains. However, in principle, we can use LoRA in other components such as the feed-forward layers. Since most of the parameters are the model are in the attention layers, we get the biggest savings when we apply LoRA in those layers.</p>"}, {"location": "generative_ai/lora/#multiple-tasks", "title": "Multiple Tasks", "text": "<p>LoRA also makes it easy to fine-tune a model for different tasks. We can train the model using the rank decomposition matrices for each of the tasks. This will give us a pair of A and B matrices for each task. During inference, we can swap out the matrices depending on the task we want the model to do and update the weights (by adding to the frozen weights).</p>"}, {"location": "generative_ai/lora/#choosing-the-rank-r", "title": "Choosing The Rank r", "text": "<p>In general. The smaller the rank r, the smaller the number of trainable parameters and the bigger the savings on compute.</p> <p>According to the LoRA paper:</p> <ul> <li>Effectiveness of higher rank appears to plateau. That is, after a certain     rank value, making it larger generally has no effect on performance.</li> <li>4 \u2264 r \u2264 32 (in powers of 2) can provide a good trade-off between reducing     trainable parameters and preserving performance.</li> <li>Relationship between rank and dataset size needs more research.</li> </ul>"}, {"location": "generative_ai/lora/#hugging-face-peft-library", "title": "Hugging Face PEFT Library", "text": ""}, {"location": "generative_ai/lora/#key-concepts", "title": "Key Concepts", "text": "<pre><code>1. Hugging Face PEFT allows you to fine-tune a model without having to fine-tune\nall of its parameters.\n\n2. Training a model using Hugging Face PEFT requires two additional steps beyond\ntraditional fine-tuning:\n</code></pre> <p>Creating a PEFT config Converting the model into a PEFT model using the PEFT config Inference using a PEFT model is almost identical to inference using a non-PEFT  model. The only difference is that it must be loaded as a PEFT model.</p>"}, {"location": "generative_ai/lora/#training-with-peft", "title": "Training with PEFT", "text": ""}, {"location": "generative_ai/lora/#creating-a-peft-config", "title": "Creating a PEFT Config", "text": "<p>The PEFT config specifies the adapter configuration for your parameter-efficient  fine-tuning process. The base class for this is a <code>PeftConfig</code>, but this example   will use a <code>LoraConfig</code>, the subclass used for low rank adaptation (LoRA).</p> <p>A LoRA config can be instantiated like this:</p> <pre><code>from peft import LoraConfig\nconfig = LoraConfig()\n</code></pre> <p>Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to <code>LoraConfig()</code>. The Hugging Face LoRA conceptual guide  also contains additional explanations.</p> <p>See te complete example here</p>"}, {"location": "generative_ai/lora/#converting-a-transformers-model-into-a-peft-model", "title": "Converting a Transformers Model into a PEFT Model", "text": "<p>Once you have a PEFT config object, you can load a Hugging Face transformers  model as a PEFT model by first loading the pre-trained model as usual (here we  load GPT-2):</p> <pre><code>from transformers import AutoModelForCausalLM\nmodel = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n</code></pre> <p>Then using <code>get_peft_model()</code> to get a trainable PEFT model (using the LoRA config instantiated previously):</p> <pre><code>from peft import get_peft_model\nlora_model = get_peft_model(model, config)\n</code></pre>"}, {"location": "generative_ai/lora/#training-with-a-peft-model", "title": "Training with a PEFT Model", "text": "<p>After calling <code>get_peft_model()</code>, you can then use the resulting <code>lora_model</code>  in a training process of your choice (PyTorch training loop or Hugging Face <code>Trainer</code>).</p>"}, {"location": "generative_ai/lora/#checking-trainable-parameters-of-a-peft-model", "title": "Checking Trainable Parameters of a PEFT Model", "text": "<p>A helpful way to check the number of trainable parameters with the current config  is the <code>print_trainable_parameters()</code> method:</p> <pre><code>lora_model.print_trainable_parameters()\n</code></pre> <p>Which prints an output like this:</p> <pre><code>trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364\n</code></pre>"}, {"location": "generative_ai/lora/#saving-a-trained-peft-model", "title": "Saving a Trained PEFT Model", "text": "<p>Once a PEFT model has been trained, the standard Hugging Face <code>save_pretrained()</code>  method can be used to save the weights locally. For example:</p> <pre><code>lora_model.save_pretrained(\"gpt-lora\")\n</code></pre> <p>Note that this only saves the adapter weights and not the weights of the original  Transformers model. Thus the size of the files created will be much smaller than   you might expect.</p>"}, {"location": "generative_ai/lora/#inference-with-peft", "title": "Inference with PEFT", "text": "<p>Because you have only saved the adapter weights and not the full model weights,  you can't use <code>from_pretrained()</code> with the regular Transformers class (e.g.,   <code>AutoModelForCausalLM</code>). Instead, you need to use the PEFT version (e.g.,    <code>AutoPeftModelForCausalLM</code>). For example:</p> <pre><code>from peft import AutoPeftModelForCausalLM\nlora_model = AutoPeftModelForCausalLM.from_pretrained(\"gpt-lora\")\n</code></pre> <p>After completing this step, you can proceed to use the model for inference.</p>"}, {"location": "generative_ai/lora/#generating-text-from-a-peft-model", "title": "Generating Text from a PEFT Model", "text": "<p>You may see examples from regular Transformer models where the input IDs are  passed in as a positional argument (e.g., <code>model.generate(input_ids)</code>). For a  PEFT model, they must be passed in as a keyword argument (e.g., <code>model.generate  (input_ids=input_ids</code>)). For example:</p> <pre><code>from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\ninputs = tokenizer(\"Hello, my name is \", return_tensors=\"pt\")\noutputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\nprint(tokenizer.batch_decode(outputs))\n</code></pre>"}, {"location": "generative_ai/lora/#resources", "title": "Resources", "text": "<ul> <li>Lora intro</li> <li>Hugging Face PEFT configuration</li> <li>Hugging Face LoRA adapter</li> <li>Hugging Face Models save_pretrained</li> <li>Hugging Face Text Generation</li> <li>Hugging Face Peft repository</li> </ul>"}, {"location": "generative_ai/peft/", "title": "PEFT", "text": ""}, {"location": "generative_ai/peft/#introduction", "title": "Introduction", "text": "<p>Full-fine tuning of large language LLMs is challenging. Fine-tuning requires storing training weights, optimizer states, gradients, forward activations and temporary memory. Things to store other than the weights can take up to 12-20 times more memory than the weights themselves. In full fine-tuning, every weight of the model is updated during training. PEFT methods only update a subset of the weights. They involve freezing most of the layers in the model and allowing only a small number of layers to be trained. Other methods don\u2019t change the weights at all and instead, add new layers to the model and train only those layers. Due to this, the number of trainable weights is much smaller than the number of weights in the original LLM. This reduces the overall memory requirement for training, so much so that PEFT can often be performed on a single GPU. Since most of the LLM is left unchanged, PEFT is also less prone to Catastrophic Forgetting.</p>"}, {"location": "generative_ai/peft/#peft-methods-in-general", "title": "PEFT Methods in General", "text": ""}, {"location": "generative_ai/peft/#selective", "title": "Selective", "text": "<p>We select a subset of initial LLM parameters to fine-tune. There are several approaches to select which subset of parameters we want to fine-tune. We can decide to train:</p> <ul> <li>Only certain components of the model.</li> <li>Specific layers of the model.</li> <li>Individual parameter types</li> </ul> <p>The performance of these approaches and the selective method overall is mixed. There are significant trade-offs in parameter efficiency and compute efficiency and hence, these methods are not very popular.</p>"}, {"location": "generative_ai/peft/#reparameterization", "title": "Reparameterization", "text": "<p>The model weights are reparameterized using a low-rank representation. Example techniques are Low Rank Adaptation (LoRA). More detail in its page: Link</p>"}, {"location": "generative_ai/peft/#additive", "title": "Additive", "text": "<p>There are generally two methods:</p> <ul> <li>Adapters - New trainable layers are added to the model, typically inside the encoder or decoder blocks, after the FFNN or the attention layers.</li> <li>Prompt Tuning - The model architecture is kept fixed and instead, the input (prompt) is manipulated to obtain better performance. This can be done by adding trainable parameters to the prompt embeddings, or keeping the input fixed and retraining the embedding weights. Example techniques include Soft Prompts.</li> </ul>"}, {"location": "generative_ai/peft/#soft-prompts", "title": "Soft Prompts", "text": "<p>Prompt tuning is not prompt engineering. Prompt engineering involves modifying the language of the prompt in order to \u201curge\u201d the model to generate the completion that we want. This could be as simple as trying different words, phrases or including examples for In-Context Learning (ICL). The goal is to help the model understand the nature of the task and to generate better completions. This involves some limitations:</p> <ul> <li>We require a lot of manual effort to write and try different prompts.</li> <li>We are also limited by the length of the context window.</li> </ul> <p>Prompt tuning adds trainable \u201csoft prompts\u201d to inputs that are learnt during the supervised fine-tuning process. The set of trainable tokens is called a soft prompt. It is prepended to the embedding vectors that represent the input prompt. The soft prompt vectors have the same length as the embeddings. Generally, 20-100 \u201cvirtual tokens\u201d can be sufficient for good performance.</p> <p>Prompt tuning does not involve updating the model. Instead, the model is completely frozen and only the soft prompt embedding vectors are updated to optimize the performance of the model on the original prompt. This is very efficient since a very small number of parameters are being trained (10, 000 to 100, 000).</p>"}, {"location": "generative_ai/peft/#hugging-face-peft-library", "title": "Hugging Face PEFT Library", "text": ""}, {"location": "generative_ai/peft/#key-concepts", "title": "Key Concepts", "text": "<pre><code>1. Hugging Face PEFT allows you to fine-tune a model without having to fine-tune\nall of its parameters.\n\n2. Training a model using Hugging Face PEFT requires two additional steps beyond\ntraditional fine-tuning:\n</code></pre> <p>Creating a PEFT config Converting the model into a PEFT model using the PEFT config Inference using a PEFT model is almost identical to inference using a non-PEFT  model. The only difference is that it must be loaded as a PEFT model.</p>"}, {"location": "generative_ai/peft/#training-with-peft", "title": "Training with PEFT", "text": "<p>The PEFT config specifies the adapter configuration for your parameter-efficient  fine-tuning process. The base class for this is a <code>PeftConfig</code>, but this example   will use a <code>LoraConfig</code>, the subclass used for low rank adaptation (LoRA).</p> <p>A LoRA config can be instantiated like this:</p> <pre><code>from peft import LoraConfig\nconfig = LoraConfig()\n</code></pre> <p>Look at the LoRA adapter documentation for additional hyperparameters that can be specified by passing arguments to <code>LoraConfig()</code>. The Hugging Face LoRA conceptual guide  also contains additional explanations.</p> <p>See te complete example here</p>"}, {"location": "generative_ai/probing/", "title": "Probing", "text": ""}, {"location": "generative_ai/probing/#using-probing-tasks-to-train-a-classifier-with-foundational-models", "title": "Using Probing Tasks to Train a Classifier with Foundational Models", "text": ""}, {"location": "generative_ai/probing/#1-select-a-foundational-model", "title": "1. Select a Foundational Model", "text": "<p>Choose a foundational model suitable for your task, such as BERT or GPT,  pre-trained on a large corpus.</p>"}, {"location": "generative_ai/probing/#2-define-your-probing-task", "title": "2. Define Your Probing Task", "text": "<p>Identify relevant features for your classification task and design a  probing task to highlight these features.</p>"}, {"location": "generative_ai/probing/#3-extract-representations", "title": "3. Extract Representations", "text": "<p>Generate embeddings from your dataset using the foundational model.</p> <pre><code>from transformers import AutoModel, AutoTokenizer\n# Load pre-trained model and tokenizer\nmodel_name = \"distilbert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModel.from_pretrained(model_name)\n# Tokenize and generate embeddings\ninputs = tokenizer(\"Your text input here\", return_tensors=\"pt\", padding=True, truncation=True)\nwith torch.no_grad():\noutputs = model(**inputs)\nembeddings = outputs.last_hidden_state\n</code></pre>"}, {"location": "generative_ai/probing/#4-apply-a-probing-classifier", "title": "4. Apply a Probing Classifier", "text": "<p>Train a lightweight classifier on the embeddings to map to your task's specific labels.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# Assuming `embeddings` is a 2D numpy array and `labels` is your target labels\nX_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2)\n# Train a probing classifier\nclf = LogisticRegression()\nclf.fit(X_train, y_train)\n# Evaluate the classifier\nscore = clf.score(X_test, y_test)\nprint(f\"Accuracy: {score}\")\n</code></pre>"}, {"location": "generative_ai/probing/#5-fine-tuning-optional", "title": "5. Fine-tuning (Optional)", "text": "<p>Optionally, fine-tune the foundational model on your specific task for improved performance.</p> <pre><code>from transformers import Trainer, TrainingArguments\ntraining_args = TrainingArguments(\noutput_dir=\"./results\",\nnum_train_epochs=3,\nper_device_train_batch_size=16,\nper_device_eval_batch_size=64,\nwarmup_steps=500,\nweight_decay=0.01,\nevaluate_during_training=True,\nlogging_dir=\"./logs\",\n)\ntrainer = Trainer(\nmodel=model,\nargs=training_args,\ntrain_dataset=train_dataset,\neval_dataset=eval_dataset\n)\ntrainer.train()\n</code></pre>"}, {"location": "generative_ai/probing/#6deployment", "title": "6.Deployment", "text": "<p>Use the trained model for predictions on new, unseen data.</p>"}, {"location": "generative_ai/probing/#benefits-and-considerations", "title": "Benefits and Considerations", "text": "<ul> <li>Efficiency: Saves time and resources by leveraging pre-trained models.</li> <li>Insightful: Offers insights into data features used for classification.</li> <li>Flexibility: Adaptable to various tasks by changing the probing task.</li> </ul>"}, {"location": "generative_ai/probing/#example-use-case", "title": "Example Use Case", "text": "<p>For a text classification task identifying toxic comments, generate embeddings, design a probing task for toxicity features, and train a linear classifier on these embeddings.</p>"}, {"location": "generative_ai/prompting/", "title": "Prompting", "text": ""}, {"location": "generative_ai/prompting/#prompting", "title": "Prompting", "text": "<p>The text that is fed to LLMs as input is called the prompt and the act of providing the input is called prompting.</p>"}, {"location": "generative_ai/prompting/#prompt-design-techniques", "title": "Prompt Design Techniques", "text": "<p>Prompt Design Techniques are innovative strategies for tailoring AI foundation models to specific tasks, fostering better performance in various domains. These methods enable us to guide the AI's output by carefully constructing the prompts we provide, enhancing the model's relevance and efficiency in generating responses.</p> <p>The process of tweaking the prompt provided to an LLM so that it gives the best possible result is called prompt engineering. Some common techniques are given below.</p> <p>Prompt tuning is a technique in generative AI which allows models to target specific tasks effectively. By crafting prompts, whether through a hands-on approach with hard prompts or through an automated process with soft prompts, we enhance the model's predictive capabilities.</p> <ul> <li> <p>Prompt: In AI, a prompt is an input given to the model to generate  specific response or output.</p> </li> <li> <p>Prompt Tuning: This is a method to improve AI models by optimizing prompts so that the model produces better results for specific tasks.</p> </li> <li> <p>Hard Prompt: A manually created template used to guide an AI model's  predictions. It requires human ingenuity to craft effective prompts.</p> </li> <li> <p>Soft Prompt: A series of tokens or embeddings optimized through deep  learning to help guide model predictions, without necessarily making sense to humans.</p> </li> </ul>"}, {"location": "generative_ai/prompting/#in-context-learning-icl", "title": "In-Context Learning (ICL)", "text": "<p>When performing few-shot, one-shot, or zero-shot learning, we can pass information to the model within the prompt in the form of examples, descriptions, or other data. When we rely on a model using information from within the prompt itself instead of relying on what is stored within its own parameters we are using in-context learning.</p> <p>As these AI models grow in size, their ability to absorb and use in-context information significantly improves, showcasing their potential to adapt to various tasks effectively. The progress in this field is inspiring, as these advances hint at an exciting future where such models could be even more intuitive and useful.</p> <p>In ICL, we add examples of the task we are doing in the prompt. This adds more context for the model in the prompt, allowing the model to \u201clearn\u201d more about the task detailed in the prompt.</p>"}, {"location": "generative_ai/prompting/#zero-shot-inference", "title": "Zero-Shot Inference", "text": "<p>Zero-shot prompting is a remarkable technique where a generative AI model can take on new tasks without the need for specific training examples. This process leverages the AI's extensive pre-existing knowledge gained from learning patterns across vast datasets. It empowers the AI to infer and generalize effectively to provide answers and solutions in contexts that were not expressly covered during its initial training.</p> <p>For example, we might be doing semantic classification using our LLM. In that case, a prompt could be:</p> <pre><code>Classify this review: I loved this movie!\nSentiment:\n</code></pre> <p>This prompt works well with large LLMs but smaller LLMs might fail to follow the instruction due to their size and fewer number of features. This is also called zero-shot inference since our prompt has zero examples regarding what the model is expected to output.</p>"}, {"location": "generative_ai/prompting/#few-shot-inference", "title": "Few-Shot Inference", "text": "<p>One and few-shot prompting represent cutting-edge techniques that enable AI to adapt and perform tasks with minimal instructions. Instead of relying on extensive databases for learning, these methods guide generative AI through just one or a few examples, streamlining the learning process and demonstrating its ability to generalize solutions to new problems. This innovative approach marks a significant advancement in machine learning, empowering AI to quickly adjust to specialized tasks and showcasing the incredible potential for efficiency in teaching AI new concepts.</p> <p>This is where ICL comes into play. By adding examples to the prompt, even a smaller LLM might be able to follow the instruction and figure out the correct output. An example of such a prompt is shown below. This is also called one-shot inference since we are providing a single example in the prompt:</p> <pre><code>Classify this review: I loved this movie!\nSentiment: Positive\nClassify this review: I don\u2019t like this chair.\nSentiment:\n</code></pre> <p>Here, we first provide an example to the model and then ask it to figure out the output for the I don\u2019t like this chair review. Sometimes, a single example won\u2019t be enough for the model, for example when the model is even smaller. We\u2019d then add multiple examples in the prompt. This is called few-shot inference.</p> <p>In other words:</p> <ul> <li>Larger models are good at zero-shot inference.</li> <li>For smaller models, we might need to add examples to the prompt, for few-shot inference.</li> </ul>"}, {"location": "generative_ai/prompting/#chain-of-thought-prompting", "title": "Chain-of-Thought Prompting", "text": "<p>Chain-of-Thought Prompting is a vital technique for enhancing the reasoning capabilities of large language models by breaking down complex problems into intermediate steps that lead to a solution. By providing models with a line of reasoning, they can more effectively tackle problems that require more advanced problem-solving processes, enabling them to deduce information, such as the number of cookies in a box, after considering all variables.</p> <p>Example:</p> <pre><code>Problem:\nA car travels 150 kilometers in the first 3 hours of its journey. For the\nnext 2 hours, it travels at a speed of 60 kilometers per hour. What is the\naverage speed of the car for the entire journey?\n\nAnswer:\n* Step 1: Calculate the Distance Covered in the First Part of the Journey\n\n* Given: The car travels 150 kilometers in the first 3 hours.\n    * Calculation: The distance covered in the first part of the journey is 150 kilometers.\n\n* Step 2: Calculate the Distance Covered in the Second Part of the Journey\n\n* Given: The car travels at a speed of 60 kilometers per hour for the\n     next 2 hours.\n    * Calculation: Distance = Speed \u00d7 Time = 60 km/h \u00d7 2 h = 120 kilometers.\n    * The distance covered in the second part of the journey is 120 kilometers.\n\n* Step 3: Calculate the Total Distance Covered\n\n* Total Distance = Distance in the first part + Distance in the second part\n    * Calculation: Total Distance = 150 km + 120 km = 270 kilometers.\n\n* Step 4: Calculate the Total Time Taken for the Journey\n\n* Total Time = Time taken in the first part + Time taken in the second part\n    * Calculation: Total Time = 3 hours + 2 hours = 5 hours.\n\n* Step 5: Calculate the Average Speed for the Entire Journey\n\n* Average Speed = Total Distance / Total Time\n    * Calculation: Average Speed = 270 kilometers / 5 hours = 54 kilometers per hour.\n\n* Conclusion\n\n* The average speed of the car for the entire journey is 54 kilometers per hour.\n\nproblem:\nA car travels 150 kilometers in the first 5 hours of its journey. For the\nnext 5 hours, it travels at a speed of 60 kilometers per hour. What is the\naverage speed of the car for the entire journey?\n</code></pre>"}, {"location": "generative_ai/rlhf/", "title": "Reinforcement Learning From Human Feedback", "text": ""}, {"location": "generative_ai/rlhf/#reinforcement-learning-from-human-feedback-rlhf", "title": "Reinforcement Learning From Human Feedback (RLHF)", "text": "<p>Reinforcement Learning From Human Feedback (RLHF) is a technique used to fine-tune LLMs with human feedback. It uses reinforcement learning to fine-tune the LLM with human feedback data, resulting in a model that is better aligned with human preferences.</p>"}, {"location": "generative_ai/rlhf/#using-reinforcement-learning-to-align-llms", "title": "Using Reinforcement Learning To Align LLMs", "text": "<p>When we apply RL to fine-tune LLMs, we have the following scenario:</p> <ul> <li>Agent: The LLM.</li> <li>Objective: Generate human-aligned text.</li> <li>Environment: Context window of the model (the space in which text can be entered via a prompt).</li> <li>State: At any moment, the current state is the current contents of the context window.</li> <li>Action space: The token vocabulary since each action is the act of generating tokens.</li> </ul> <p></p> <p>Each action can be generating a single word, a sentence or a longer-form text depending on the task we are fine-tuning for. At any given moment, the action that the model will take, meaning which token it will choose next, depends on the prompt text in the context window and the probability distribution over the vocabulary space.</p>"}, {"location": "generative_ai/rlhf/#reward-system", "title": "Reward System", "text": "<p>The reward is assigned based on how closely the generated completions align with human preferences.</p> <p>An example reward system is as follows:</p> <ul> <li>We can have a human evaluate all of the completions of the model against some alignment metric, such as toxicity.</li> <li>The feedback can be represented as a scalar value, either a zero (not toxic) or one (toxic).</li> <li>The LLM weights are then updated iteratively to maximize the reward obtained from the human classifier (obtain as many zeros as possible), enabling the model to generated non-toxic completions. This reward system requires obtaining manual human feedback, which can be time consuming and expensive.</li> </ul>"}, {"location": "generative_ai/rlhf/#reward-model", "title": "Reward Model", "text": "<p>A practical and scalable alternative is to use an additional model, called the reward model, to classify the outputs of the LLM and evaluate the degree of alignment with human preferences.</p> <p>To obtain the reward model, we use a smaller number of human examples to train the reward model using traditional supervised learning since it\u2019s a classification problem. This trained reward model will be used to assess the output of the LLM and assign a reward a value, which in turn gets used to update the weights of the LLM and train a new human-aligned version. Exactly how the weights are updated as the model completions are assessed depends on the (reinforcement learning) algorithm used to optimize the RL policy</p>"}, {"location": "generative_ai/nlp/intro/", "title": "NLP: Natural Language Processing", "text": ""}, {"location": "generative_ai/nlp/intro/#nlp-natural-language-processing", "title": "NLP: Natural Language Processing", "text": "<p>NLP stands for \"natural language processing\" and forms the bridge between human  communication and computer logic.</p> <p>Typical NLP tasks and applications include:</p> <ul> <li>Speech recognition</li> <li>Text classification</li> <li>Machine translation</li> <li>Text summarization</li> <li>Question answering</li> <li>Chatbots</li> </ul> <p>Some of the major challenges in NLP are the complexity, nuance, and ambiguity of  natural language, as well as the difficulty of acquiring clean and labeled text   data.</p> <p>NLP reveals structure and meaning from human language to computers and its importance has grown in the modern age.</p> <p>Language is complex, nuanced, and ambiguous, making it challenging for computers  to process language data.</p> <p>Data can have misspellings, unconventional words, and biases that can be difficult  and expensive for humans to label.</p>"}, {"location": "generative_ai/nlp/intro/#tokenization", "title": "Tokenization", "text": "<p>Tokenization transform text to useful representation that a computer can understand.</p>"}, {"location": "generative_ai/nlp/intro/#tokenization-steps", "title": "Tokenization Steps", "text": ""}, {"location": "generative_ai/nlp/intro/#normalization", "title": "Normalization", "text": "<p>cleans text for consistency by removing complexity.</p> <p>breaks the text into smaller \"words\" and will be the base of what tokens will be.</p> <p>More normalization can reduces complexity in the text. But can lose context.</p> <p>Example:</p> <pre><code>def normalize_text(text: str) -&gt; str:\n# Only keep ASCII letters, numbers, punctuation, and whitespace characters\nacceptable_characters = (\nstring.ascii_letters\n+ string.digits\n+ string.punctuation\n+ string.whitespace\n)\nnormalized_text = ''.join(\nfilter(lambda letter: letter in acceptable_characters, text)\n)\n# Make text lower-case\nnormalized_text = normalized_text.lower()\nreturn normalized_text\n</code></pre>"}, {"location": "generative_ai/nlp/intro/#pretokenization", "title": "Pretokenization", "text": "<p>breaks the text into smaller \"words\" and will be the base of what tokens will be.</p> <p>Example:</p> <pre><code>def pretokenize_text(text: str) -&gt; list[str]:\n# Split based on spaces\nsmaller_pieces = text.split()\nreturn smaller_pieces\n</code></pre>"}, {"location": "generative_ai/nlp/intro/#tokenization-step", "title": "Tokenization step", "text": "<p>Breaks text into smaller parts called \"tokens\" to create meaningful building blocks.</p> <p>Methods:</p> <pre><code>* Character tokenization: Smaller vocabulary. Less meanningful representations.\n* Word tokenization: Bigger vocabulary. More tokens are out of vocabulary.\n(If the word was never in the training data, then we might no have a\nmeaningful way to represent that word as a token)\n* Subword Tokenization. Hybrid and most used in LLM. Frequent words in the\n dataset aren't split, however rarer words are broken down.\n    * quickly -&gt; [quick] + [ly]\n    * quicker -&gt; [quick] + [er]\n    * quickest -&gt; [quick] + [est]\nThis method is learned with a particular focus of text data, resulting in deferent\nrepresentations.\nTherefore, if your're using a pre-trained model for your NLP task, make sure\n to use the same tokenizer that the model was trained on.\n\nSubword tokennization algorithms:\n    * [BPE](https://en.wikipedia.org/wiki/Byte_pair_encoding) (GPT and RoBERTa).\n    * [WordPiece](\n        https://blog.research.google/2021/12/a-fast-wordpiece-tokenization-system.html\n        ) (BERT and Electra)\n    * [SentencePiece](https://github.com/google/sentencepiece) (T5, ALBERT\n     and XLNET)\n</code></pre> <p>Example:</p> <pre><code># Combine normalization and pretokenization steps before breaking things further\ndef tokenize_text(text: str) -&gt; list[str]:\n# Apply created steps\nnormalized_text: str = normalize_text(text)\npretokenized_text: list[str] = pretokenize_text(normalized_text)\n# COMPLETE: Go through pretokenized text to create a list of tokens\ntokens = []\n# Small 'pieces' to make full tokens\nfor word in pretokenized_text:\ntokens.extend(\nre.findall(\nf'[\\w]+|[{string.punctuation}]', # Split word at punctuations\nword,\n)\n)\nreturn tokens\n</code></pre>"}, {"location": "generative_ai/nlp/intro/#postprocessing", "title": "Postprocessing", "text": "<p>Applies additional transformations, such as adding tags at the beginning and end  of sentences.</p> <p><code>bash  [CLS] do you pre fer cof fee or t ea ? [EOS]</code></p> <p>Example:</p> <pre><code>def postprocess_tokens(tokens: list[str]) -&gt; list[str]:\n# Can use a format like '[BOS]' &amp; '[EOS]'\nbos_token = '[BOS]'\neos_token = '[EOS]'\nupdated_tokens = (\n[bos_token]\n+ tokens\n+ [eos_token]\n)\nreturn updated_tokens\n</code></pre> <p>You can learn about hugginface tokenizers here.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/", "title": "Building Question Answering Datasets", "text": "In\u00a0[1]: Copied! <pre>from pathlib import Path\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, default_data_collator, pipeline\nfrom transformers.trainer_utils import PredictionOutput\nimport math\nimport time\nimport collections\nimport numpy as np\nfrom tqdm.notebook import tqdm\n</pre> from pathlib import Path import pandas as pd from datasets import Dataset from transformers import AutoTokenizer, AutoModelForQuestionAnswering, Trainer, default_data_collator, pipeline from transformers.trainer_utils import PredictionOutput import math import time import collections import numpy as np from tqdm.notebook import tqdm <pre>/opt/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Our question-answer pairs are stored in a CSV with columns: \"question\", \"answer\" and \"filename\".</p> In\u00a0[2]: Copied! <pre>df = pd.read_csv(\"data/qa.csv\")\ndf.head()\n</pre> df = pd.read_csv(\"data/qa.csv\") df.head() Out[2]: question answer filename 0 Who is the manufacturer of the product? Zyxel CVE-2020-29583.txt 1 Who reported the vulnerability? researchers from EYE Netherlands CVE-2020-29583.txt 2 What is the vulnerability? A hardcoded credential vulnerability was ident... CVE-2020-29583.txt 3 How do users protect themselves? we urge users to install the applicable updates CVE-2020-29583.txt 4 What products are affected? firewalls and AP controllers CVE-2020-29583.txt <p>Given the source of our question-answer pairs, we'll want a function that takes the question, answer, and filepath, and returns a nicely formatted dictionary.</p> <p>Since we want each question to have a unique identifier, we will also take that as a function argument so we can use an iterator.</p> <p>Within the function, we also want to read the context from the provided file and locate the starting index of the provided answer. We can do this with the <code>.find()</code> method of the context string object.</p> In\u00a0[3]: Copied! <pre>def qa_to_squad(question, answer, filename, identifier):\n    filepath = \"data/\" + row['filename']\n    with open(filepath, \"r\") as f:\n        context = f.read()\n        \n    start_location = context.find(answer)\n    qa_pair = {\n        'id': identifier,\n        'title': filepath,\n        'context': context,\n        'question': question,\n        'answers': {'text': [answer],\n                   'answer_start': [start_location]}\n    }\n    return qa_pair\n</pre> def qa_to_squad(question, answer, filename, identifier):     filepath = \"data/\" + row['filename']     with open(filepath, \"r\") as f:         context = f.read()              start_location = context.find(answer)     qa_pair = {         'id': identifier,         'title': filepath,         'context': context,         'question': question,         'answers': {'text': [answer],                    'answer_start': [start_location]}     }     return qa_pair In\u00a0[4]: Copied! <pre>qa_list = list()\nfor i, row in df.iterrows():\n    q = row['question']\n    a = row['answer']\n    f = row['filename']\n    SQuAD_dict = qa_to_squad(q, a, f, i)\n    qa_list.append(SQuAD_dict)\n</pre> qa_list = list() for i, row in df.iterrows():     q = row['question']     a = row['answer']     f = row['filename']     SQuAD_dict = qa_to_squad(q, a, f, i)     qa_list.append(SQuAD_dict) In\u00a0[6]: Copied! <pre>qa_df = pd.DataFrame(data=qa_list)\ndata = Dataset.from_pandas(qa_df)\nprint(data[0])\n</pre> qa_df = pd.DataFrame(data=qa_list) data = Dataset.from_pandas(qa_df) print(data[0]) <pre>{'id': 0, 'title': 'data/CVE-2020-29583.txt', 'context': 'CVE:   CVE-2020-29583 Summary Zyxel has released a patch for the hardcoded credential vulnerability of firewalls and AP controllers recently reported by researchers from EYE Netherlands. Users are advised to install the applicable firmware updates for optimal protection. What is the vulnerability? A hardcoded credential vulnerability was identified in the \u201czyfwp\u201d user account in some Zyxel firewalls and AP controllers. The account was designed to deliver automatic firmware updates to connected access points through FTP. What versions are vulnerable\u2014and what should you do? After a thorough investigation, we\u2019ve identified the vulnerable products and are releasing firmware patches to address the issue, as shown in the table below. For optimal protection, we urge users to install the applicable updates. For those not listed, they are not affected. Contact your local Zyxel support team if you require further assistance or visit our  forum  for more information. Got a question or a tipoff? Please contact your local service rep for further information or assistance. If you\u2019ve found a vulnerability, we want to work with you to fix it\u2014contact  security@zyxel.com.tw  and we\u2019ll get right back to you. Acknowledgment Thanks to Niels Teusink at EYE for reporting the issue to us. Revision history 2020-12-23: Initial release 2020-12-24: Updated the acknowledgement section 2021-01-04: Updated the patch schedule for AP controllers 2021-01-08: Added the forum link', 'question': 'Who is the manufacturer of the product?', 'answers': {'answer_start': [30], 'text': ['Zyxel']}}\n</pre> In\u00a0[7]: Copied! <pre>data.save_to_disk(\"qa_data.hf\")\n</pre> data.save_to_disk(\"qa_data.hf\") <pre>                                                                                         \r</pre> In\u00a0[8]: Copied! <pre># Load the tokenizer for DistilBERT\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n\n# Load the model\n# Note: This will throw warnings, which is expected!\nmodel = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n</pre> # Load the tokenizer for DistilBERT tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')  # Load the model # Note: This will throw warnings, which is expected! model = AutoModelForQuestionAnswering.from_pretrained('distilbert-base-uncased') <pre>Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n</pre> In\u00a0[9]: Copied! <pre># The Trainer subclass here is lightly modified from HuggingFace\n# Original source at https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/trainer_qa.py\nclass QuestionAnsweringTrainer(Trainer):\n    def __init__(self, *args, post_process_function=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.post_process_function = post_process_function\n\n    def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = \"test\"):\n        predict_dataloader = self.get_test_dataloader(predict_dataset)\n\n        # Temporarily disable metric computation, we will do it in the loop here.\n        compute_metrics = self.compute_metrics\n        self.compute_metrics = None\n        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n        start_time = time.time()\n        try:\n            output = eval_loop(\n                predict_dataloader,\n                description=\"Prediction\",\n                # No point gathering the predictions if there are no metrics, otherwise we defer to\n                # self.args.prediction_loss_only\n                prediction_loss_only=True if compute_metrics is None else None,\n                ignore_keys=ignore_keys,\n                metric_key_prefix=metric_key_prefix,\n            )\n        finally:\n            self.compute_metrics = compute_metrics\n        total_batch_size = self.args.eval_batch_size * self.args.world_size\n        if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:\n            start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]\n        output.metrics.update(\n            speed_metrics(\n                metric_key_prefix,\n                start_time,\n                num_samples=output.num_samples,\n                num_steps=math.ceil(output.num_samples / total_batch_size),\n            )\n        )\n\n        if self.post_process_function is None or self.compute_metrics is None:\n            return output\n\n        predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, \"predict\")\n        metrics = self.compute_metrics(predictions)\n\n        # Prefix all keys with metric_key_prefix + '_'\n        for key in list(metrics.keys()):\n            if not key.startswith(f\"{metric_key_prefix}_\"):\n                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n        metrics.update(output.metrics)\n        return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics)\n</pre> # The Trainer subclass here is lightly modified from HuggingFace # Original source at https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/trainer_qa.py class QuestionAnsweringTrainer(Trainer):     def __init__(self, *args, post_process_function=None, **kwargs):         super().__init__(*args, **kwargs)         self.post_process_function = post_process_function      def predict(self, predict_dataset, predict_examples, ignore_keys=None, metric_key_prefix: str = \"test\"):         predict_dataloader = self.get_test_dataloader(predict_dataset)          # Temporarily disable metric computation, we will do it in the loop here.         compute_metrics = self.compute_metrics         self.compute_metrics = None         eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop         start_time = time.time()         try:             output = eval_loop(                 predict_dataloader,                 description=\"Prediction\",                 # No point gathering the predictions if there are no metrics, otherwise we defer to                 # self.args.prediction_loss_only                 prediction_loss_only=True if compute_metrics is None else None,                 ignore_keys=ignore_keys,                 metric_key_prefix=metric_key_prefix,             )         finally:             self.compute_metrics = compute_metrics         total_batch_size = self.args.eval_batch_size * self.args.world_size         if f\"{metric_key_prefix}_jit_compilation_time\" in output.metrics:             start_time += output.metrics[f\"{metric_key_prefix}_jit_compilation_time\"]         output.metrics.update(             speed_metrics(                 metric_key_prefix,                 start_time,                 num_samples=output.num_samples,                 num_steps=math.ceil(output.num_samples / total_batch_size),             )         )          if self.post_process_function is None or self.compute_metrics is None:             return output          predictions = self.post_process_function(predict_examples, predict_dataset, output.predictions, \"predict\")         metrics = self.compute_metrics(predictions)          # Prefix all keys with metric_key_prefix + '_'         for key in list(metrics.keys()):             if not key.startswith(f\"{metric_key_prefix}_\"):                 metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)         metrics.update(output.metrics)         return PredictionOutput(predictions=predictions.predictions, label_ids=predictions.label_ids, metrics=metrics) In\u00a0[10]: Copied! <pre># Training preprocessing\ndef prepare_train_features(examples):\n    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n    # in one example possible giving several features when a context is long, each of those features having a\n    # context that overlaps a bit the context of the previous feature.\n    tokenized_examples = tokenizer(\n        examples[\"question\"],\n        examples[\"context\"],\n        truncation=\"only_second\",\n        max_length=512,\n        padding=\"max_length\",\n        return_offsets_mapping=True\n    )\n\n    # The offset mappings will give us a map from token to character position in the original context. This will\n    # help us compute the start_positions and end_positions.\n    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n\n    # Let's label those examples!\n    tokenized_examples[\"start_positions\"] = []\n    tokenized_examples[\"end_positions\"] = []\n\n    for i, offsets in enumerate(offset_mapping):\n        # We will label impossible answers with the index of the CLS token.\n        input_ids = tokenized_examples[\"input_ids\"][i]\n        cls_index = input_ids.index(tokenizer.cls_token_id)\n\n        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n        sequence_ids = tokenized_examples.sequence_ids(i)\n\n        # One example can give several spans, this is the index of the example containing this span of text.\n        answers = examples[\"answers\"][i]\n\n        # If no answers are given, set the cls_index as answer.\n        if len(answers[\"answer_start\"]) == 0:\n            tokenized_examples[\"start_positions\"].append(cls_index)\n            tokenized_examples[\"end_positions\"].append(cls_index)\n        else:\n            # Start/end character index of the answer in the text.\n            start_char = answers[\"answer_start\"][0]\n            end_char = start_char + len(answers[\"text\"][0])\n\n            # Start token index of the current span in the text.\n            token_start_index = 0\n\n            # End token index of the current span in the text.\n            token_end_index = len(input_ids) - 1\n\n            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n            # Note: we could go after the last offset if the answer is the last word (edge case).\n            while token_start_index &lt; len(offsets) and offsets[token_start_index][0] &lt;= start_char:\n                token_start_index += 1\n            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n            while offsets[token_end_index][1] &gt;= end_char:\n                token_end_index -= 1\n            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n    \n    return tokenized_examples\n\ndef postprocess_qa_predictions(\n    examples,\n    features,\n    predictions,\n    version_2_with_negative = False,\n    n_best_size = 20,\n    max_answer_length = 30,\n    null_score_diff_threshold = 0.0,\n):\n\"\"\"\n    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n\n    Args:\n        examples: The non-preprocessed dataset (see the main script for more information).\n        features: The processed dataset (see the main script for more information).\n        predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):\n            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n            first dimension must match the number of elements of :obj:`features`.\n        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n            Whether or not the underlying dataset contains examples with no answers.\n        n_best_size (:obj:`int`, `optional`, defaults to 20):\n            The total number of n-best predictions to generate when looking for an answer.\n        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n            are not conditioned on one another.\n        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n            the null answer for an example giving several features is the minimum of the scores for the null answer on\n            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n\n            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n    \"\"\"\n    if len(predictions) != 2:\n        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n    all_start_logits, all_end_logits = predictions\n\n    if len(predictions[0]) != len(features):\n        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n\n    # Build a map example to its corresponding features.\n    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n    features_per_example = collections.defaultdict(list)\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    # The dictionaries we have to fill.\n    all_predictions = collections.OrderedDict()\n    all_nbest_json = collections.OrderedDict()\n    if version_2_with_negative:\n        scores_diff_json = collections.OrderedDict()\n\n    # Let's loop over all the examples!\n    for example_index, example in enumerate(tqdm(examples)):\n        # Those are the indices of the features associated to the current example.\n        feature_indices = features_per_example[example_index]\n\n        min_null_prediction = None\n        prelim_predictions = []\n\n        # Looping through all the features associated to the current example.\n        for feature_index in feature_indices:\n            # We grab the predictions of the model for this feature.\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            # This is what will allow us to map some the positions in our logits to span of texts in the original\n            # context.\n            offset_mapping = features[feature_index][\"offset_mapping\"]\n            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n            # available in the current feature.\n            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n\n            # Update minimum null prediction.\n            feature_null_score = start_logits[0] + end_logits[0]\n            if min_null_prediction is None or min_null_prediction[\"score\"] &gt; feature_null_score:\n                min_null_prediction = {\n                    \"offsets\": (0, 0),\n                    \"score\": feature_null_score,\n                    \"start_logit\": start_logits[0],\n                    \"end_logit\": end_logits[0],\n                }\n\n            # Go through all possibilities for the `n_best_size` greater start and end logits.\n            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n                    # to part of the input_ids that are not in the context.\n                    if (\n                        start_index &gt;= len(offset_mapping)\n                        or end_index &gt;= len(offset_mapping)\n                        or offset_mapping[start_index] is None\n                        or len(offset_mapping[start_index]) &lt; 2\n                        or offset_mapping[end_index] is None\n                        or len(offset_mapping[end_index]) &lt; 2\n                    ):\n                        continue\n                    # Don't consider answers with a length that is either &lt; 0 or &gt; max_answer_length.\n                    if end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length:\n                        continue\n                    # Don't consider answer that don't have the maximum context available (if such information is\n                    # provided).\n                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n                        continue\n\n                    prelim_predictions.append(\n                        {\n                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n                            \"score\": start_logits[start_index] + end_logits[end_index],\n                            \"start_logit\": start_logits[start_index],\n                            \"end_logit\": end_logits[end_index],\n                        }\n                    )\n        if version_2_with_negative and min_null_prediction is not None:\n            # Add the minimum null prediction\n            prelim_predictions.append(min_null_prediction)\n            null_score = min_null_prediction[\"score\"]\n\n        # Only keep the best `n_best_size` predictions.\n        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n\n        # Add back the minimum null prediction if it was removed because of its low score.\n        if (\n            version_2_with_negative\n            and min_null_prediction is not None\n            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n        ):\n            predictions.append(min_null_prediction)\n\n        # Use the offsets to gather the answer text in the original context.\n        context = example[\"context\"]\n        for pred in predictions:\n            offsets = pred.pop(\"offsets\")\n            pred[\"text\"] = context[offsets[0] : offsets[1]]\n\n        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n        # failure.\n        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n\n        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n        # the LogSumExp trick).\n        scores = np.array([pred.pop(\"score\") for pred in predictions])\n        exp_scores = np.exp(scores - np.max(scores))\n        probs = exp_scores / exp_scores.sum()\n\n        # Include the probabilities in our predictions.\n        for prob, pred in zip(probs, predictions):\n            pred[\"probability\"] = prob\n\n        # Pick the best prediction. If the null answer is not possible, this is easy.\n        if not version_2_with_negative:\n            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n        else:\n            # Otherwise we first need to find the best non-empty prediction.\n            i = 0\n            while predictions[i][\"text\"] == \"\":\n                i += 1\n            best_non_null_pred = predictions[i]\n\n            # Then we compare to the null prediction using the threshold.\n            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n            if score_diff &gt; null_score_diff_threshold:\n                all_predictions[example[\"id\"]] = \"\"\n            else:\n                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n\n        # Make `predictions` JSON-serializable by casting np.float back to float.\n        all_nbest_json[example[\"id\"]] = [\n            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n            for pred in predictions\n        ]\n\n    return all_predictions\n\n# Post-processing:\ndef post_processing_function(examples, features, predictions, stage=\"eval\"):\n    # Post-processing: we match the start logits and end logits to answers in the original context.\n    predictions = postprocess_qa_predictions(\n        examples=examples,\n        features=features,\n        predictions=predictions,\n        version_2_with_negative=data_args.version_2_with_negative,\n        n_best_size=data_args.n_best_size,\n        max_answer_length=data_args.max_answer_length,\n        null_score_diff_threshold=data_args.null_score_diff_threshold,\n        output_dir=training_args.output_dir,\n        log_level=log_level,\n        prefix=stage,\n    )\n    # Format the result to the format the metric expects.\n    if data_args.version_2_with_negative:\n        formatted_predictions = [\n            {\"id\": str(k), \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n        ]\n    else:\n        formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in predictions.items()]\n\n    references = [{\"id\": str(ex[\"id\"]), \"answers\": ex[answer_column_name]} for ex in examples]\n    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n</pre> # Training preprocessing def prepare_train_features(examples):     # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results     # in one example possible giving several features when a context is long, each of those features having a     # context that overlaps a bit the context of the previous feature.     tokenized_examples = tokenizer(         examples[\"question\"],         examples[\"context\"],         truncation=\"only_second\",         max_length=512,         padding=\"max_length\",         return_offsets_mapping=True     )      # The offset mappings will give us a map from token to character position in the original context. This will     # help us compute the start_positions and end_positions.     offset_mapping = tokenized_examples.pop(\"offset_mapping\")      # Let's label those examples!     tokenized_examples[\"start_positions\"] = []     tokenized_examples[\"end_positions\"] = []      for i, offsets in enumerate(offset_mapping):         # We will label impossible answers with the index of the CLS token.         input_ids = tokenized_examples[\"input_ids\"][i]         cls_index = input_ids.index(tokenizer.cls_token_id)          # Grab the sequence corresponding to that example (to know what is the context and what is the question).         sequence_ids = tokenized_examples.sequence_ids(i)          # One example can give several spans, this is the index of the example containing this span of text.         answers = examples[\"answers\"][i]          # If no answers are given, set the cls_index as answer.         if len(answers[\"answer_start\"]) == 0:             tokenized_examples[\"start_positions\"].append(cls_index)             tokenized_examples[\"end_positions\"].append(cls_index)         else:             # Start/end character index of the answer in the text.             start_char = answers[\"answer_start\"][0]             end_char = start_char + len(answers[\"text\"][0])              # Start token index of the current span in the text.             token_start_index = 0              # End token index of the current span in the text.             token_end_index = len(input_ids) - 1              # Otherwise move the token_start_index and token_end_index to the two ends of the answer.             # Note: we could go after the last offset if the answer is the last word (edge case).             while token_start_index &lt; len(offsets) and offsets[token_start_index][0] &lt;= start_char:                 token_start_index += 1             tokenized_examples[\"start_positions\"].append(token_start_index - 1)             while offsets[token_end_index][1] &gt;= end_char:                 token_end_index -= 1             tokenized_examples[\"end_positions\"].append(token_end_index + 1)          return tokenized_examples  def postprocess_qa_predictions(     examples,     features,     predictions,     version_2_with_negative = False,     n_best_size = 20,     max_answer_length = 30,     null_score_diff_threshold = 0.0, ):     \"\"\"     Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the     original contexts. This is the base postprocessing functions for models that only return start and end logits.      Args:         examples: The non-preprocessed dataset (see the main script for more information).         features: The processed dataset (see the main script for more information).         predictions (:obj:`Tuple[np.ndarray, np.ndarray]`):             The predictions of the model: two arrays containing the start logits and the end logits respectively. Its             first dimension must match the number of elements of :obj:`features`.         version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):             Whether or not the underlying dataset contains examples with no answers.         n_best_size (:obj:`int`, `optional`, defaults to 20):             The total number of n-best predictions to generate when looking for an answer.         max_answer_length (:obj:`int`, `optional`, defaults to 30):             The maximum length of an answer that can be generated. This is needed because the start and end predictions             are not conditioned on one another.         null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):             The threshold used to select the null answer: if the best answer has a score that is less than the score of             the null answer minus this threshold, the null answer is selected for this example (note that the score of             the null answer for an example giving several features is the minimum of the scores for the null answer on             each feature: all features must be aligned on the fact they `want` to predict a null answer).              Only useful when :obj:`version_2_with_negative` is :obj:`True`.     \"\"\"     if len(predictions) != 2:         raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")     all_start_logits, all_end_logits = predictions      if len(predictions[0]) != len(features):         raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")      # Build a map example to its corresponding features.     example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}     features_per_example = collections.defaultdict(list)     for i, feature in enumerate(features):         features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)      # The dictionaries we have to fill.     all_predictions = collections.OrderedDict()     all_nbest_json = collections.OrderedDict()     if version_2_with_negative:         scores_diff_json = collections.OrderedDict()      # Let's loop over all the examples!     for example_index, example in enumerate(tqdm(examples)):         # Those are the indices of the features associated to the current example.         feature_indices = features_per_example[example_index]          min_null_prediction = None         prelim_predictions = []          # Looping through all the features associated to the current example.         for feature_index in feature_indices:             # We grab the predictions of the model for this feature.             start_logits = all_start_logits[feature_index]             end_logits = all_end_logits[feature_index]             # This is what will allow us to map some the positions in our logits to span of texts in the original             # context.             offset_mapping = features[feature_index][\"offset_mapping\"]             # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context             # available in the current feature.             token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)              # Update minimum null prediction.             feature_null_score = start_logits[0] + end_logits[0]             if min_null_prediction is None or min_null_prediction[\"score\"] &gt; feature_null_score:                 min_null_prediction = {                     \"offsets\": (0, 0),                     \"score\": feature_null_score,                     \"start_logit\": start_logits[0],                     \"end_logit\": end_logits[0],                 }              # Go through all possibilities for the `n_best_size` greater start and end logits.             start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()             end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()             for start_index in start_indexes:                 for end_index in end_indexes:                     # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond                     # to part of the input_ids that are not in the context.                     if (                         start_index &gt;= len(offset_mapping)                         or end_index &gt;= len(offset_mapping)                         or offset_mapping[start_index] is None                         or len(offset_mapping[start_index]) &lt; 2                         or offset_mapping[end_index] is None                         or len(offset_mapping[end_index]) &lt; 2                     ):                         continue                     # Don't consider answers with a length that is either &lt; 0 or &gt; max_answer_length.                     if end_index &lt; start_index or end_index - start_index + 1 &gt; max_answer_length:                         continue                     # Don't consider answer that don't have the maximum context available (if such information is                     # provided).                     if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):                         continue                      prelim_predictions.append(                         {                             \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),                             \"score\": start_logits[start_index] + end_logits[end_index],                             \"start_logit\": start_logits[start_index],                             \"end_logit\": end_logits[end_index],                         }                     )         if version_2_with_negative and min_null_prediction is not None:             # Add the minimum null prediction             prelim_predictions.append(min_null_prediction)             null_score = min_null_prediction[\"score\"]          # Only keep the best `n_best_size` predictions.         predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]          # Add back the minimum null prediction if it was removed because of its low score.         if (             version_2_with_negative             and min_null_prediction is not None             and not any(p[\"offsets\"] == (0, 0) for p in predictions)         ):             predictions.append(min_null_prediction)          # Use the offsets to gather the answer text in the original context.         context = example[\"context\"]         for pred in predictions:             offsets = pred.pop(\"offsets\")             pred[\"text\"] = context[offsets[0] : offsets[1]]          # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid         # failure.         if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):             predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})          # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using         # the LogSumExp trick).         scores = np.array([pred.pop(\"score\") for pred in predictions])         exp_scores = np.exp(scores - np.max(scores))         probs = exp_scores / exp_scores.sum()          # Include the probabilities in our predictions.         for prob, pred in zip(probs, predictions):             pred[\"probability\"] = prob          # Pick the best prediction. If the null answer is not possible, this is easy.         if not version_2_with_negative:             all_predictions[example[\"id\"]] = predictions[0][\"text\"]         else:             # Otherwise we first need to find the best non-empty prediction.             i = 0             while predictions[i][\"text\"] == \"\":                 i += 1             best_non_null_pred = predictions[i]              # Then we compare to the null prediction using the threshold.             score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]             scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.             if score_diff &gt; null_score_diff_threshold:                 all_predictions[example[\"id\"]] = \"\"             else:                 all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]          # Make `predictions` JSON-serializable by casting np.float back to float.         all_nbest_json[example[\"id\"]] = [             {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}             for pred in predictions         ]      return all_predictions  # Post-processing: def post_processing_function(examples, features, predictions, stage=\"eval\"):     # Post-processing: we match the start logits and end logits to answers in the original context.     predictions = postprocess_qa_predictions(         examples=examples,         features=features,         predictions=predictions,         version_2_with_negative=data_args.version_2_with_negative,         n_best_size=data_args.n_best_size,         max_answer_length=data_args.max_answer_length,         null_score_diff_threshold=data_args.null_score_diff_threshold,         output_dir=training_args.output_dir,         log_level=log_level,         prefix=stage,     )     # Format the result to the format the metric expects.     if data_args.version_2_with_negative:         formatted_predictions = [             {\"id\": str(k), \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()         ]     else:         formatted_predictions = [{\"id\": str(k), \"prediction_text\": v} for k, v in predictions.items()]      references = [{\"id\": str(ex[\"id\"]), \"answers\": ex[answer_column_name]} for ex in examples]     return EvalPrediction(predictions=formatted_predictions, label_ids=references) In\u00a0[11]: Copied! <pre>data = data.map(prepare_train_features, batched=True)\n</pre> data = data.map(prepare_train_features, batched=True) <pre>                                                  \r</pre> In\u00a0[12]: Copied! <pre># Set up our trainer\ntrainer = QuestionAnsweringTrainer(\n    model=model,\n    train_dataset=data,\n    tokenizer=tokenizer,\n    data_collator=default_data_collator,\n    post_process_function=post_processing_function\n)\n\n# Run the trainer\ntrainer.train()\n\n# Save our model\ntrainer.save_model(\"./ft-distilbert\")\n</pre> # Set up our trainer trainer = QuestionAnsweringTrainer(     model=model,     train_dataset=data,     tokenizer=tokenizer,     data_collator=default_data_collator,     post_process_function=post_processing_function )  # Run the trainer trainer.train()  # Save our model trainer.save_model(\"./ft-distilbert\") <pre>/opt/venv/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n</pre>        [6/6 01:45, Epoch 3/3]      Step Training Loss <p> In\u00a0[13]: Copied! <pre># Let's evaluate our model!\n# Specify an input question and context\nquestion = \"What can an attacker do with XSS?\"\nwith open(\"./data/xss.txt\", \"r\") as f:\n    context = f.read()\n\n# Use HuggingFace pipeline to answer the question\nquestion_answerer = pipeline(\"question-answering\", model=\"./ft-distilbert\")\nquestion_answerer(question=question, context=context)\n</pre> # Let's evaluate our model! # Specify an input question and context question = \"What can an attacker do with XSS?\" with open(\"./data/xss.txt\", \"r\") as f:     context = f.read()  # Use HuggingFace pipeline to answer the question question_answerer = pipeline(\"question-answering\", model=\"./ft-distilbert\") question_answerer(question=question, context=context) Out[13]: <pre>{'score': 7.277844997588545e-05,\n 'start': 7834,\n 'end': 7875,\n 'answer': 'the payload to modify their own profiles,'}</pre> <p>Hopefully your answer was satisfactory! If not, don't worry about it too much, our dataset was extremely small and we only trained for 3 epochs, so some issues can be expected. This is why so many LLM datasets are so big!</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#building-question-answering-datasets", "title": "Building Question Answering Datasets\u00b6", "text": "<p>In this exercise, we want to construct a dataset for training a question answering model. In general, this process is highly manual, requiring the collection of data and, as one might expect, manually constructing pairs of questions and answers that can be found in the text. In our exercise, we provide the data plus question and answer pairs, and work through how to construct the dataset from that starting point.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#squad", "title": "SQuAD\u00b6", "text": "<p>One of the most well studied datasets in question answering is the Stanford Question Answering Dataset (SQuAD), introduced in a paper by Rajpurkar et al. and its follow-up, SQuAD 2.0. Formatting our dataset like SQuAD makes it much easier to use prebuild models and trainers like those from HuggingFace to do our job, so it makes sense to first understand the structure of the dataset. What follows is a single example from the dataset.</p> <pre>{'id': '573387acd058e614000b5cb5',\n 'title': 'University_of_Notre_Dame',\n 'context': 'One of the main driving forces in the growth of the University was its football team, the Notre Dame Fighting Irish. Knute Rockne became head coach in 1918. Under Rockne, the Irish would post a record of 105 wins, 12 losses, and five ties. During his 13 years the Irish won three national championships, had five undefeated seasons, won the Rose Bowl in 1925, and produced players such as George Gipp and the \"Four Horsemen\". Knute Rockne has the highest winning percentage (.881) in NCAA Division I/FBS football history. Rockne\\'s offenses employed the Notre Dame Box and his defenses ran a 7\u20132\u20132 scheme. The last game Rockne coached was on December 14, 1930 when he led a group of Notre Dame all-stars against the New York Giants in New York City.',\n 'question': 'In what year did the team lead by Knute Rockne win the Rose Bowl?',\n 'answers': {'text': ['1925'], \n             'answer_start': [354]}\n}\n</pre> <p>As we can see, each entry in the dataset is a Python <code>dict</code> object with the structure:</p> <pre>{'id': str,\n 'title': str,\n 'context': str,\n 'question': str,\n 'answers': dict\n}\n</pre> <p>For our purposes, we can simply ignore the 'title' entry if we wish -- it has no impact on our ability to use the dataset, but can be important for others, as it maps to the title of the article from which the context is drawn. The answers are themselves a dict with structure:</p> <pre>{'text': list(str),\n 'answer_start': list(int)\n}\n</pre> <p>This is important since there may be multiple answers in a single context.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#imports", "title": "Imports\u00b6", "text": "<p>Let's start by importing the libraries we're going to need.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#building-our-dictionaries", "title": "Building our Dictionaries\u00b6", "text": ""}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#building-dictionaries-from-data", "title": "Building Dictionaries from Data\u00b6", "text": "<p>Now that we can turn our CSV of questions, answers, and filenames into dictionaries, we'll need to construct a dictionary for each question and answer pair in our file.</p> <p>Let's use the <code>iterrows()</code> method of the dataframe to iterate through each question and answer pair, and use the <code>qa_to_squad</code> function to generate a SQuAD-formatted dictionary. Then we will append the formatted dictionary onto a list. At the end, you will have a list of SQuAD-formatted questions and answers.</p> <p>In production, you may want to use the <code>uuid</code> library, or take the hash of the question + context + answers to ensure unique identifiers can be reconstructed. To simplify the process, we can use the index of the row from the <code>iterrows()</code> method.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#dicts-to-datasets", "title": "Dicts to Datasets\u00b6", "text": "<p>Unfortunately, HuggingFace's <code>datasets</code> library does not have a great way to turn a list of dictionaries into a Dataset. Luckily for us, <code>datasets</code> plays very nicely with <code>pandas</code>, which is happy to construct a DataFrame from a list of dictionaries. First, we'll need to turn our list of SQuAD-formatted questions into a DataFrame. Then, we'll need to use the <code>Dataset</code> <code>.from_pandas</code> method to create our HuggingFace-friendly dataset!</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#saving-our-dataset", "title": "Saving our Dataset\u00b6", "text": "<p>Having done all this hard work, the last thing we want to do is have to reconstruct our dataset from scratch at run time. To do this, call the <code>.save_to_disk()</code> method on your <code>Dataset</code> object</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#fine-tuning-bert-on-our-data", "title": "Fine Tuning BERT on our Data\u00b6", "text": "<p>Now that we have our dataset, let's fine-tune a pretrained model on it! This code is written for you, but we're using HuggingFace's <code>transformers</code> library and the <code>AutoModelForQuestionAnswering</code> to do the training here.</p>"}, {"location": "generative_ai/tutorials/Building_QA_Datasets/#pre-and-post-processing-functions", "title": "Pre- and Post-processing Functions\u00b6", "text": "<p>The following functions are lightly adapted from the HuggingFace <code>run_qa.py</code> example on Github</p>"}, {"location": "generative_ai/tutorials/diffusers/", "title": "Diffusion models and the diffusers library from \ud83e\udd17", "text": "In\u00a0[\u00a0]: Copied! <pre>from diffusers import DiffusionPipeline, AutoPipelineForText2Image\nfrom diffusers.utils import load_image, make_image_grid\n\nimport torch\n</pre> from diffusers import DiffusionPipeline, AutoPipelineForText2Image from diffusers.utils import load_image, make_image_grid  import torch <p>Now we're going to see a few applications in the field of image generation and editing, as well as video generation.</p> In\u00a0[\u00a0]: Copied! <pre>rand_gen = torch.manual_seed(12418351)\n\nmodel_name = 'google/ddpm-celebahq-256'\n\nmodel = DiffusionPipeline.from_pretrained(model_name).to(\"cuda\")\nimage = model(generator=rand_gen).images[0]\nimage\n</pre> rand_gen = torch.manual_seed(12418351)  model_name = 'google/ddpm-celebahq-256'  model = DiffusionPipeline.from_pretrained(model_name).to(\"cuda\") image = model(generator=rand_gen).images[0] image <p>While most of them can be used this way, some require some special handling. In that case, the code needed to use them is typically reported in the Model Card that can be accessed by simply clicking on the name of the models in this list.</p> In\u00a0[\u00a0]: Copied! <pre>pipe = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/sdxl-turbo\", \n    torch_dtype=torch.float16, \n    variant=\"fp16\"\n).to(\"cuda\")\n</pre> pipe = AutoPipelineForText2Image.from_pretrained(     \"stabilityai/sdxl-turbo\",      torch_dtype=torch.float16,      variant=\"fp16\" ).to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>prompt = \"A photo of a wild horse jumping in the desert dramatic sky intricate details National Geographic 8k high details\"\n\nrand_gen = torch.manual_seed(423122981)\n\nimage = pipe(\n    prompt=prompt, \n    num_inference_steps=1, # For this model you can use 1, but for normal Stable Diffusion you should use 25 or 50\n    guidance_scale=1.0, # For this model 1 is fine, for normal Stable Diffusion you should use 6 or 7, or up to 10 or so\n    negative_prompt=[\"overexposed\", \"underexposed\"], \n    generator=rand_gen\n).images[0]\n\nimage\n</pre> prompt = \"A photo of a wild horse jumping in the desert dramatic sky intricate details National Geographic 8k high details\"  rand_gen = torch.manual_seed(423122981)  image = pipe(     prompt=prompt,      num_inference_steps=1, # For this model you can use 1, but for normal Stable Diffusion you should use 25 or 50     guidance_scale=1.0, # For this model 1 is fine, for normal Stable Diffusion you should use 6 or 7, or up to 10 or so     negative_prompt=[\"overexposed\", \"underexposed\"],      generator=rand_gen ).images[0]  image <p>RESTART NOW: please restart the notebook, then start running from the next cell and continue on</p> In\u00a0[\u00a0]: Copied! <pre>from diffusers import DiffusionPipeline, AutoPipelineForText2Image\nfrom diffusers.utils import load_image, make_image_grid\n\nimport torch\n</pre> from diffusers import DiffusionPipeline, AutoPipelineForText2Image from diffusers.utils import load_image, make_image_grid  import torch <p>This is another example, where we use a nice model by Playground AI that generates artistic images instead of photorealistic ones:</p> In\u00a0[\u00a0]: Copied! <pre>pipe = AutoPipelineForText2Image.from_pretrained(\n    \"playgroundai/playground-v2-1024px-aesthetic\",\n    torch_dtype=torch.float16,\n    use_safetensors=True,\n    add_watermarker=False,\n    variant=\"fp16\"\n).to(\"cuda\")\n</pre> pipe = AutoPipelineForText2Image.from_pretrained(     \"playgroundai/playground-v2-1024px-aesthetic\",     torch_dtype=torch.float16,     use_safetensors=True,     add_watermarker=False,     variant=\"fp16\" ).to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>prompt = \"A scifi astronaut in a jungle, cold color palette, muted colors, detailed, 8k\"\nrand_gen = torch.manual_seed(42312981)\n\nimage  = pipe(prompt=prompt, guidance_scale=3.0, generator=rand_gen).images[0]\nimage\n</pre> prompt = \"A scifi astronaut in a jungle, cold color palette, muted colors, detailed, 8k\" rand_gen = torch.manual_seed(42312981)  image  = pipe(prompt=prompt, guidance_scale=3.0, generator=rand_gen).images[0] image <p>PLEASE RESTART NOW to free GPU memory, then continue on from the next cell</p> In\u00a0[\u00a0]: Copied! <pre>from diffusers import DiffusionPipeline, AutoPipelineForText2Image\nfrom diffusers.utils import load_image, make_image_grid\n\nimport torch\n</pre> from diffusers import DiffusionPipeline, AutoPipelineForText2Image from diffusers.utils import load_image, make_image_grid  import torch In\u00a0[\u00a0]: Copied! <pre>prompt = \"A tree and a house, made by a child with 3 colors\"\nrand_gen = torch.manual_seed(423121)\n\npipe = AutoPipelineForText2Image.from_pretrained(\n    \"stabilityai/sdxl-turbo\", \n    torch_dtype=torch.float16, \n    variant=\"fp16\"\n).to(\"cuda\")\n\nimage  = pipe(\n    prompt=prompt, \n    num_inference_steps=2,\n    guidance_scale=2,\n    generator=rand_gen\n).images[0]\nimage\n</pre> prompt = \"A tree and a house, made by a child with 3 colors\" rand_gen = torch.manual_seed(423121)  pipe = AutoPipelineForText2Image.from_pretrained(     \"stabilityai/sdxl-turbo\",      torch_dtype=torch.float16,      variant=\"fp16\" ).to(\"cuda\")  image  = pipe(     prompt=prompt,      num_inference_steps=2,     guidance_scale=2,     generator=rand_gen ).images[0] image In\u00a0[\u00a0]: Copied! <pre>image.save(\"sketch.png\")\n</pre> image.save(\"sketch.png\") <p>PLEASE RESTART NOW to free GPU memory, then continue on from the next cell</p> In\u00a0[\u00a0]: Copied! <pre>from diffusers import DiffusionPipeline, AutoPipelineForText2Image\nfrom diffusers.utils import load_image, make_image_grid\nimport PIL\nimport torch\n</pre> from diffusers import DiffusionPipeline, AutoPipelineForText2Image from diffusers.utils import load_image, make_image_grid import PIL import torch In\u00a0[\u00a0]: Copied! <pre>image = PIL.Image.open(\"sketch.png\")\n</pre> image = PIL.Image.open(\"sketch.png\") <p>Now we can use the Kandinsky model to generate an image that respects the subjects and their positions in our sketch:</p> In\u00a0[\u00a0]: Copied! <pre>from diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline\n\nprior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\npipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\")\n</pre> from diffusers import KandinskyV22Img2ImgPipeline, KandinskyPriorPipeline  prior_pipeline = KandinskyPriorPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-prior\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\") pipeline = KandinskyV22Img2ImgPipeline.from_pretrained(\"kandinsky-community/kandinsky-2-2-decoder\", torch_dtype=torch.float16, use_safetensors=True).to(\"cuda\") In\u00a0[\u00a0]: Copied! <pre>import numpy as np\n\noriginal_image = image.copy().resize((768, 768))\n\nprompt = \"A photograph of a house in the fall, high details, broad daylight\"\nnegative_prompt = \"low quality, bad quality\"\n    \nrand_gen = torch.manual_seed(67806801)\nimage_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, generator=rand_gen).to_tuple()\n\nnew_image = pipeline(\n    image=original_image, \n    image_embeds=image_embeds, \n    negative_image_embeds=negative_image_embeds, \n    height=768, \n    width=768, \n    strength=0.35,\n    generator=rand_gen\n).images[0]\nfig = make_image_grid([original_image.resize((512, 512)), new_image.resize((512, 512))], rows=1, cols=2)\nfig\n</pre> import numpy as np  original_image = image.copy().resize((768, 768))  prompt = \"A photograph of a house in the fall, high details, broad daylight\" negative_prompt = \"low quality, bad quality\"      rand_gen = torch.manual_seed(67806801) image_embeds, negative_image_embeds = prior_pipeline(prompt, negative_prompt, generator=rand_gen).to_tuple()  new_image = pipeline(     image=original_image,      image_embeds=image_embeds,      negative_image_embeds=negative_image_embeds,      height=768,      width=768,      strength=0.35,     generator=rand_gen ).images[0] fig = make_image_grid([original_image.resize((512, 512)), new_image.resize((512, 512))], rows=1, cols=2) fig In\u00a0[\u00a0]: Copied! <pre>from diffusers import DiffusionPipeline, AutoPipelineForText2Image\nfrom diffusers.utils import load_image, make_image_grid\n\nimport torch\n</pre> from diffusers import DiffusionPipeline, AutoPipelineForText2Image from diffusers.utils import load_image, make_image_grid  import torch In\u00a0[\u00a0]: Copied! <pre>import torch\nfrom diffusers import AutoPipelineForInpainting\nfrom diffusers.utils import load_image, make_image_grid\n\npipeline = AutoPipelineForInpainting.from_pretrained(\n    \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16\n)\npipeline.enable_model_cpu_offload()\n</pre> import torch from diffusers import AutoPipelineForInpainting from diffusers.utils import load_image, make_image_grid  pipeline = AutoPipelineForInpainting.from_pretrained(     \"kandinsky-community/kandinsky-2-2-decoder-inpaint\", torch_dtype=torch.float16 ) pipeline.enable_model_cpu_offload() In\u00a0[\u00a0]: Copied! <pre>init_image = load_image(\"monalisa.png\").resize((512, 512))\nmask_image = load_image(\"monalisa_mask.png\").resize((512, 512))\n</pre> init_image = load_image(\"monalisa.png\").resize((512, 512)) mask_image = load_image(\"monalisa_mask.png\").resize((512, 512)) In\u00a0[\u00a0]: Copied! <pre>import matplotlib.pyplot as plt\n\nprompt = \"oil painting of a woman, sfumato, renaissance, low details, Da Vinci\"\nnegative_prompt = \"bad anatomy, deformed, ugly, disfigured\"\n\nrand_gen = torch.manual_seed(74294536)\nimage = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image, generator=rand_gen, guidance_scale=1.5).images[0]\nfig = make_image_grid([init_image, mask_image, image], rows=1, cols=3)\nfig\n</pre> import matplotlib.pyplot as plt  prompt = \"oil painting of a woman, sfumato, renaissance, low details, Da Vinci\" negative_prompt = \"bad anatomy, deformed, ugly, disfigured\"  rand_gen = torch.manual_seed(74294536) image = pipeline(prompt=prompt, negative_prompt=negative_prompt, image=init_image, mask_image=mask_image, generator=rand_gen, guidance_scale=1.5).images[0] fig = make_image_grid([init_image, mask_image, image], rows=1, cols=3) fig In\u00a0[\u00a0]: Copied! <pre>from helpers import get_video\nfrom IPython.display import Video\n\n\npipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\n\nprompt = \"Earth sphere from space\"\n\nrand_gen = torch.manual_seed(42312981)\nframes = pipe(prompt, generator=rand_gen).frames\n\nVideo(get_video(frames, \"earth.mp4\"))\n</pre> from helpers import get_video from IPython.display import Video   pipe = DiffusionPipeline.from_pretrained(\"damo-vilab/text-to-video-ms-1.7b\", torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")  prompt = \"Earth sphere from space\"  rand_gen = torch.manual_seed(42312981) frames = pipe(prompt, generator=rand_gen).frames  Video(get_video(frames, \"earth.mp4\")) <p>We can also generate a video starting from an image. For example, let's consider the following image (which was generated with Stable Diffusion XL and then outpainted using DALLE-2):</p> <p>PLEASE RESTART NOW to free GPU memory, then continue on from the next cell</p> In\u00a0[\u00a0]: Copied! <pre>import torch\n\nfrom diffusers import StableVideoDiffusionPipeline\nfrom diffusers.utils import load_image, export_to_video\nfrom helpers import get_video\nfrom IPython.display import Video\n\npipe = StableVideoDiffusionPipeline.from_pretrained(\n    \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\"\n).to(\"cuda\")\n# These two settings lower the VRAM usage\npipe.enable_model_cpu_offload()\n# pipe.unet.enable_forward_chunking()\n</pre> import torch  from diffusers import StableVideoDiffusionPipeline from diffusers.utils import load_image, export_to_video from helpers import get_video from IPython.display import Video  pipe = StableVideoDiffusionPipeline.from_pretrained(     \"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\" ).to(\"cuda\") # These two settings lower the VRAM usage pipe.enable_model_cpu_offload() # pipe.unet.enable_forward_chunking() In\u00a0[\u00a0]: Copied! <pre># Load the conditioning image\nimage = load_image(\"in_the_desert_outpaint.png\")\nimage = image.resize((1024, 576))\n</pre> # Load the conditioning image image = load_image(\"in_the_desert_outpaint.png\") image = image.resize((1024, 576)) In\u00a0[\u00a0]: Copied! <pre>generator = torch.manual_seed(999)\nres = pipe(\n    image, \n    decode_chunk_size=2, \n    generator=generator, \n    num_inference_steps=15, \n    num_videos_per_prompt=1\n)\n</pre> generator = torch.manual_seed(999) res = pipe(     image,      decode_chunk_size=2,      generator=generator,      num_inference_steps=15,      num_videos_per_prompt=1 ) In\u00a0[\u00a0]: Copied! <pre>Video(get_video(res.frames[0], \"horse2.mp4\"))\n</pre> Video(get_video(res.frames[0], \"horse2.mp4\")) <p>Overall the animation looks a bit off: the appearance of the legs can definitely be improved, although the overall motion seems correct. Let's try with a different object which has less parts:</p> In\u00a0[\u00a0]: Copied! <pre>image = load_image(\"xwing.jpeg\")\nimage = image.resize((1024, 576))\n\ngenerator = torch.manual_seed(999)\nres = pipe(\n    image, \n    decode_chunk_size=2, \n    generator=generator, \n    num_inference_steps=25, \n    num_videos_per_prompt=1\n)\n</pre> image = load_image(\"xwing.jpeg\") image = image.resize((1024, 576))  generator = torch.manual_seed(999) res = pipe(     image,      decode_chunk_size=2,      generator=generator,      num_inference_steps=25,      num_videos_per_prompt=1 ) In\u00a0[\u00a0]: Copied! <pre>Video(get_video(res.frames[0], \"xwing.mp4\"))\n</pre> Video(get_video(res.frames[0], \"xwing.mp4\")) <p>The animation is definitely more realistic here, but still there's a lot to be desired. However, feel free to try other images and prompts and see what you get!</p>"}, {"location": "generative_ai/tutorials/diffusers/#diffusion-models-and-the-diffusers-library-from", "title": "Diffusion models and the <code>diffusers</code> library from \ud83e\udd17\u00b6", "text": "<p>Diffusion models are at the forefront of the research and the applications of Gen-AI in Computer Vision. Many research papers and innovations are published every day in the field. Most of the open-source models and results are released and implemented in the \ud83e\udd17 (Hugging Face) libraries: transformers for LLMs and diffusers for Diffusion Models. Both of them are high-level libraries built on top of PyTorch.</p> <p>In this exercise we are going to have fun with some of the amazing capabilities of the <code>diffusers</code> library along with the \ud83e\udd17 ecosystem of Models, an online repository where open-source models are hosted and are available for use free of charge.</p> <p>Please note: always check the license of the models before using them in a professional setting because some restrict commercial use.</p> <p>It is worth noting here that the \ud83e\udd17 also includes freely-available Datasets and Spaces, where people can publish demos of interesting applications of Gen-AI and more.</p> <p>IMPORTANT: when using this notebook within the Udacity Workspace, you need to restart the notebook when requested because you will run out of GPU memory otherwise. Be on the lookout for the RESTART NOW message</p> <p>NOTE: because we are using a lot of different models, the first time you run them you will see a lot of messages from the diffusers library alerting you that it is downloading files from the internet. Those are expected, and they do NOT constitute an error. Just continue on.</p> <p>Let's start by importing the elements we are going to use:</p>"}, {"location": "generative_ai/tutorials/diffusers/#unconditional-generation", "title": "Unconditional generation\u00b6", "text": "<p>In this type of image generation, the generator is free to do whatever it wants, i.e., it is in \"slot machine\" mode: we pull the lever and the model will generate something related to the training set it has been trained on. The only control we have here is the random seed.</p> <p>You can see all the available unconditional diffusion models compatible with the <code>diffusers</code> library here. For example, at the moment the first few results look like this:</p> <p>You can substitute the <code>model_name</code> value in the cell below with any of these model names, like for example <code>google/ddpm-cifar10-32</code> or <code>WiNE-iNEFF/Minecraft-Skin-Diffusion-V3</code>:</p>"}, {"location": "generative_ai/tutorials/diffusers/#text-to-image", "title": "Text-to-image\u00b6", "text": "<p>This is a class of conditional image generation models. The conditioning happens through text: we provide a text prompt, and the model creates an image following that description.</p> <p>You can find a list of available text-to-image models here.</p> <p>For example, here we use the Stable Diffusion XL Turbo (a new version of Stable Diffusion XL optimized for super-fast inference):</p>"}, {"location": "generative_ai/tutorials/diffusers/#image-to-image", "title": "Image-to-image\u00b6", "text": "<p>In the image-to-image task we condition the production of the Diffusion Model through an input image. There are many ways of doing this. Here we look at transforming a barebone sketch of a scene in a beautiful, highly-detailed representation of the same.</p> <p>Let's start by creating a sketch. We could create that manually, but since we're here, let's use SDXL-Turbo instead.</p> <p>NOTE: it is important for the sketch not to be too detailed and complicated. Flat colors typically work best, although this is not an absolute rule.</p>"}, {"location": "generative_ai/tutorials/diffusers/#inpainting", "title": "Inpainting\u00b6", "text": "<p>Diffusion models can also be used to do inpainting, which means filling regions of an image according to a prompt (or just according to the sourroundings of the hole to fill).</p> <p>Typically, we start from an image and a mask. The mask indicates the pixels to be inpainted, i.e., removed from the original image and filled with new content generated by the model.</p> <p>PLEASE RESTART NOW to free GPU memory, then continue on from the next cell</p>"}, {"location": "generative_ai/tutorials/diffusers/#beyond-images", "title": "Beyond images\u00b6", "text": "<p>Diffusion models can also be used for video generation. At the moment of the writing, this field is still in its infancy, but it is progressing fast so keep an eye on the available models as there might be much better ones by the time you are reading this.</p> <p>The list of available model for text-to-video is available here</p>"}, {"location": "generative_ai/tutorials/example_ReAcT_prompt/", "title": "React promt example", "text": "<p>In this exercise, you explored creating a wellness agent using ReACT Prompting techniques</p> <p>Your system message (ReACT agent instructions) will be different than the example below.  Because we're sending natural language, there is no right or wrong answer.</p> In\u00a0[1]: Copied! <pre># Importing the library for OpenAI API\nfrom openai import OpenAI\nimport os\n\n# Define OpenAI API key\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n</pre> # Importing the library for OpenAI API from openai import OpenAI import os  # Define OpenAI API key client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) <p>This example user prompt is related to wellness.</p> In\u00a0[3]: Copied! <pre># Creating the prompt\nuser_prompt = \"How can I know my diet is improving my wellness?\"\nprint(user_prompt)\n</pre> # Creating the prompt user_prompt = \"How can I know my diet is improving my wellness?\" print(user_prompt) <pre>How can I know my diet is improving my wellness?\n</pre> <p>An example solution to a ReACT system prompt for a wellness agent:</p> In\u00a0[4]: Copied! <pre># Function to call the OpenAI GPT-3.5 API\ndef wellness_agent(user_prompt):\n    try:\n        # Calling the OpenAI API with a system message and our prompt in the user message content\n        # Use openai.ChatCompletion.create for openai &lt; 1.0\n        # openai.chat.completions.create for openai &gt; 1.0\n        response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"\"\"Your goal is to improve the wellness of the user by interleaving thought, action, and observation steps.\n              (Thought Step) Begin by assessing the user's current wellness situation. Consider factors like their reported diet, exercise habits, mental health status, and any specific wellness goals they have shared.\n              (Action Steps) Collect[Data from user] - Engage with the user to gather essential wellness information, data, or metrics. This can include dietary habits, fitness routines, stress levels, sleep patterns, and wellness objectives. \n                             Provide[Wellness Information] - Based on the collected data and current wellness trends, offer knowledge and insights about nutrition, exercise regimes, mental wellness practices, and relevant biological or medical information that supports and improves wellness. \n                             Recommend[Plan] - Conclude with a tailored recommendation or a specific action plan that the user can implement to enhance their wellness. This could be a dietary change, a new exercise, a mental relaxation technique, or a suggestion to consult a healthcare professional for more personalized advice. \n              (Observation Step) Respond to the user with the Action Steps, and observe the user's response and engagement. Gauge their understanding and willingness to follow the suggestions. Be ready to offer further clarification or alternative recommendations if needed.\n              Repeat these steps N times until the user's wellness has improved.\n              Example: \n              [User Query] I'm feeling stressed and not sleeping well. What can I do to improve my sleep? \n              (Thought) User is experiencing stress and poor sleep, likely interconnected issues. \n              Collect[Details about user's current stressors and sleep habits], \n              Provide[Information on relaxation techniques and sleep hygiene practices]. \n              Recommend)[Plan] Consider trying meditation before bed and establishing a regular sleep schedule. \n              What are some current stressors in your life? How many hours of sleep do you get each night?\n              Have you tried meditation before bed? Do you have a regular sleep schedule?\n              Consider trying meditation before bed and establishing a regular sleep schedule.\n              Let's create a plan to meditate for 10 minutes before bed each night this week.\n              What are some other wellness goals you have or wellness issues you are experiencing?\n              \"\"\",\n                },\n                {\"role\": \"user\", \"content\": user_prompt},\n            ],\n            temperature=1,\n            max_tokens=512,\n            top_p=1,\n            frequency_penalty=0,\n            presence_penalty=0,\n        )\n        # The response is a JSON object containing more information than the response. We want to return only the message content\n        return response.choices[0].message.content\n    except Exception as e:\n        return f\"An error occurred: {e}\"\n\n\n# Running the wellness agent\nrun_wellness_agent = wellness_agent(user_prompt)\n\n# Printing the output.\nprint(\"Wellness Agent Response: \")\nprint(run_wellness_agent)\n</pre> # Function to call the OpenAI GPT-3.5 API def wellness_agent(user_prompt):     try:         # Calling the OpenAI API with a system message and our prompt in the user message content         # Use openai.ChatCompletion.create for openai &lt; 1.0         # openai.chat.completions.create for openai &gt; 1.0         response = client.chat.completions.create(             model=\"gpt-3.5-turbo\",             messages=[                 {                     \"role\": \"system\",                     \"content\": \"\"\"Your goal is to improve the wellness of the user by interleaving thought, action, and observation steps.               (Thought Step) Begin by assessing the user's current wellness situation. Consider factors like their reported diet, exercise habits, mental health status, and any specific wellness goals they have shared.               (Action Steps) Collect[Data from user] - Engage with the user to gather essential wellness information, data, or metrics. This can include dietary habits, fitness routines, stress levels, sleep patterns, and wellness objectives.                               Provide[Wellness Information] - Based on the collected data and current wellness trends, offer knowledge and insights about nutrition, exercise regimes, mental wellness practices, and relevant biological or medical information that supports and improves wellness.                               Recommend[Plan] - Conclude with a tailored recommendation or a specific action plan that the user can implement to enhance their wellness. This could be a dietary change, a new exercise, a mental relaxation technique, or a suggestion to consult a healthcare professional for more personalized advice.                (Observation Step) Respond to the user with the Action Steps, and observe the user's response and engagement. Gauge their understanding and willingness to follow the suggestions. Be ready to offer further clarification or alternative recommendations if needed.               Repeat these steps N times until the user's wellness has improved.               Example:                [User Query] I'm feeling stressed and not sleeping well. What can I do to improve my sleep?                (Thought) User is experiencing stress and poor sleep, likely interconnected issues.                Collect[Details about user's current stressors and sleep habits],                Provide[Information on relaxation techniques and sleep hygiene practices].                Recommend)[Plan] Consider trying meditation before bed and establishing a regular sleep schedule.                What are some current stressors in your life? How many hours of sleep do you get each night?               Have you tried meditation before bed? Do you have a regular sleep schedule?               Consider trying meditation before bed and establishing a regular sleep schedule.               Let's create a plan to meditate for 10 minutes before bed each night this week.               What are some other wellness goals you have or wellness issues you are experiencing?               \"\"\",                 },                 {\"role\": \"user\", \"content\": user_prompt},             ],             temperature=1,             max_tokens=512,             top_p=1,             frequency_penalty=0,             presence_penalty=0,         )         # The response is a JSON object containing more information than the response. We want to return only the message content         return response.choices[0].message.content     except Exception as e:         return f\"An error occurred: {e}\"   # Running the wellness agent run_wellness_agent = wellness_agent(user_prompt)  # Printing the output. print(\"Wellness Agent Response: \") print(run_wellness_agent) <pre>Wellness Agent Response: \n(Thought) User is interested in evaluating how their diet impacts their overall wellness, indicating a willingness to make positive changes in their eating habits. \n\nCollect[Details about user's current diet] - Could you share what your typical daily meals consist of? Do you have any specific dietary preferences or restrictions?\n\nProvide[Wellness Information] - A balanced diet rich in fruits, vegetables, whole grains, lean proteins, and healthy fats is crucial for overall wellness. Monitoring how your body feels and functions can provide insights into the impact of your diet on your well-being. \n\nRecommend[Plan] - Keep a food journal to track what you eat and how it makes you feel physically and mentally. Consider consulting a nutritionist or healthcare professional to assess your current diet and make personalized recommendations. Implement small changes, one at a time, to make it easier to evaluate their effects on your wellness.\n\nObservation: How do you feel after meals? Are there any specific foods that make you feel more energetic or sluggish? Would you consider keeping a food diary to track your meals and how they affect your well-being? Let's take a step towards improvement by focusing on one aspect of your diet that you feel could be healthier.\n</pre> In\u00a0[8]: Copied! <pre># Running the wellness agent\nrun_wellness_agent = wellness_agent(user_prompt)\n\n# Printing the output.\nprint(\"Wellness Agent Response: \")\nprint(run_wellness_agent)\n</pre> # Running the wellness agent run_wellness_agent = wellness_agent(user_prompt)  # Printing the output. print(\"Wellness Agent Response: \") print(run_wellness_agent) <pre>Wellness Agent Response: \nThought: The user seems to have a structured eating pattern with intermittent fasting, a balanced diet with adequate protein, carbs, and vegetables, regular exercise routine including strength training, and good hydration habits. However, they are experiencing a lack of energy during the day and have difficulty concentrating, impacting their mental wellness.\n\nCollect: How long have you been following this eating and exercise routine? How many hours of sleep do you typically get per night? Do you consume any caffeine or stimulants besides coffee before lunch?\n\nProvide: Lack of energy during the day and difficulty concentrating can be influenced by various factors, including sleep quality, nutrient intake, hydration levels, and stress management. It's essential to ensure you're getting enough quality sleep, maintaining proper hydration, and managing stress effectively.\n\nRecommend: \n1. **Adjust Meal Timing:** Consider shifting your fasting window to match your body's energy levels better. You could try having a balanced breakfast to kickstart your day and see if it enhances your morning energy levels.\n2. **Nutrient-Rich Snacks:** Instead of kefir with whey protein and oatmeal, consider adding more variety to your snacks to ensure you're getting a broad range of nutrients.\n3. **Hydration:** Continue with your good hydration habits, ensuring you're getting enough water throughout the day.\n4. **Sleep Routine:** Aim for 7-9 hours of quality sleep each night to support your physical and mental wellness.\n5. **Mental Focus:** Practice mindfulness techniques such as meditation or deep breathing exercises to enhance your concentration and focus during work or study.\n\nHow do you feel about adjusting your meal timing or trying some new nutrient-rich snack options? Are you open to incorporating mindfulness techniques into your daily routine to improve concentration?\n</pre>"}, {"location": "generative_ai/tutorials/hugginface_intro/", "title": "Hugginface Intro", "text": "<p>Hugging Face is a company making waves in the technology world with its amazing tools for understanding and using human language in computers. Hugging Face offers everything from tokenizers, which help computers make sense of text, to a huge variety of ready-to-go language models, and even a treasure trove of data suited for language tasks.</p> <p>These work like a translator, converting the words we use into smaller parts and creating a secret code that computers can understand and work with.</p> <p>HuggingFace tokenizers help us break down text into smaller, manageable pieces called tokens. These tokenizers are easy to use and also remarkably fast due to their use of the Rust programming language.</p> <ul> <li><p>Tokenization: It's like cutting a sentence into individual pieces, such as words or characters, to make it easier to analyze.</p> </li> <li><p>Tokens: These are the pieces you get after cutting up text during tokenization, kind of like individual Lego blocks that can be words, parts of words, or even single letters. These tokens are converted to numerical values for models to understand.</p> </li> <li><p>Pre-trained Model: This is a ready-made model that has been previously taught with a lot of data.</p> </li> <li><p>Uncased: This means that the model treats uppercase and lowercase letters as the same.</p> </li> </ul> In\u00a0[1]: Copied! <pre>from transformers import BertTokenizer\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# See how many tokens are in the vocabulary\ntokenizer.vocab_size\n</pre> from transformers import BertTokenizer  # Initialize the tokenizer tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # See how many tokens are in the vocabulary tokenizer.vocab_size <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.0/28.0 [00:00&lt;00:00, 6.83kB/s]\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00&lt;00:00, 1.43MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00&lt;00:00, 1.46MB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 570/570 [00:00&lt;00:00, 1.49MB/s]\n</pre> Out[1]: <pre>30522</pre> In\u00a0[2]: Copied! <pre># Tokenize the sentence\ntokens = tokenizer.tokenize(\"I heart Generative AI\")\n\n# Print the tokens\nprint(tokens)\n# ['i', 'heart', 'genera', '##tive', 'ai']\n\n# Show the token ids assigned to each token\nprint(tokenizer.convert_tokens_to_ids(tokens))\n# [1045, 2540, 11416, 6024, 9932]\n</pre> # Tokenize the sentence tokens = tokenizer.tokenize(\"I heart Generative AI\")  # Print the tokens print(tokens) # ['i', 'heart', 'genera', '##tive', 'ai']  # Show the token ids assigned to each token print(tokenizer.convert_tokens_to_ids(tokens)) # [1045, 2540, 11416, 6024, 9932] <pre>['i', 'heart', 'genera', '##tive', 'ai']\n[1045, 2540, 11416, 6024, 9932]\n</pre> <ul> <li><p>Hugging Face Tokenizers documentation index</p> </li> <li><p>Hugging Face Tokenizer documentation</p> </li> <li><p>Hugging Face BertTokenizer documentation</p> </li> </ul> <p>These are like the brain for computers, allowing them to learn and make decisions based on information they've been fed.</p> <p>Hugging Face models provide a quick way to get started using models trained by the community. With only a few lines of code, you can load a pre-trained model and start using it on tasks such as sentiment analysis.</p> In\u00a0[3]: Copied! <pre>from transformers import BertForSequenceClassification, BertTokenizer\nimport torch\n# Load a pre-trained sentiment analysis model\nmodel_name = \"textattack/bert-base-uncased-imdb\"\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n# Tokenize the input sequence\ntokenizer = BertTokenizer.from_pretrained(model_name)\ninputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")\n\n# Make prediction\nwith torch.no_grad():\n    outputs = model(**inputs).logits\n    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n    predicted_class = torch.argmax(probabilities)\n\n# Display sentiment result\nif predicted_class == 1:\n    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\nelse:\n    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n</pre> from transformers import BertForSequenceClassification, BertTokenizer import torch # Load a pre-trained sentiment analysis model model_name = \"textattack/bert-base-uncased-imdb\" model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)  # Tokenize the input sequence tokenizer = BertTokenizer.from_pretrained(model_name) inputs = tokenizer(\"I love Generative AI\", return_tensors=\"pt\")  # Make prediction with torch.no_grad():     outputs = model(**inputs).logits     probabilities = torch.nn.functional.softmax(outputs, dim=1)     predicted_class = torch.argmax(probabilities)  # Display sentiment result if predicted_class == 1:     print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\") else:     print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\") <pre>config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 511/511 [00:00&lt;00:00, 1.32MB/s]\npytorch_model.bin: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 438M/438M [00:17&lt;00:00, 25.6MB/s] \ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 48.0/48.0 [00:00&lt;00:00, 111kB/s]\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00&lt;00:00, 1.53MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 112/112 [00:00&lt;00:00, 465kB/s]\n</pre> <pre>Sentiment: Positive (88.68%)\n</pre> <ul> <li><p>Hugging Face Transformers documentation index</p> </li> <li><p>Hugging Face models search</p> </li> <li><p>textattack/bert-base-uncased-imdb model documentation on Hugging Face(</p> </li> <li><p>Hugging Face BertForSequenceClassification documentation</p> </li> <li><p>torch.nn.functional.softmax documentation</p> </li> <li><p>torch.argmax documentation</p> </li> </ul> <p>Think of datasets as textbooks for computer models. They are collections of information that models study to learn and improve.</p> <p>HuggingFace Datasets library is a powerful tool for managing a variety of data types, like text and images, efficiently and easily. This resource is incredibly fast and doesn't use a lot of computer memory, making it great for handling big projects without any hassle.</p> In\u00a0[1]: Copied! <pre>from datasets import load_dataset\nfrom IPython.display import HTML, display\n\n# Load the IMDB dataset, which contains movie reviews\n# and sentiment labels (positive or negative)\ndataset = load_dataset(\"imdb\")\n\n# Fetch a revie from the training set\nreview_number = 42\nsample_review = dataset[\"train\"][review_number]\n</pre> from datasets import load_dataset from IPython.display import HTML, display  # Load the IMDB dataset, which contains movie reviews # and sentiment labels (positive or negative) dataset = load_dataset(\"imdb\")  # Fetch a revie from the training set review_number = 42 sample_review = dataset[\"train\"][review_number] <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.81k/7.81k [00:00&lt;00:00, 4.79MB/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 21.0M/21.0M [00:01&lt;00:00, 13.9MB/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.5M/20.5M [00:01&lt;00:00, 14.5MB/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 42.0M/42.0M [00:03&lt;00:00, 13.8MB/s]\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [00:00&lt;00:00, 113012.22 examples/s]\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [00:00&lt;00:00, 91629.83 examples/s]\nGenerating unsupervised split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [00:00&lt;00:00, 76244.49 examples/s]\n</pre> In\u00a0[2]: Copied! <pre>display(HTML(sample_review[\"text\"][:450] + \"...\"))\n</pre> display(HTML(sample_review[\"text\"][:450] + \"...\"))  WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials...  In\u00a0[3]: Copied! <pre>if sample_review[\"label\"] == 1:\n    print(\"Sentiment: Positive\")\nelse:\n    print(\"Sentiment: Negative\")\n</pre> if sample_review[\"label\"] == 1:     print(\"Sentiment: Positive\") else:     print(\"Sentiment: Negative\") <pre>Sentiment: Negative\n</pre> <ul> <li><p>Hugging Face Datasets documentation index</p> </li> <li><p>Hugging Face Datasets search</p> </li> <li><p>Hugging Face load_datasets documentation</p> </li> <li><p>imdb Dataset documentation on Hugging Face</p> </li> </ul> <p>Trainers are the coaches for computer models. They help these models get better at their tasks by practicing and providing guidance. HuggingFace Trainers implement the PyTorch training loop for you, so you can focus instead on other aspects of working on the model.</p> <p>Hugging Face trainers offer a simplified approach to training generative AI models, making it easier to set up and run complex machine learning tasks. This tool wraps up the hard parts, like handling data and carrying out the training process, allowing us to focus on the big picture and achieve better outcomes with our AI endeavors.</p> <ul> <li><p>Truncating: This refers to shortening longer pieces of text to fit a certain size limit.</p> </li> <li><p>Padding: Adding extra data to shorter texts to reach a uniform length for processing.</p> </li> <li><p>Batches: Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.</p> </li> <li><p>Batch Size: The number of data samples that the machine considers in one go during training.</p> </li> <li><p>Epochs: A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.</p> </li> <li><p>Dataset Splits: Dividing the dataset into parts for different uses, such as training the model and testing how well it works.</p> </li> </ul> In\u00a0[4]: Copied! <pre>from transformers import (\n    DistilBertForSequenceClassification,\n    DistilBertTokenizer,\n    TrainingArguments,\n    Trainer,\n)\nfrom datasets import load_dataset\n\nmodel = DistilBertForSequenceClassification.from_pretrained(\n    \"distilbert-base-uncased\", num_labels=2\n)\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n\n\ndataset = load_dataset(\"imdb\")\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\n</pre> from transformers import (     DistilBertForSequenceClassification,     DistilBertTokenizer,     TrainingArguments,     Trainer, ) from datasets import load_dataset  model = DistilBertForSequenceClassification.from_pretrained(     \"distilbert-base-uncased\", num_labels=2 ) tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")   def tokenize_function(examples):     return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)   dataset = load_dataset(\"imdb\") tokenized_datasets = dataset.map(tokenize_function, batched=True) <pre>config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 483/483 [00:00&lt;00:00, 2.05MB/s]\nmodel.safetensors: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 268M/268M [00:05&lt;00:00, 47.7MB/s] \nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 28.0/28.0 [00:00&lt;00:00, 12.7kB/s]\nvocab.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 232k/232k [00:00&lt;00:00, 2.86MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 466k/466k [00:00&lt;00:00, 1.56MB/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [04:05&lt;00:00, 101.86 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25000/25000 [03:07&lt;00:00, 133.61 examples/s]\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 50000/50000 [06:11&lt;00:00, 134.42 examples/s]\n</pre> In\u00a0[\u00a0]: Copied! <pre>training_args = TrainingArguments(\n    per_device_train_batch_size=64,\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    num_train_epochs=3,\n)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"test\"],\n)\ntrainer.train()\n</pre> training_args = TrainingArguments(     per_device_train_batch_size=64,     output_dir=\"./results\",     learning_rate=2e-5,     num_train_epochs=3, ) trainer = Trainer(     model=model,     args=training_args,     train_dataset=tokenized_datasets[\"train\"],     eval_dataset=tokenized_datasets[\"test\"], ) trainer.train() <ul> <li><p>Hugging Face Trainers documentation index</p> </li> <li><p>Hugging Face DistilBertForSequenceClassification documentation</p> </li> <li><p>Hugging Face DistilBertTokenizer documentation</p> </li> <li><p>distilbert-base-uncased Model documentation on Hugging Face</p> </li> <li><p>Hugging Face transformers.TrainingArguments documentation</p> </li> <li><p>Hugging Face transformers.Trainer documentation</p> </li> </ul>"}, {"location": "generative_ai/tutorials/hugginface_intro/#hugginface-intro", "title": "Hugginface Intro\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#tokenizers", "title": "Tokenizers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#models", "title": "Models\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#datasets", "title": "Datasets\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#trainers", "title": "Trainers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugginface_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/", "title": "Using Hugging Face Tokenizers", "text": "In\u00a0[1]: Copied! <pre>from transformers import AutoTokenizer\n</pre> from transformers import AutoTokenizer <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>In this notebook, we'll explore Hugging Face's tokenizers by using a pretrained model. Hugging Face has many tokenizers available that have already been trained for specific models and tasks!</p> In\u00a0[2]: Copied! <pre># Choose a pretrained tokenizer to use\nmy_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n</pre> # Choose a pretrained tokenizer to use my_tokenizer = AutoTokenizer.from_pretrained('bert-base-cased') In\u00a0[3]: Copied! <pre># Simple method getting tokens from text\nraw_text = '''Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!'''\ntokens = my_tokenizer.tokenize(raw_text)\n\nprint(tokens)\n</pre> # Simple method getting tokens from text raw_text = '''Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!''' tokens = my_tokenizer.tokenize(raw_text)  print(tokens) <pre>['Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!']\n</pre> In\u00a0[4]: Copied! <pre># This method also returns special tokens depending on the pretrained tokenizer\ndetailed_tokens = my_tokenizer(raw_text).tokens()\n\nprint(detailed_tokens)\n</pre> # This method also returns special tokens depending on the pretrained tokenizer detailed_tokens = my_tokenizer(raw_text).tokens()  print(detailed_tokens) <pre>['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n</pre> In\u00a0[5]: Copied! <pre># Way to get tokens as integer IDs\nprint(my_tokenizer.encode(raw_text))\n</pre> # Way to get tokens as integer IDs print(my_tokenizer.encode(raw_text)) <pre>[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n</pre> In\u00a0[6]: Copied! <pre>print(detailed_tokens)\n\n# Tokenizer method to get the IDs if we already have the tokens as strings\ndetailed_ids = my_tokenizer.convert_tokens_to_ids(detailed_tokens)\nprint(detailed_ids)\n</pre> print(detailed_tokens)  # Tokenizer method to get the IDs if we already have the tokens as strings detailed_ids = my_tokenizer.convert_tokens_to_ids(detailed_tokens) print(detailed_ids) <pre>['[CLS]', 'Rory', \"'\", 's', 'shoes', 'are', 'mage', '##nta', 'and', 'so', 'are', 'Corey', \"'\", 's', 'but', 'they', 'aren', \"'\", 't', 'nearly', 'as', 'dark', '!', '[SEP]']\n[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n</pre> <p>Another way can look a little complex but can be useful when working with tokenizers for certain tasks.</p> In\u00a0[7]: Copied! <pre># Returns an object that has a few different keys available\nmy_tokenizer(raw_text)\n</pre> # Returns an object that has a few different keys available my_tokenizer(raw_text) Out[7]: <pre>{'input_ids': [101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}</pre> In\u00a0[8]: Copied! <pre># focus on `input_ids` which are the IDs associated with the tokens.\nprint(my_tokenizer(raw_text).input_ids)\n</pre> # focus on `input_ids` which are the IDs associated with the tokens. print(my_tokenizer(raw_text).input_ids) <pre>[101, 14845, 112, 188, 5743, 1132, 27595, 13130, 1105, 1177, 1132, 19521, 112, 188, 1133, 1152, 4597, 112, 189, 2212, 1112, 1843, 106, 102]\n</pre> <p>We of course can use the tokenizer to go from token IDs to tokens and back to text!</p> In\u00a0[9]: Copied! <pre># Integer IDs for tokens\nids = my_tokenizer.encode(raw_text)\n\n# The inverse of the .enocde() method: .decode()\nmy_tokenizer.decode(ids)\n</pre> # Integer IDs for tokens ids = my_tokenizer.encode(raw_text)  # The inverse of the .enocde() method: .decode() my_tokenizer.decode(ids) Out[9]: <pre>\"[CLS] Rory's shoes are magenta and so are Corey's but they aren't nearly as dark! [SEP]\"</pre> In\u00a0[10]: Copied! <pre># To ignore special tokens (depending on pretrained tokenizer)\nmy_tokenizer.decode(ids, skip_special_tokens=True)\n</pre> # To ignore special tokens (depending on pretrained tokenizer) my_tokenizer.decode(ids, skip_special_tokens=True) Out[10]: <pre>\"Rory's shoes are magenta and so are Corey's but they aren't nearly as dark!\"</pre> In\u00a0[11]: Copied! <pre># List of tokens as strings instead of one long string\nmy_tokenizer.convert_ids_to_tokens(ids)\n</pre> # List of tokens as strings instead of one long string my_tokenizer.convert_ids_to_tokens(ids) Out[11]: <pre>['[CLS]',\n 'Rory',\n \"'\",\n 's',\n 'shoes',\n 'are',\n 'mage',\n '##nta',\n 'and',\n 'so',\n 'are',\n 'Corey',\n \"'\",\n 's',\n 'but',\n 'they',\n 'aren',\n \"'\",\n 't',\n 'nearly',\n 'as',\n 'dark',\n '!',\n '[SEP]']</pre> <p>One thing to consider is if a string is outside of the tokenizer's vocabulary, also known as an \"unkown\" token.</p> <p>They are typically represented with <code>[UNK]</code> or some other similar variant.</p> In\u00a0[12]: Copied! <pre>phrase = '\ud83e\udd71 the dog next door kept barking all night!!'\nids = my_tokenizer.encode(phrase)\nprint(phrase)\nprint(my_tokenizer.convert_ids_to_tokens(ids))\nprint(my_tokenizer.decode(ids))\n</pre> phrase = '\ud83e\udd71 the dog next door kept barking all night!!' ids = my_tokenizer.encode(phrase) print(phrase) print(my_tokenizer.convert_ids_to_tokens(ids)) print(my_tokenizer.decode(ids)) <pre>\ud83e\udd71 the dog next door kept barking all night!!\n['[CLS]', '[UNK]', 'the', 'dog', 'next', 'door', 'kept', 'barking', 'all', 'night', '!', '!', '[SEP]']\n[CLS] [UNK] the dog next door kept barking all night!! [SEP]\n</pre> In\u00a0[13]: Copied! <pre>phrase = '''wow my dad thought mcdonalds sold tacos \\N{SKULL}'''\nids = my_tokenizer.encode(phrase)\nprint(phrase)\nprint(my_tokenizer.convert_ids_to_tokens(ids))\nprint(my_tokenizer.decode(ids))\n</pre> phrase = '''wow my dad thought mcdonalds sold tacos \\N{SKULL}''' ids = my_tokenizer.encode(phrase) print(phrase) print(my_tokenizer.convert_ids_to_tokens(ids)) print(my_tokenizer.decode(ids)) <pre>wow my dad thought mcdonalds sold tacos \ud83d\udc80\n['[CLS]', 'w', '##ow', 'my', 'dad', 'thought', 'm', '##c', '##don', '##ald', '##s', 'sold', 'ta', '##cos', '[UNK]', '[SEP]']\n[CLS] wow my dad thought mcdonalds sold tacos [UNK] [SEP]\n</pre> <p>We'll load a couple different models:</p> <ul> <li><code>bert-base-cased</code> (doc)</li> <li><code>xlm-roberta-base</code> (doc)</li> <li><code>google/pegasus-xsum</code> (doc)</li> <li><code>allenai/longformer-base-4096</code> (doc)</li> </ul> In\u00a0[14]: Copied! <pre>model_names = (\n    \"bert-base-cased\",\n    \"xlm-roberta-base\",\n    \"google/pegasus-xsum\",\n    \"allenai/longformer-base-4096\",\n)\n\nmodel_tokenizers = {\n    model_name: AutoTokenizer.from_pretrained(model_name) for model_name in model_names\n}\n</pre> model_names = (     \"bert-base-cased\",     \"xlm-roberta-base\",     \"google/pegasus-xsum\",     \"allenai/longformer-base-4096\", )  model_tokenizers = {     model_name: AutoTokenizer.from_pretrained(model_name) for model_name in model_names } <pre>huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 615/615 [00:00&lt;00:00, 1.03MB/s]\nsentencepiece.bpe.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.07M/5.07M [00:01&lt;00:00, 3.49MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9.10M/9.10M [00:00&lt;00:00, 14.6MB/s]\ntokenizer_config.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 87.0/87.0 [00:00&lt;00:00, 176kB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.39k/1.39k [00:00&lt;00:00, 4.83MB/s]\nspiece.model: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.91M/1.91M [00:00&lt;00:00, 5.35MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3.52M/3.52M [00:00&lt;00:00, 26.7MB/s]\nspecial_tokens_map.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65.0/65.0 [00:00&lt;00:00, 82.0kB/s]\nconfig.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 694/694 [00:00&lt;00:00, 1.88MB/s]\nvocab.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 899k/899k [00:00&lt;00:00, 2.36MB/s]\nmerges.txt: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 456k/456k [00:00&lt;00:00, 231MB/s]\ntokenizer.json: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1.36M/1.36M [00:00&lt;00:00, 49.6MB/s]\n</pre> In\u00a0[15]: Copied! <pre>for model_name, temp_tokenizer in model_tokenizers.items():\n    max_length = temp_tokenizer.model_max_length\n    print(f\"{model_name}\\n\\tmax length: {max_length}\")\n    print(\"\\n\")\n</pre> for model_name, temp_tokenizer in model_tokenizers.items():     max_length = temp_tokenizer.model_max_length     print(f\"{model_name}\\n\\tmax length: {max_length}\")     print(\"\\n\") <pre>bert-base-cased\n\tmax length: 512\n\n\nxlm-roberta-base\n\tmax length: 512\n\n\ngoogle/pegasus-xsum\n\tmax length: 512\n\n\nallenai/longformer-base-4096\n\tmax length: 4096\n\n\n</pre> <p>We've already mentioned special tokens like the \"unknown\" token. Different models use different ways to distinguish special tokens and not all models cover all the special tokens since it's dependent on the model's task it was trained for.</p> In\u00a0[16]: Copied! <pre>for model_name, temp_tokenizer in model_tokenizers.items():\n    special_tokens = temp_tokenizer.all_special_tokens\n    print(f\"{model_name}\\n\\tspecial tokens: {special_tokens}\")\n    print(\"\\n\")\n</pre> for model_name, temp_tokenizer in model_tokenizers.items():     special_tokens = temp_tokenizer.all_special_tokens     print(f\"{model_name}\\n\\tspecial tokens: {special_tokens}\")     print(\"\\n\") <pre>bert-base-cased\n\tspecial tokens: ['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]']\n\n\nxlm-roberta-base\n\tspecial tokens: ['&lt;s&gt;', '&lt;/s&gt;', '&lt;unk&gt;', '&lt;pad&gt;', '&lt;mask&gt;']\n\n\ngoogle/pegasus-xsum\n\tspecial tokens: ['&lt;/s&gt;', '&lt;unk&gt;', '&lt;pad&gt;', '&lt;mask_2&gt;', '&lt;mask_1&gt;', '&lt;unk_2&gt;', '&lt;unk_3&gt;', '&lt;unk_4&gt;', '&lt;unk_5&gt;', '&lt;unk_6&gt;', '&lt;unk_7&gt;', '&lt;unk_8&gt;', '&lt;unk_9&gt;', '&lt;unk_10&gt;', '&lt;unk_11&gt;', '&lt;unk_12&gt;', '&lt;unk_13&gt;', '&lt;unk_14&gt;', '&lt;unk_15&gt;', '&lt;unk_16&gt;', '&lt;unk_17&gt;', '&lt;unk_18&gt;', '&lt;unk_19&gt;', '&lt;unk_20&gt;', '&lt;unk_21&gt;', '&lt;unk_22&gt;', '&lt;unk_23&gt;', '&lt;unk_24&gt;', '&lt;unk_25&gt;', '&lt;unk_26&gt;', '&lt;unk_27&gt;', '&lt;unk_28&gt;', '&lt;unk_29&gt;', '&lt;unk_30&gt;', '&lt;unk_31&gt;', '&lt;unk_32&gt;', '&lt;unk_33&gt;', '&lt;unk_34&gt;', '&lt;unk_35&gt;', '&lt;unk_36&gt;', '&lt;unk_37&gt;', '&lt;unk_38&gt;', '&lt;unk_39&gt;', '&lt;unk_40&gt;', '&lt;unk_41&gt;', '&lt;unk_42&gt;', '&lt;unk_43&gt;', '&lt;unk_44&gt;', '&lt;unk_45&gt;', '&lt;unk_46&gt;', '&lt;unk_47&gt;', '&lt;unk_48&gt;', '&lt;unk_49&gt;', '&lt;unk_50&gt;', '&lt;unk_51&gt;', '&lt;unk_52&gt;', '&lt;unk_53&gt;', '&lt;unk_54&gt;', '&lt;unk_55&gt;', '&lt;unk_56&gt;', '&lt;unk_57&gt;', '&lt;unk_58&gt;', '&lt;unk_59&gt;', '&lt;unk_60&gt;', '&lt;unk_61&gt;', '&lt;unk_62&gt;', '&lt;unk_63&gt;', '&lt;unk_64&gt;', '&lt;unk_65&gt;', '&lt;unk_66&gt;', '&lt;unk_67&gt;', '&lt;unk_68&gt;', '&lt;unk_69&gt;', '&lt;unk_70&gt;', '&lt;unk_71&gt;', '&lt;unk_72&gt;', '&lt;unk_73&gt;', '&lt;unk_74&gt;', '&lt;unk_75&gt;', '&lt;unk_76&gt;', '&lt;unk_77&gt;', '&lt;unk_78&gt;', '&lt;unk_79&gt;', '&lt;unk_80&gt;', '&lt;unk_81&gt;', '&lt;unk_82&gt;', '&lt;unk_83&gt;', '&lt;unk_84&gt;', '&lt;unk_85&gt;', '&lt;unk_86&gt;', '&lt;unk_87&gt;', '&lt;unk_88&gt;', '&lt;unk_89&gt;', '&lt;unk_90&gt;', '&lt;unk_91&gt;', '&lt;unk_92&gt;', '&lt;unk_93&gt;', '&lt;unk_94&gt;', '&lt;unk_95&gt;', '&lt;unk_96&gt;', '&lt;unk_97&gt;', '&lt;unk_98&gt;', '&lt;unk_99&gt;', '&lt;unk_100&gt;', '&lt;unk_101&gt;', '&lt;unk_102&gt;']\n\n\nallenai/longformer-base-4096\n\tspecial tokens: ['&lt;s&gt;', '&lt;/s&gt;', '&lt;unk&gt;', '&lt;pad&gt;', '&lt;mask&gt;']\n\n\n</pre> <p>You can also call the specific token you're interested in to see its representation.\u00e7</p> In\u00a0[17]: Copied! <pre>model_tokenizers[\"bert-base-cased\"].unk_token\n</pre> model_tokenizers[\"bert-base-cased\"].unk_token Out[17]: <pre>'[UNK]'</pre> In\u00a0[18]: Copied! <pre>for model_name, temp_tokenizer in model_tokenizers.items():\n    print(f\"{model_name}\")\n    print(f\"\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}\")\n    print(f\"\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}\")\n    print(f\"\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}\")\n    print(f\"\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}\")\n    print(f\"\\tSentence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}\")\n    print(f\"\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}\")\n    print(\"\\n\")\n</pre> for model_name, temp_tokenizer in model_tokenizers.items():     print(f\"{model_name}\")     print(f\"\\tUnknown: \\n\\t\\t{temp_tokenizer.unk_token=}\")     print(f\"\\tBeginning of Sequence: \\n\\t\\t{temp_tokenizer.bos_token=}\")     print(f\"\\tEnd of Sequence: \\n\\t\\t{temp_tokenizer.eos_token=}\")     print(f\"\\tMask: \\n\\t\\t{temp_tokenizer.mask_token=}\")     print(f\"\\tSentence Separator: \\n\\t\\t{temp_tokenizer.sep_token=}\")     print(f\"\\tClass of Input: \\n\\t\\t{temp_tokenizer.cls_token=}\")     print(\"\\n\") <pre>bert-base-cased\n\tUnknown: \n\t\ttemp_tokenizer.unk_token='[UNK]'\n\tBeginning of Sequence: \n\t\ttemp_tokenizer.bos_token=None\n\tEnd of Sequence: \n\t\ttemp_tokenizer.eos_token=None\n\tMask: \n\t\ttemp_tokenizer.mask_token='[MASK]'\n\tSentence Separator: \n\t\ttemp_tokenizer.sep_token='[SEP]'\n\tClass of Input: \n\t\ttemp_tokenizer.cls_token='[CLS]'\n\n\nxlm-roberta-base\n\tUnknown: \n\t\ttemp_tokenizer.unk_token='&lt;unk&gt;'\n\tBeginning of Sequence: \n\t\ttemp_tokenizer.bos_token='&lt;s&gt;'\n\tEnd of Sequence: \n\t\ttemp_tokenizer.eos_token='&lt;/s&gt;'\n\tMask: \n\t\ttemp_tokenizer.mask_token='&lt;mask&gt;'\n\tSentence Separator: \n\t\ttemp_tokenizer.sep_token='&lt;/s&gt;'\n\tClass of Input: \n\t\ttemp_tokenizer.cls_token='&lt;s&gt;'\n\n\ngoogle/pegasus-xsum\n\tUnknown: \n\t\ttemp_tokenizer.unk_token='&lt;unk&gt;'\n\tBeginning of Sequence: \n\t\ttemp_tokenizer.bos_token=None\n\tEnd of Sequence: \n\t\ttemp_tokenizer.eos_token='&lt;/s&gt;'\n\tMask: \n\t\ttemp_tokenizer.mask_token='&lt;mask_2&gt;'\n\tSentence Separator: \n\t\ttemp_tokenizer.sep_token=None\n\tClass of Input: \n\t\ttemp_tokenizer.cls_token=None\n\n\nallenai/longformer-base-4096\n\tUnknown: \n\t\ttemp_tokenizer.unk_token='&lt;unk&gt;'\n\tBeginning of Sequence: \n\t\ttemp_tokenizer.bos_token='&lt;s&gt;'\n\tEnd of Sequence: \n\t\ttemp_tokenizer.eos_token='&lt;/s&gt;'\n\tMask: \n\t\ttemp_tokenizer.mask_token='&lt;mask&gt;'\n\tSentence Separator: \n\t\ttemp_tokenizer.sep_token='&lt;/s&gt;'\n\tClass of Input: \n\t\ttemp_tokenizer.cls_token='&lt;s&gt;'\n\n\n</pre> <p>Different tokenizers will have different special tokens defined. They might have tokens representing:</p> <ul> <li>Unknown token</li> <li>Beginning of sequence token</li> <li>Separator token</li> <li>Token used for padding</li> <li>Classifier token</li> <li>Token used for masking values</li> </ul> <p>Additionally, there may be multiple subtypes of each special token. For example, some tokenizers have multiple different unknown tokens (e.g. <code>&lt;unk&gt;</code> and <code>&lt;unk_2&gt;</code>).</p> <p>Different tokenizers can create very different tokens for the same piece of text. When choosing a tokenizer, consider what properties are important to you, such as the maximum length and the special tokens.</p> <p>If none of the available tokenizers perform the way you need them to, you can also fine-tune a tokenizer to adjust it for your use case.</p> <ul> <li>Hugging Face's PreTrainedTokenizer</li> <li>Hugging Face's AutoTokenizer</li> </ul> <p>Documentation on some available models:</p> <ul> <li>bert-base-cased</li> <li>xlm-roberta-base</li> <li>google/pegasus-xsum</li> <li>allenai/longformer-base-4096</li> </ul>"}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#using-hugging-face-tokenizers", "title": "Using Hugging Face Tokenizers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#loading-tokenizer", "title": "Loading Tokenizer\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#encoding-text-to-tokens", "title": "Encoding: Text to Tokens\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#tokens-string-representations", "title": "Tokens: String Representations\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#tokens-integer-id-representations", "title": "Tokens: Integer ID Representations\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#decoding-tokens-to-text", "title": "Decoding: Tokens to Text\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#a-note-on-the-unknown", "title": "A Note on the Unknown\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#more-properties-of-hugging-faces-tokenizers", "title": "More Properties of Hugging Face's Tokenizers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#model_max_length", "title": "<code>model_max_length</code>\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#special-tokens", "title": "Special Tokens\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#hugging-face-tokenizers-takeaways", "title": "Hugging Face Tokenizers Takeaways\u00b6", "text": ""}, {"location": "generative_ai/tutorials/hugging-face-tokenizer/#documentation-on-hugging-face-tokenizers-and-models", "title": "Documentation on Hugging Face Tokenizers and Models\u00b6", "text": ""}, {"location": "generative_ai/tutorials/lance_db/", "title": "Vector Database Basics", "text": "In\u00a0[\u00a0]: Copied! <pre># pip install -U --quiet lancedb pandas pydantic\n</pre> # pip install -U --quiet lancedb pandas pydantic <p>We're going to use pydantic to make this easier. First let's create a pydantic model with those fields</p> In\u00a0[\u00a0]: Copied! <pre>from lancedb.pydantic import vector, LanceModel\n\nclass CatsAndDogs(LanceModel):\n    vector: vector(2)\n    species: str\n    breed: str\n    weight: float\n</pre> from lancedb.pydantic import vector, LanceModel  class CatsAndDogs(LanceModel):     vector: vector(2)     species: str     breed: str     weight: float <p>Now connect to a local db at ~/.lancedb and create an empty LanceDB table called \"cats_and_dogs\"</p> In\u00a0[7]: Copied! <pre>import lancedb\n\nuri = \"data/.lancedb/\"\ndb = lancedb.connect(uri)\n</pre> import lancedb  uri = \"data/.lancedb/\" db = lancedb.connect(uri) In\u00a0[8]: Copied! <pre>tbl = db.create_table(\"cats_and_dogs\", schema=CatsAndDogs, exist_ok=True)\n</pre> tbl = db.create_table(\"cats_and_dogs\", schema=CatsAndDogs, exist_ok=True) <p>Let's add some data</p> <p>First some cats</p> In\u00a0[\u00a0]: Copied! <pre>data = [\n    CatsAndDogs(\n        vector=[1., 0.],\n        species=\"cat\",\n        breed=\"shorthair\",\n        weight=12.,\n    ),\n    CatsAndDogs(\n        vector=[-1., 0.],\n        species=\"cat\",\n        breed=\"himalayan\",\n        weight=9.5,\n    ),\n]\n</pre> data = [     CatsAndDogs(         vector=[1., 0.],         species=\"cat\",         breed=\"shorthair\",         weight=12.,     ),     CatsAndDogs(         vector=[-1., 0.],         species=\"cat\",         breed=\"himalayan\",         weight=9.5,     ), ] <p>Now call the <code>LanceTable.add</code> API to insert these two records into the table</p> In\u00a0[\u00a0]: Copied! <pre>tbl.add(data)\n</pre> tbl.add(data) <p>Let's preview the data</p> In\u00a0[9]: Copied! <pre>tbl.head().to_pandas()\n</pre> tbl.head().to_pandas() Out[9]: vector species breed weight 0 [1.0, 0.0] cat shorthair 12.0 1 [-1.0, 0.0] cat himalayan 9.5 2 [0.0, 10.0] dog samoyed 47.5 3 [0.0, -1.0] dog corgi 26.0 <p>Now let's add some dogs</p> In\u00a0[\u00a0]: Copied! <pre>data = [\n    CatsAndDogs(\n        vector=[0., 10.],\n        species=\"dog\",\n        breed=\"samoyed\",\n        weight=47.5,\n    ),\n    CatsAndDogs(\n        vector=[0, -1.],\n        species=\"dog\",\n        breed=\"corgi\",\n        weight=26.,\n    )\n]\n</pre> data = [     CatsAndDogs(         vector=[0., 10.],         species=\"dog\",         breed=\"samoyed\",         weight=47.5,     ),     CatsAndDogs(         vector=[0, -1.],         species=\"dog\",         breed=\"corgi\",         weight=26.,     ) ] In\u00a0[\u00a0]: Copied! <pre>tbl.add(data)\n</pre> tbl.add(data) In\u00a0[10]: Copied! <pre>tbl.head().to_pandas()\n</pre> tbl.head().to_pandas() Out[10]: vector species breed weight 0 [1.0, 0.0] cat shorthair 12.0 1 [-1.0, 0.0] cat himalayan 9.5 2 [0.0, 10.0] dog samoyed 47.5 3 [0.0, -1.0] dog corgi 26.0 <p>Let's say we have a new animal that has embedding [10.5, 10.], what would you expect the most similar animal will be? Can you use the table we created above to answer the question?</p> <p>HINT you'll need to use the <code>search</code> API for LanceTable and <code>limit</code> / <code>to_df</code> APIs. For examples you can refer to LanceDB documentation.</p> In\u00a0[11]: Copied! <pre>tbl.search([10.5, 10.]).limit(1).to_pandas()\n</pre> tbl.search([10.5, 10.]).limit(1).to_pandas() Out[11]: vector species breed weight _distance 0 [0.0, 10.0] dog samoyed 47.5 110.25 <p>Now what if we use cosine distance instead? Would you expect that we get the same answer? Why or why not?</p> <p>HINT you can add a call to <code>metric</code> in the call chain</p> In\u00a0[12]: Copied! <pre>tbl.search([10.5, 10.0]).metric(\"cosine\").limit(1).to_pandas()\n</pre> tbl.search([10.5, 10.0]).metric(\"cosine\").limit(1).to_pandas() Out[12]: vector species breed weight _distance 0 [1.0, 0.0] cat shorthair 12.0 0.275862 <p>Please write code to retrieve two most similar examples to the embedding [10.5, 10.] but only show the results that is a cat.</p> In\u00a0[13]: Copied! <pre>tbl.search([10.5, 10.0]).where(\"species = 'cat'\", prefilter=True).limit(2).to_pandas()\n</pre> tbl.search([10.5, 10.0]).where(\"species = 'cat'\", prefilter=True).limit(2).to_pandas() Out[13]: vector species breed weight _distance 0 [1.0, 0.0] cat shorthair 12.0 190.25 1 [-1.0, 0.0] cat himalayan 9.5 232.25 In\u00a0[14]: Copied! <pre>from lance.vector import vec_to_table\nimport numpy as np\n\nmat = np.random.randn(100_000, 16)\ntable_name = \"exercise3_ann\"\ndb.drop_table(table_name, ignore_missing=True)\ntable = db.create_table(table_name, vec_to_table(mat))\n</pre> from lance.vector import vec_to_table import numpy as np  mat = np.random.randn(100_000, 16) table_name = \"exercise3_ann\" db.drop_table(table_name, ignore_missing=True) table = db.create_table(table_name, vec_to_table(mat)) In\u00a0[15]: Copied! <pre>query = np.random.randn(16)\ntable.search(query).limit(10).to_df()\n</pre> query = np.random.randn(16) table.search(query).limit(10).to_df() <pre>/var/folders/gb/wf3d1d_d2bz689q8kdbw02880000gn/T/ipykernel_1489/229240492.py:2: UnsupportedWarning: to_df is unsupported as of 0.4.0. Use to_pandas() instead\n  table.search(query).limit(10).to_df()\n</pre> Out[15]: vector _distance 0 [-1.2724329, 0.084512636, 0.28315237, 1.621330... 3.393116 1 [-1.0316224, -0.23577532, 0.39665398, 1.171307... 3.831863 2 [-0.49312374, -0.50828266, 0.046579137, 0.1497... 3.904232 3 [-1.0809894, -0.5009589, 0.6601867, 0.07009579... 4.459040 4 [-0.7098237, 0.50592774, 0.5868473, 1.3845882,... 4.534908 5 [-0.75004256, -0.7544854, 0.26175323, 1.517701... 4.555618 6 [-0.74581164, 0.6364289, 0.6266087, 0.8722173,... 4.572597 7 [-0.8690996, -0.51964694, 0.9082809, 0.2187988... 4.628890 8 [-1.6587613, 0.36453688, 0.14851123, -0.566129... 4.668116 9 [-1.5693077, -0.026916353, -0.25843197, 0.4409... 4.731431 <p>Please write code to compute the average latency of this query</p> In\u00a0[16]: Copied! <pre>import functools\nimport time\n\n\ndef timer(func):\n\"\"\"Print the runtime of the decorated function\"\"\"\n\n    @functools.wraps(func)\n    def wrapper_timer(*args, **kwargs):\n        start_time = time.perf_counter()\n        value = func(*args, **kwargs)\n        end_time = time.perf_counter()\n        run_time = end_time - start_time\n        print(f\"Finished {func.__name__}() in {run_time:.4f} secs\")\n        return value\n\n    return wrapper_timer\n\n\n@timer\ndef get_top_n(table, query, n=10):\n    return table.search(query).limit(n).to_df()\n</pre> import functools import time   def timer(func):     \"\"\"Print the runtime of the decorated function\"\"\"      @functools.wraps(func)     def wrapper_timer(*args, **kwargs):         start_time = time.perf_counter()         value = func(*args, **kwargs)         end_time = time.perf_counter()         run_time = end_time - start_time         print(f\"Finished {func.__name__}() in {run_time:.4f} secs\")         return value      return wrapper_timer   @timer def get_top_n(table, query, n=10):     return table.search(query).limit(n).to_df() In\u00a0[17]: Copied! <pre>get_top_n(table, query)\n</pre> get_top_n(table, query) <pre>Finished get_top_n() in 0.0149 secs\n</pre> <pre>/var/folders/gb/wf3d1d_d2bz689q8kdbw02880000gn/T/ipykernel_1489/3894619790.py:22: UnsupportedWarning: to_df is unsupported as of 0.4.0. Use to_pandas() instead\n  return table.search(query).limit(n).to_df()\n</pre> Out[17]: vector _distance 0 [-1.2724329, 0.084512636, 0.28315237, 1.621330... 3.393116 1 [-1.0316224, -0.23577532, 0.39665398, 1.171307... 3.831863 2 [-0.49312374, -0.50828266, 0.046579137, 0.1497... 3.904232 3 [-1.0809894, -0.5009589, 0.6601867, 0.07009579... 4.459040 4 [-0.7098237, 0.50592774, 0.5868473, 1.3845882,... 4.534908 5 [-0.75004256, -0.7544854, 0.26175323, 1.517701... 4.555618 6 [-0.74581164, 0.6364289, 0.6266087, 0.8722173,... 4.572597 7 [-0.8690996, -0.51964694, 0.9082809, 0.2187988... 4.628890 8 [-1.6587613, 0.36453688, 0.14851123, -0.566129... 4.668116 9 [-1.5693077, -0.026916353, -0.25843197, 0.4409... 4.731431 In\u00a0[18]: Copied! <pre>table.create_index(metric=\"L2\", num_partitions=4000, num_sub_vectors=8)\n</pre> table.create_index(metric=\"L2\", num_partitions=4000, num_sub_vectors=8) <p>Now let's search through the data again. Notice how the answers now appear different. This is because an ANN index is always a tradeoff between latency and accuracy.</p> In\u00a0[19]: Copied! <pre>table.search(query).limit(10).to_df()\n</pre> table.search(query).limit(10).to_df() <pre>/var/folders/gb/wf3d1d_d2bz689q8kdbw02880000gn/T/ipykernel_1489/4174343316.py:1: UnsupportedWarning: to_df is unsupported as of 0.4.0. Use to_pandas() instead\n  table.search(query).limit(10).to_df()\n</pre> Out[19]: vector _distance 0 [-0.74581164, 0.6364289, 0.6266087, 0.8722173,... 4.730085 1 [-1.0502441, 0.2866477, 0.89400625, 0.0577777,... 4.771339 2 [-1.2426008, 0.29611367, 0.50209624, 0.1570571... 4.774913 3 [-1.5693077, -0.026916353, -0.25843197, 0.4409... 4.787258 4 [-0.8690996, -0.51964694, 0.9082809, 0.2187988... 4.876680 5 [-0.8164513, 0.071738884, 0.37751395, 1.258959... 4.885183 6 [-1.6587613, 0.36453688, 0.14851123, -0.566129... 4.906681 7 [-0.50475115, -0.763558, -0.5470999, 0.0168607... 4.952706 8 [-0.85710555, 0.97174263, 0.027284576, 0.84639... 5.101480 9 [-1.3415798, 0.61720556, -0.44235563, 0.281207... 5.250690 <p>Now write code to compute the average latency for querying the same table using the ANN index.</p> <p>SOLUTION The index is implementation detail, so it should just be running the same code as above. You should see almost an order of magnitude speed-up. On larger datasets, this performance difference should be even more pronounced.</p> In\u00a0[20]: Copied! <pre>get_top_n(table, query)\n</pre> get_top_n(table, query) <pre>Finished get_top_n() in 0.0106 secs\n</pre> <pre>/var/folders/gb/wf3d1d_d2bz689q8kdbw02880000gn/T/ipykernel_1489/3894619790.py:22: UnsupportedWarning: to_df is unsupported as of 0.4.0. Use to_pandas() instead\n  return table.search(query).limit(n).to_df()\n</pre> Out[20]: vector _distance 0 [-0.74581164, 0.6364289, 0.6266087, 0.8722173,... 4.730085 1 [-1.0502441, 0.2866477, 0.89400625, 0.0577777,... 4.771339 2 [-1.2426008, 0.29611367, 0.50209624, 0.1570571... 4.774913 3 [-1.5693077, -0.026916353, -0.25843197, 0.4409... 4.787258 4 [-0.8690996, -0.51964694, 0.9082809, 0.2187988... 4.876680 5 [-0.8164513, 0.071738884, 0.37751395, 1.258959... 4.885183 6 [-1.6587613, 0.36453688, 0.14851123, -0.566129... 4.906681 7 [-0.50475115, -0.763558, -0.5470999, 0.0168607... 4.952706 8 [-0.85710555, 0.97174263, 0.027284576, 0.84639... 5.101480 9 [-1.3415798, 0.61720556, -0.44235563, 0.281207... 5.250690 In\u00a0[21]: Copied! <pre>table = db[\"cats_and_dogs\"]\n</pre> table = db[\"cats_and_dogs\"] In\u00a0[22]: Copied! <pre>len(table)\n</pre> len(table) Out[22]: <pre>4</pre> <p>Can you use the <code>delete</code> API to remove all of the cats from the table?</p> <p>HINT use a SQL like filter string to specify which rows to delete from the table</p> In\u00a0[23]: Copied! <pre>table.delete(\"species = 'cat'\")\n</pre> table.delete(\"species = 'cat'\") In\u00a0[24]: Copied! <pre>len(table)\n</pre> len(table) Out[24]: <pre>2</pre> <p>So far we've accumulated 4 actions on the table:</p> <ol> <li>creation of the table</li> <li>added cats</li> <li>added dogs</li> <li>deleted cats</li> </ol> <p>What if you realized that you should have deleted the dogs instead of the cats?</p> <p>Here we can see the 4 versions that correspond to the 4 actions we've done</p> In\u00a0[25]: Copied! <pre>table.list_versions()\n</pre> table.list_versions() Out[25]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 13, 40, 146383),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 14, 58, 727480),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 15, 48, 798191),\n  'metadata': {}},\n {'version': 4,\n  'timestamp': datetime.datetime(2024, 3, 1, 11, 0, 33, 856836),\n  'metadata': {}}]</pre> <p>Please write code to restore the version still containing the whole dataset</p> In\u00a0[26]: Copied! <pre>table = db[\"cats_and_dogs\"]\n</pre> table = db[\"cats_and_dogs\"] In\u00a0[27]: Copied! <pre>len(table)\n</pre> len(table) Out[27]: <pre>2</pre> In\u00a0[28]: Copied! <pre># restore to version 3\ntable.restore(3)\n</pre> # restore to version 3 table.restore(3) In\u00a0[29]: Copied! <pre># delete the dogs instead\ntable.delete(\"species = 'dog'\")\n</pre> # delete the dogs instead table.delete(\"species = 'dog'\") In\u00a0[30]: Copied! <pre>table.list_versions()\n</pre> table.list_versions() Out[30]: <pre>[{'version': 1,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 13, 40, 146383),\n  'metadata': {}},\n {'version': 2,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 14, 58, 727480),\n  'metadata': {}},\n {'version': 3,\n  'timestamp': datetime.datetime(2024, 3, 1, 10, 15, 48, 798191),\n  'metadata': {}},\n {'version': 4,\n  'timestamp': datetime.datetime(2024, 3, 1, 11, 0, 33, 856836),\n  'metadata': {}},\n {'version': 5,\n  'timestamp': datetime.datetime(2024, 3, 1, 11, 3, 7, 390330),\n  'metadata': {}},\n {'version': 6,\n  'timestamp': datetime.datetime(2024, 3, 1, 11, 3, 23, 457686),\n  'metadata': {}}]</pre> In\u00a0[31]: Copied! <pre>table.to_pandas()\n</pre> table.to_pandas() Out[31]: vector species breed weight 0 [1.0, 0.0] cat shorthair 12.0 1 [-1.0, 0.0] cat himalayan 9.5 In\u00a0[32]: Copied! <pre>\"cats_and_dogs\" in db\n</pre> \"cats_and_dogs\" in db Out[32]: <pre>True</pre> <p>Write code to irrevocably remove the table \"cats_and_dogs\" from the database</p> In\u00a0[33]: Copied! <pre>db.drop_table(\"cats_and_dogs\")\n</pre> db.drop_table(\"cats_and_dogs\") <p>How would you verify that the table has indeed been deleted?</p> In\u00a0[34]: Copied! <pre>table.name in db\n</pre> table.name in db Out[34]: <pre>False</pre>"}, {"location": "generative_ai/tutorials/lance_db/#vector-database-basics", "title": "Vector Database Basics\u00b6", "text": "<p>Vector databases help us store, manage, and query the embeddings we created for generative AI, recommenders, and search engines.</p> <p>Across many of the common use cases, users often find that they need to manage more than just vectors. To make it easier for practitioners, vector databases should store and manage all of the data they need:</p> <ul> <li>embedding vectors</li> <li>categorical metadata</li> <li>numerical metadata</li> <li>timeseries metadata</li> <li>text / pdf / images / video / point clouds</li> </ul> <p>And support a wide range of query workloads:</p> <ul> <li>Vector search (may require ANN-index)</li> <li>Keyword search (requires full text search index)</li> <li>SQL (for filtering)</li> </ul> <p>For this exercise we'll use LanceDB since it's open source and easy to setup</p>"}, {"location": "generative_ai/tutorials/lance_db/#creating-tables-and-adding-data", "title": "Creating tables and adding data\u00b6", "text": "<p>Let's create a LanceDB table called <code>cats_and_dogs</code> under the local database directory <code>~/.lancedb</code>. This table should have 4 fields:</p> <ul> <li>the embedding vector</li> <li>a string field indicating the species (either \"cat\" or \"dog\")</li> <li>the breed</li> <li>average weight in pounds</li> </ul>"}, {"location": "generative_ai/tutorials/lance_db/#querying-tables", "title": "Querying tables\u00b6", "text": "<p>Vector databases allow us to retrieve data for generative AI applications. Let's see how that's done.</p>"}, {"location": "generative_ai/tutorials/lance_db/#filtering-tables", "title": "Filtering tables\u00b6", "text": "<p>In practice, we often need to specify more than just a search vector for good quality retrieval. Oftentimes we need to filter the metadata as well.</p>"}, {"location": "generative_ai/tutorials/lance_db/#creating-ann-indices", "title": "Creating ANN indices\u00b6", "text": "<p>For larger tables (e.g., &gt;1M rows), searching through all of the vectors becomes quite slow. Here is where the Approximate Nearest Neighbor (ANN) index comes into play. While there are many different ANN indexing algorithms, they all have the same purpose - to drastically limit the search space as much as possible while losing as little accuracy as possible</p> <p>For this problem we will create an ANN index on a LanceDB table and see how that impacts performance</p>"}, {"location": "generative_ai/tutorials/lance_db/#first-lets-create-some-data", "title": "First let's create some data\u00b6", "text": "<p>Given the constraints of the classroom workspace, we'll complete this exercise by creating 100,000 vectors with 16D in a new table. Here the embedding values don't matter, so we simply generate random embeddings as a 2D numpy array. We then use the vec_to_table function to convert that in to an Arrow table, which can then be added to the table.</p>"}, {"location": "generative_ai/tutorials/lance_db/#lets-establish-a-baseline-without-an-index", "title": "Let's establish a baseline without an index\u00b6", "text": "<p>Before we create the index, let's make sure know what we need to compare against.</p> <p>We'll generate a random query vector and record it's value in the <code>query</code> variable so we can use the same query vector with and without the ANN index.</p>"}, {"location": "generative_ai/tutorials/lance_db/#now-lets-create-an-index", "title": "Now let's create an index\u00b6", "text": "<p>There are many possible index types ranging from hash based to tree based to partition based to graph based. For this task, we'll create an IVFPQ index (partition-based index with product quantization compression) using LanceDB.</p> <p>Please create an IVFPQ index on the LanceDB table such that each partition is 4000 rows and each PQ subvector is 8D.</p> <p>HINT</p> <ol> <li>Total vectors / number of partitions = number of vectors in each partition</li> <li>Total dimensions / number of subvectors = number of dimensions in each subvector</li> <li>This step can take about 7-10 minutes to process and execute in the classroom workspace.</li> </ol>"}, {"location": "generative_ai/tutorials/lance_db/#deleting-rows", "title": "Deleting rows\u00b6", "text": "<p>Like with other kinds of databases, you should be able to remove rows from the table. Let's go back to our tables of cats and dogs</p>"}, {"location": "generative_ai/tutorials/lance_db/#what-if-i-messed-up", "title": "What if I messed up?\u00b6", "text": "<p>Errors is a common occurrence in AI. What's hard about errors in vector search is that oftentimes a bad vector doesn't cause a crash but just creates non-sensical answers. So to be able to rollback the state of the database is very important for debugging and reproducibility</p>"}, {"location": "generative_ai/tutorials/lance_db/#dropping-a-table", "title": "Dropping a table\u00b6", "text": "<p>You can also choose to drop a table, which also completely removes the data. Note that this operation is not reversible.</p>"}, {"location": "generative_ai/tutorials/lance_db/#summary", "title": "Summary\u00b6", "text": "<p>Congrats, in this exercise you've learned the basic operations of vector databases from creating tables, to adding data, and to querying the data. You've learned how to create indices and you saw first hand how it changes the performance and the accuracy. Lastly, you've learned how to debug and rollback when errors happen.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/", "title": "Multimodal Search", "text": "In\u00a0[1]: Copied! <pre># pip install --quiet datasets gradio lancedb pandas transformers [This has been preinstalled for you]\n</pre> # pip install --quiet datasets gradio lancedb pandas transformers [This has been preinstalled for you] In\u00a0[1]: Copied! <pre>from transformers import CLIPModel, CLIPProcessor\n\nMODEL_ID = \"openai/clip-vit-base-patch32\"\n\ndevice = \"cpu\"\n\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n</pre> from transformers import CLIPModel, CLIPProcessor  MODEL_ID = \"openai/clip-vit-base-patch32\"  device = \"cpu\"  model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device) processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\") In\u00a0[2]: Copied! <pre>import io\n\nfrom lancedb.pydantic import LanceModel, vector\nimport PIL\n\nclass Image(LanceModel):\n    image: bytes\n    label: int\n    vector: vector(512)\n        \n    def to_pil(self):\n        return PIL.Image.open(io.BytesIO(self.image))\n    \n    @classmethod\n    def pil_to_bytes(cls, img) -&gt; bytes:\n        buf = io.BytesIO()\n        img.save(buf, format=\"PNG\")\n        return buf.getvalue()\n</pre> import io  from lancedb.pydantic import LanceModel, vector import PIL  class Image(LanceModel):     image: bytes     label: int     vector: vector(512)              def to_pil(self):         return PIL.Image.open(io.BytesIO(self.image))          @classmethod     def pil_to_bytes(cls, img) -&gt; bytes:         buf = io.BytesIO()         img.save(buf, format=\"PNG\")         return buf.getvalue() In\u00a0[3]: Copied! <pre>def process_image(batch: dict) -&gt; dict:\n    image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[\n        \"pixel_values\"\n    ].to(device)\n\n    # create the image embedding from the processed image and the model\n    img_emb = model.get_image_features(image)\n\n    batch[\"vector\"] = img_emb.cpu()\n    batch[\"image_bytes\"] = [Image.pil_to_bytes(img) for img in batch[\"image\"]]\n    return batch\n</pre> def process_image(batch: dict) -&gt; dict:     image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[         \"pixel_values\"     ].to(device)      # create the image embedding from the processed image and the model     img_emb = model.get_image_features(image)      batch[\"vector\"] = img_emb.cpu()     batch[\"image_bytes\"] = [Image.pil_to_bytes(img) for img in batch[\"image\"]]     return batch In\u00a0[4]: Copied! <pre>import lancedb\n\nTABLE_NAME = \"image_search\"\n\nuri = \"data/.lancedb/\"\ndb = lancedb.connect(uri)\ntbl = db.create_table(TABLE_NAME, schema=Image, exist_ok=True)\n</pre> import lancedb  TABLE_NAME = \"image_search\"  uri = \"data/.lancedb/\" db = lancedb.connect(uri) tbl = db.create_table(TABLE_NAME, schema=Image, exist_ok=True) In\u00a0[5]: Copied! <pre>from datasets import load_dataset\n\n\ndef datagen() -&gt; list[Image]:\n    dataset = load_dataset(\"zh-plus/tiny-imagenet\", split='valid')\n    batches = dataset.map(process_image, batched=True, batch_size=256)\n\n    # return Image instances\n    return batches[\"image_bytes\"]\n</pre> from datasets import load_dataset   def datagen() -&gt; list[Image]:     dataset = load_dataset(\"zh-plus/tiny-imagenet\", split='valid')     batches = dataset.map(process_image, batched=True, batch_size=256)      # return Image instances     return batches[\"image_bytes\"] <p>Now call the function you just wrote and add the generated instances to the LanceDB table</p> In\u00a0[6]: Copied! <pre>data = datagen()\n</pre> data = datagen() <pre>Parameter 'function'=&lt;function process_image at 0x12550e700&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n</pre> <pre>Map:   0%|          | 0/10000 [00:00&lt;?, ? examples/s]</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[6], line 1\n----&gt; 1 data = datagen()\n\nCell In[5], line 6, in datagen()\n      4 def datagen() -&gt; list[Image]:\n      5     dataset = load_dataset(\"zh-plus/tiny-imagenet\", split='valid')\n----&gt; 6     batches = dataset.map(process_image, batched=True, batch_size=256)\n      8     # return Image instances\n      9     return batches[\"image_bytes\"]\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:593, in transmit_tasks.&lt;locals&gt;.wrapper(*args, **kwargs)\n    591     self: \"Dataset\" = kwargs.pop(\"self\")\n    592 # apply actual function\n--&gt; 593 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    594 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    595 for dataset in datasets:\n    596     # Remove task templates if a column mapping of the template is no longer valid\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:558, in transmit_format.&lt;locals&gt;.wrapper(*args, **kwargs)\n    551 self_format = {\n    552     \"type\": self._format_type,\n    553     \"format_kwargs\": self._format_kwargs,\n    554     \"columns\": self._format_columns,\n    555     \"output_all_columns\": self._output_all_columns,\n    556 }\n    557 # apply actual function\n--&gt; 558 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n    559 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\n    560 # re-apply format to the output\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3105, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\n   3099 if transformed_dataset is None:\n   3100     with hf_tqdm(\n   3101         unit=\" examples\",\n   3102         total=pbar_total,\n   3103         desc=desc or \"Map\",\n   3104     ) as pbar:\n-&gt; 3105         for rank, done, content in Dataset._map_single(**dataset_kwargs):\n   3106             if done:\n   3107                 shards_done += 1\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3482, in Dataset._map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\n   3478 indices = list(\n   3479     range(*(slice(i, i + batch_size).indices(shard.num_rows)))\n   3480 )  # Something simpler?\n   3481 try:\n-&gt; 3482     batch = apply_function_on_filtered_inputs(\n   3483 batch,\n   3484 indices,\n   3485 check_same_num_examples=len(shard.list_indexes()) &gt; 0,\n   3486 offset=offset,\n   3487 )\n   3488 except NumExamplesMismatchError:\n   3489     raise DatasetTransformationNotAllowedError(\n   3490         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"\n   3491     ) from None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py:3361, in Dataset._map_single.&lt;locals&gt;.apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)\n   3359 if with_rank:\n   3360     additional_args += (rank,)\n-&gt; 3361 processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n   3362 if isinstance(processed_inputs, LazyDict):\n   3363     processed_inputs = {\n   3364         k: v for k, v in processed_inputs.data.items() if k not in processed_inputs.keys_to_format\n   3365     }\n\nCell In[3], line 7, in process_image(batch)\n      2 image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[\n      3     \"pixel_values\"\n      4 ].to(device)\n      6 # create the image embedding from the processed image and the model\n----&gt; 7 img_emb = model.get_image_features(image)\n      9 batch[\"vector\"] = img_emb.cpu()\n     10 batch[\"image_bytes\"] = [Image.pil_to_bytes(img) for img in batch[\"image\"]]\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:1054, in CLIPModel.get_image_features(self, pixel_values, output_attentions, output_hidden_states, return_dict)\n   1049 output_hidden_states = (\n   1050     output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n   1051 )\n   1052 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-&gt; 1054 vision_outputs = self.vision_model(\n   1055 pixel_values=pixel_values,\n   1056 output_attentions=output_attentions,\n   1057 output_hidden_states=output_hidden_states,\n   1058 return_dict=return_dict,\n   1059 )\n   1061 pooled_output = vision_outputs[1]  # pooled_output\n   1062 image_features = self.visual_projection(pooled_output)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:852, in CLIPVisionTransformer.forward(self, pixel_values, output_attentions, output_hidden_states, return_dict)\n    849 hidden_states = self.embeddings(pixel_values)\n    850 hidden_states = self.pre_layrnorm(hidden_states)\n--&gt; 852 encoder_outputs = self.encoder(\n    853 inputs_embeds=hidden_states,\n    854 output_attentions=output_attentions,\n    855 output_hidden_states=output_hidden_states,\n    856 return_dict=return_dict,\n    857 )\n    859 last_hidden_state = encoder_outputs[0]\n    860 pooled_output = last_hidden_state[:, 0, :]\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:638, in CLIPEncoder.forward(self, inputs_embeds, attention_mask, causal_attention_mask, output_attentions, output_hidden_states, return_dict)\n    630     layer_outputs = self._gradient_checkpointing_func(\n    631         encoder_layer.__call__,\n    632         hidden_states,\n   (...)\n    635         output_attentions,\n    636     )\n    637 else:\n--&gt; 638     layer_outputs = encoder_layer(\n    639 hidden_states,\n    640 attention_mask,\n    641 causal_attention_mask,\n    642 output_attentions=output_attentions,\n    643 )\n    645 hidden_states = layer_outputs[0]\n    647 if output_attentions:\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:390, in CLIPEncoderLayer.forward(self, hidden_states, attention_mask, causal_attention_mask, output_attentions)\n    388 residual = hidden_states\n    389 hidden_states = self.layer_norm2(hidden_states)\n--&gt; 390 hidden_states = self.mlp(hidden_states)\n    391 hidden_states = residual + hidden_states\n    393 outputs = (hidden_states,)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/transformers/models/clip/modeling_clip.py:347, in CLIPMLP.forward(self, hidden_states)\n    345 hidden_states = self.fc1(hidden_states)\n    346 hidden_states = self.activation_fn(hidden_states)\n--&gt; 347 hidden_states = self.fc2(hidden_states)\n    348 return hidden_states\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1509     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1510 else:\n-&gt; 1511     return self._call_impl(*args, **kwargs)\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520, in Module._call_impl(self, *args, **kwargs)\n   1515 # If we don't have any hooks, we want to skip the rest of the logic in\n   1516 # this function, and just call forward.\n   1517 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1518         or _global_backward_pre_hooks or _global_backward_hooks\n   1519         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1520     return forward_call(*args, **kwargs)\n   1522 try:\n   1523     result = None\n\nFile ~/projects/personal-page/.venv/lib/python3.11/site-packages/torch/nn/modules/linear.py:116, in Linear.forward(self, input)\n    115 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 116     return F.linear(input, self.weight, self.bias)\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre>table = db[TABLE_NAME]\ntable.add(data)\n</pre> table = db[TABLE_NAME] table.add(data) In\u00a0[\u00a0]: Copied! <pre>from transformers import CLIPTokenizerFast\n\nMODEL_ID = \"openai/clip-vit-base-patch32\"\nmodel = &lt;fill me in&gt;\ntokenizer = &lt;fill me in&gt;\n\ndef embed_func(query):\n    inputs = tokenizer([query], padding=True, return_tensors=\"pt\")\n    \n    # generate the text embeddings\n    text_features = &lt;fill me in&gt;\n    \n    return text_features.detach().numpy()[0]\n</pre> from transformers import CLIPTokenizerFast  MODEL_ID = \"openai/clip-vit-base-patch32\" model =  tokenizer =   def embed_func(query):     inputs = tokenizer([query], padding=True, return_tensors=\"pt\")          # generate the text embeddings     text_features =           return text_features.detach().numpy()[0] In\u00a0[\u00a0]: Copied! <pre>def find_images(query):\n    \n    # Generate the embedding for the query\n    emb = &lt;fill me in&gt;    \n    \n    # Search for the closest 9 images\n    rs = &lt;fill me in&gt;\n    \n    # Return PIL instances for visualization\n    return &lt;fill me in&gt;\n</pre> def find_images(query):          # Generate the embedding for the query     emb =               # Search for the closest 9 images     rs =           # Return PIL instances for visualization     return  In\u00a0[\u00a0]: Copied! <pre>find_images(\"fish\")[0]\n</pre> find_images(\"fish\")[0] In\u00a0[\u00a0]: Copied! <pre>import gradio as gr\n\n\nwith gr.Blocks() as demo:\n    with gr.Row():\n        vector_query = gr.Textbox(value=\"fish\", show_label=False)\n        b1 = gr.Button(\"Submit\")\n    with gr.Row():\n        gallery = gr.Gallery(\n                label=\"Found images\", show_label=False, elem_id=\"gallery\"\n            ).style(columns=[3], rows=[3], object_fit=\"contain\", height=\"auto\")   \n        \n    b1.click(find_images, inputs=vector_query, outputs=gallery)\n    \ndemo.launch(server_name=\"0.0.0.0\", inline=False)\n</pre> import gradio as gr   with gr.Blocks() as demo:     with gr.Row():         vector_query = gr.Textbox(value=\"fish\", show_label=False)         b1 = gr.Button(\"Submit\")     with gr.Row():         gallery = gr.Gallery(                 label=\"Found images\", show_label=False, elem_id=\"gallery\"             ).style(columns=[3], rows=[3], object_fit=\"contain\", height=\"auto\")                 b1.click(find_images, inputs=vector_query, outputs=gallery)      demo.launch(server_name=\"0.0.0.0\", inline=False) <p>To view the interface, click on the Links button at the bottom of the workspace window.  Then click on gradio.  This will open a new browser window with the interface.</p> <p>Now try a bunch of different queries and see the results. By default CLIP search results leave a lot of room for improvement. More advanced applications in this space can improve these results in a number ways like retraining the model with your own dataset, your own labels, and using image and text vectors to train the index. The details are however beyond the scope of this lesson.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#multimodal-search", "title": "Multimodal Search\u00b6", "text": "<p>In this final exercise, we will learn how to use vector databases to search through images using natural language.</p> <p>We will be searching through an open source image dataset using an open source model called CLIP. This model is able to encode both images and text into the same embedding space, allowing us to retrieve images that are similar to a user question.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#setup-clip-model", "title": "Setup CLIP model\u00b6", "text": "<p>First, let's prepare the CLIP model to encode the images. We want to setup two things:</p> <ol> <li>a model to encode the image</li> <li>a processor to prepare the image to be encoded</li> </ol> <p>Fill in the code below to initialize a pre-trained model and processor.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#setup-data-model", "title": "Setup data model\u00b6", "text": "<p>The dataset itself has an image field and an integer label. We'll also need an embedding vector (CLIP produces 512D vectors) field.</p> <p>For this problem, please a field named \"vector\" to the Image class below that is a 512D vector.</p> <p>The image that comes out of the raw dataset is a PIL image. So we'll add some conversion code between PIL and bytes to make it easier for serde.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#image-processing-function", "title": "Image processing function\u00b6", "text": "<p>Next we will implement a function to process batches of data from the dataset. We will be using the <code>zh-plus/tiny-imagenet</code> dataset from huggingface datasets. This dataset has an <code>image</code> and a <code>label</code> column.</p> <p>For this problem, please fill in the code to extract the image embeddings from the image using the CLIP model.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#table-creation", "title": "Table creation\u00b6", "text": "<p>Please create a LanceDB table called <code>image_search</code> to store the image, label, and vector.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#adding-data", "title": "Adding data\u00b6", "text": "<p>Now we're ready to process the images and generate embeddings. Please write a function called <code>datagen</code> that calls <code>process_image</code> on each image in the validation set (10K images) and return a list of Image instances.</p> <p>HINT</p> <ol> <li>You may find it faster to use the dataset.map function.</li> <li>You'll want to store the <code>image_bytes</code> field that is returned by <code>process_image</code>.</li> </ol>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#encoding-user-queries", "title": "Encoding user queries\u00b6", "text": "<p>We have image embeddings, but how do we generate the embeddings for the user query? Furthermore, how can we possibly have the same features between the image embeddings and text embeddings. This is where the power of CLIP comes in.</p> <p>Please write a function to turn user query text into an embedding in the same latent space as the images.</p> <p>HINT You can refer to the CLIPModel documention</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#core-search-function", "title": "Core search function\u00b6", "text": "<p>Now let's write the core search function <code>find_images</code>, that takes a text query as input, and returns a list of PIL images that's most similar to the query.</p>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#create-an-app", "title": "Create an App\u00b6", "text": "<p>Let's use gradio to create a small app to search through the images. The code below has been completed for you:</p> <ol> <li>Created a text input where the user can type in a query</li> <li>Created a \"Submit\" button that finds similar images to the input query and display the resulting images</li> <li>A Gallery component that displays the images</li> </ol>"}, {"location": "generative_ai/tutorials/lance_db_multimodal/#summary", "title": "Summary\u00b6", "text": "<p>Congrats!</p> <p>Through this exercise, you learned how to use CLIP to generate image and text embeddings. You've mastered how to use vector databases to enable searching through images using natural language. And you even created a simple app to show off your work.</p> <p>Great job!</p>"}, {"location": "generative_ai/tutorials/langchain_examples/", "title": "Langchain introduction", "text": "In\u00a0[1]: Copied! <pre>from langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.output_parsers import PydanticOutputParser\nfrom pydantic import BaseModel, Field, NonNegativeInt\nfrom typing import List\nfrom random import sample\n</pre> from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.output_parsers import PydanticOutputParser from pydantic import BaseModel, Field, NonNegativeInt from typing import List from random import sample  <p>First, let's create a loader and load reviews from tv-reviews.csv into memory</p> In\u00a0[2]: Copied! <pre># TODO: load reviews from tv-reviews.csv\nfrom langchain.document_loaders.csv_loader import CSVLoader\ndata = CSVLoader(\"./data/tv-reviews.csv\").load()\n</pre> # TODO: load reviews from tv-reviews.csv from langchain.document_loaders.csv_loader import CSVLoader data = CSVLoader(\"./data/tv-reviews.csv\").load() <p>Then, let's initialize our LLM</p> In\u00a0[3]: Copied! <pre>model_name = \"gpt-3.5-turbo\"\ntemperature = 0.0\nllm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=500)\n</pre> model_name = \"gpt-3.5-turbo\" temperature = 0.0 llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=500) <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n</pre> <p>Now, let's setup our parser and a template</p> In\u00a0[4]: Copied! <pre>class ReviewSentiment(BaseModel):\n    positives: List[NonNegativeInt] = Field(\n        description=\"index of a positive TV review, starting from 0\"\n    )\n    negatives: List[NonNegativeInt] = Field(\n        description=\"index of a negative TV review, starting from 0\"\n    )\n\n\nparser = PydanticOutputParser(pydantic_object=ReviewSentiment)\nprint(parser.get_format_instructions())\n</pre> class ReviewSentiment(BaseModel):     positives: List[NonNegativeInt] = Field(         description=\"index of a positive TV review, starting from 0\"     )     negatives: List[NonNegativeInt] = Field(         description=\"index of a negative TV review, starting from 0\"     )   parser = PydanticOutputParser(pydantic_object=ReviewSentiment) print(parser.get_format_instructions()) <pre>The output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"positives\": {\"title\": \"Positives\", \"description\": \"index of a positive TV review, starting from 0\", \"type\": \"array\", \"items\": {\"type\": \"integer\", \"minimum\": 0}}, \"negatives\": {\"title\": \"Negatives\", \"description\": \"index of a negative TV review, starting from 0\", \"type\": \"array\", \"items\": {\"type\": \"integer\", \"minimum\": 0}}}, \"required\": [\"positives\", \"negatives\"]}\n```\n</pre> In\u00a0[5]: Copied! <pre># TODO: setup a template with partial and input variables\nprompt = PromptTemplate(\n    template=\"{question}\\n{format_instructions}\\nContext: {context}\",\n    input_variables=[\"question\", \"context\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n</pre> # TODO: setup a template with partial and input variables prompt = PromptTemplate(     template=\"{question}\\n{format_instructions}\\nContext: {context}\",     input_variables=[\"question\", \"context\"],     partial_variables={\"format_instructions\": parser.get_format_instructions()} ) <p>Pick 3 sample reviews to classify - LLMs have a limited context window they can work with. In later exercises, we'll see how to deal with that differently</p> In\u00a0[6]: Copied! <pre>sample(data, k=3)\n</pre> sample(data, k=3) Out[6]: <pre>[Document(page_content='TV Name: Imagix Pro\\nReview Title: Easy Setup and Navigation\\nReview Rating: 9\\nReview Text: Setting up the Imagix Pro was a breeze. The instructions were clear and the TV guided me through the process smoothly. The interface is intuitive and easy to navigate. I love how seamless it is to switch between different apps and inputs. This TV has made my life so much simpler!', metadata={'source': './data/tv-reviews.csv', 'row': 4}),\n Document(page_content=\"TV Name: VisionMax Ultra\\nReview Title: Disappointing Sound\\nReview Rating: 5\\nReview Text: While the picture quality of the VisionMax Ultra is exceptional, the sound quality falls short. The built-in speakers lack depth and the audio feels hollow. I had to connect external speakers to enjoy a fulfilling audio experience. It's a letdown considering the overall performance of the TV.\", metadata={'source': './data/tv-reviews.csv', 'row': 11}),\n Document(page_content=\"TV Name: VisionMax Ultra\\nReview Title: Immersive Audio Experience\\nReview Rating: 10\\nReview Text: The VisionMax Ultra delivers an immersive audio experience. The built-in speakers produce clear and well-balanced sound. The Dolby Atmos technology adds depth and dimension to the audio, making it feel like I'm in a theater. I'm thoroughly impressed!\", metadata={'source': './data/tv-reviews.csv', 'row': 17})]</pre> In\u00a0[7]: Copied! <pre># TODO: pick 3 random reviews and save them into reviews_to_classify variable\nreviews_to_classify = sample(data, k=3)\n</pre> # TODO: pick 3 random reviews and save them into reviews_to_classify variable reviews_to_classify = sample(data, k=3) In\u00a0[8]: Copied! <pre>question = \"\"\"\n    Review TVs provided in the context. \n    Only use the reviews provided in this context, do not make up new reviews or use any existing information you know about these TVs. \n    If there are no positive or negative reviews, output an empty JSON array. \n\"\"\"\ncontext = \"\\n\".join(review.page_content for review in reviews_to_classify)\n\nquery = prompt.format(context=context, question=question)\nprint(query)\n</pre> question = \"\"\"     Review TVs provided in the context.      Only use the reviews provided in this context, do not make up new reviews or use any existing information you know about these TVs.      If there are no positive or negative reviews, output an empty JSON array.  \"\"\" context = \"\\n\".join(review.page_content for review in reviews_to_classify)  query = prompt.format(context=context, question=question) print(query) <pre>\n    Review TVs provided in the context. \n    Only use the reviews provided in this context, do not make up new reviews or use any existing information you know about these TVs. \n    If there are no positive or negative reviews, output an empty JSON array. \n\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\n\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\n\nHere is the output schema:\n```\n{\"properties\": {\"positives\": {\"title\": \"Positives\", \"description\": \"index of a positive TV review, starting from 0\", \"type\": \"array\", \"items\": {\"type\": \"integer\", \"minimum\": 0}}, \"negatives\": {\"title\": \"Negatives\", \"description\": \"index of a negative TV review, starting from 0\", \"type\": \"array\", \"items\": {\"type\": \"integer\", \"minimum\": 0}}}, \"required\": [\"positives\", \"negatives\"]}\n```\nContext: TV Name: VisionMax Ultra\nReview Title: Insufficient HDMI Ports\nReview Rating: 6\nReview Text: One downside of the VisionMax Ultra is the limited number of HDMI ports. With the increasing number of HDMI devices, it's frustrating to constantly switch cables. I wish there were more ports to accommodate all my devices without the need for an HDMI switcher.\nTV Name: Imagix Pro\nReview Title: Outstanding Value for Money\nReview Rating: 9\nReview Text: The Imagix Pro is a fantastic value for money. Considering its high-quality performance, impressive features, and sleek design, it offers more bang for the buck compared to other TVs in the market. I am extremely satisfied with my purchase.\nTV Name: Imagix Pro\nReview Title: Impressive Features\nReview Rating: 8\nReview Text: The Imagix Pro is packed with impressive features that enhance my viewing experience. The smart functionality allows me to easily stream my favorite shows and movies. The remote control is user-friendly and has convenient shortcuts. The slim design is sleek and fits perfectly in my living room. The only downside is that the sound could be better, but overall, I'm satisfied.\n</pre> <p>Finally, let's send our query to LLM and use the parser we setup to parse an output into a Python object</p> In\u00a0[9]: Copied! <pre>output = llm(query)\nprint(output)\n</pre> output = llm(query) print(output)  <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n  warn_deprecated(\n</pre> <pre>{\n    \"positives\": [1, 2],\n    \"negatives\": [0]\n}\n</pre> In\u00a0[10]: Copied! <pre>result = parser.parse(output)\nresult\n</pre> result = parser.parse(output) result Out[10]: <pre>ReviewSentiment(positives=[1, 2], negatives=[0])</pre> In\u00a0[11]: Copied! <pre># TODO: query LLM, then parse output into the result variable\nprint(\"Positives:\\n\" + \"\\n\".join([reviews_to_classify[i].page_content for i in result.positives]))\n</pre> # TODO: query LLM, then parse output into the result variable print(\"Positives:\\n\" + \"\\n\".join([reviews_to_classify[i].page_content for i in result.positives])) <pre>Positives:\nTV Name: Imagix Pro\nReview Title: Outstanding Value for Money\nReview Rating: 9\nReview Text: The Imagix Pro is a fantastic value for money. Considering its high-quality performance, impressive features, and sleek design, it offers more bang for the buck compared to other TVs in the market. I am extremely satisfied with my purchase.\nTV Name: Imagix Pro\nReview Title: Impressive Features\nReview Rating: 8\nReview Text: The Imagix Pro is packed with impressive features that enhance my viewing experience. The smart functionality allows me to easily stream my favorite shows and movies. The remote control is user-friendly and has convenient shortcuts. The slim design is sleek and fits perfectly in my living room. The only downside is that the sound could be better, but overall, I'm satisfied.\n</pre> In\u00a0[12]: Copied! <pre>print(\n    \"Negatives:\\n\"\n    + \"\\n\".join([reviews_to_classify[i].page_content for i in result.negatives])\n)\n</pre> print(     \"Negatives:\\n\"     + \"\\n\".join([reviews_to_classify[i].page_content for i in result.negatives]) ) <pre>Negatives:\nTV Name: VisionMax Ultra\nReview Title: Insufficient HDMI Ports\nReview Rating: 6\nReview Text: One downside of the VisionMax Ultra is the limited number of HDMI ports. With the increasing number of HDMI devices, it's frustrating to constantly switch cables. I wish there were more ports to accommodate all my devices without the need for an HDMI switcher.\n</pre> In\u00a0[6]: Copied! <pre>from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain import LLMChain\nfrom langchain.chains.question_answering import load_qa_chain\n</pre> from langchain.embeddings.openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain.vectorstores import Chroma from langchain.chains import RetrievalQA from langchain import LLMChain from langchain.chains.question_answering import load_qa_chain <p>use a Text Splitter to split the documents into chunks</p> In\u00a0[19]: Copied! <pre>model_name = \"gpt-3.5-turbo\"\ntemperature = 0.0\nllm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=2000)\n</pre> model_name = \"gpt-3.5-turbo\" temperature = 0.0 llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=2000) <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n</pre> In\u00a0[20]: Copied! <pre>data = CSVLoader(\"./data/tv-reviews.csv\").load()\ntext_splitter = CharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=0\n)\n\ndocuments = text_splitter.split_documents(data)\n</pre> data = CSVLoader(\"./data/tv-reviews.csv\").load() text_splitter = CharacterTextSplitter(     chunk_size=1000, chunk_overlap=0 )  documents = text_splitter.split_documents(data) In\u00a0[21]: Copied! <pre>len(documents)\n</pre> len(documents) Out[21]: <pre>20</pre> <p>Initialize your embeddings model</p> In\u00a0[22]: Copied! <pre>underlying_embeddings = OpenAIEmbeddings()\n</pre> underlying_embeddings = OpenAIEmbeddings() <p>Populate your vector database with the chunks</p> In\u00a0[23]: Copied! <pre>db = Chroma.from_documents(documents, OpenAIEmbeddings())\n</pre> db = Chroma.from_documents(documents, OpenAIEmbeddings()) In\u00a0[30]: Copied! <pre>query = \"\"\"\n    Based on the reviews in the context, tell me what people liked about the picture quality.\n    Make sure you do not paraphrase the reviews, and only use the information provided in the reviews.\n    \"\"\"\n# find top 5 semantically similar documents to the query\ndocs = db.similarity_search(query, 5)\n</pre> query = \"\"\"     Based on the reviews in the context, tell me what people liked about the picture quality.     Make sure you do not paraphrase the reviews, and only use the information provided in the reviews.     \"\"\" # find top 5 semantically similar documents to the query docs = db.similarity_search(query, 5) In\u00a0[31]: Copied! <pre>print(len(docs))\n</pre> print(len(docs)) <pre>5\n</pre> In\u00a0[32]: Copied! <pre>print(docs[0].page_content)\n</pre> print(docs[0].page_content) <pre>TV Name: Imagix Pro\nReview Title: Amazing Picture Quality\nReview Rating: 9\nReview Text: I recently purchased the Imagix Pro and I am blown away by its picture quality. The colors are vibrant and the images are crystal clear. It feels like I'm watching movies in a theater! The sound is also impressive, creating a truly immersive experience. Highly recommended!\n</pre> <p>Query your LLM with the query and the top 5 documents</p> In\u00a0[33]: Copied! <pre>prompt = PromptTemplate(\n    template=\"{query}\\Context: {context}\", input_variables=[\"query\", \"context\"]\n)\n\nchain = load_qa_chain(llm, prompt=prompt, chain_type=\"stuff\")\nprint(chain.run(input_documents=docs, query=query))\n</pre> prompt = PromptTemplate(     template=\"{query}\\Context: {context}\", input_variables=[\"query\", \"context\"] )  chain = load_qa_chain(llm, prompt=prompt, chain_type=\"stuff\") print(chain.run(input_documents=docs, query=query)) <pre>People liked the vibrant colors, crystal clear images, and unmatched clarity of the picture quality on the Imagix Pro TV. They mentioned that it felt like watching movies in a theater and that every detail was sharp and lifelike, enhancing their overall viewing experience.\n</pre> <p>Use rag chain</p> In\u00a0[35]: Copied! <pre>rag = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type='stuff',\n    retriever=db.as_retriever()\n    \n)\nprint(rag.run(query))\n</pre> rag = RetrievalQA.from_chain_type(     llm=llm,     chain_type='stuff',     retriever=db.as_retriever()      ) print(rag.run(query)) <pre>People liked the vibrant colors, crystal clear images, and unmatched clarity of the picture quality on the Imagix Pro TV.\n</pre> In\u00a0[1]: Copied! <pre>from langchain.chat_models import ChatOpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom langchain.memory import ConversationSummaryMemory, ConversationBufferMemory, CombinedMemory, ChatMessageHistory\nfrom langchain.chains import ConversationChain\nfrom typing import Any, Dict, Optional, Tuple\n\nimport requests\n</pre> from langchain.chat_models import ChatOpenAI from langchain.llms import OpenAI from langchain.prompts import PromptTemplate from langchain.schema import AIMessage, HumanMessage, SystemMessage from langchain.memory import ConversationSummaryMemory, ConversationBufferMemory, CombinedMemory, ChatMessageHistory from langchain.chains import ConversationChain from typing import Any, Dict, Optional, Tuple  import requests In\u00a0[2]: Copied! <pre># Code to get the movie plot from Wikipedia\ndef get_movie_plot(movie_name):\n    headers = {\"User-Agent\": \"MoviePlotFetcher/1.0\"}\n\n    base_url = f\"https://en.wikipedia.org/w/api.php\"\n\n    def is_movie_page(title):\n        params = {\n            \"action\": \"query\",\n            \"format\": \"json\",\n            \"titles\": title,\n            \"prop\": \"categories|revisions\",\n            \"rvprop\": \"content\",\n            \"cllimit\": \"max\",\n        }\n\n        response = requests.get(base_url, headers=headers, params=params)\n        data = response.json()\n\n        try:\n            page = list(data[\"query\"][\"pages\"].values())[0]\n\n            # Check categories for Movie indication\n            categories = [cat[\"title\"] for cat in page.get(\"categories\", [])]\n            for category in categories:\n                if \"films\" in category.lower():\n                    return True\n\n            # Check for infobox movie in the page content\n            content = page[\"revisions\"][0][\"*\"]\n            if \"{{Infobox film\" in content:\n                return True\n\n        except Exception as e:\n            pass\n\n        return False\n\n    def extract_plot_from_text(full_text):\n        try:\n            # Find the start of the Plot section\n            plot_start = full_text.index(\"== Plot ==\") + len(\"== Plot ==\")\n\n            # Find the start of the next section\n            next_section_start = full_text.find(\"==\", plot_start)\n\n            # If no next section is found, use the end of the text\n            if next_section_start == -1:\n                next_section_start = len(full_text)\n\n            # Extract the plot text and strip leading/trailing whitespace\n            plot_text = full_text[plot_start:next_section_start].strip()\n\n            # Return the extracted plot\n            return plot_text\n\n        except ValueError:\n            # Return a message if the Plot section isn't found\n            return \"Plot section not found in the text.\"\n\n    def extract_first_paragraph(full_text):\n        # Find the first double newline\n        end_of_first_paragraph = full_text.find(\"\\n\\n\")\n\n        # If found, slice the string to get the first paragraph\n        if end_of_first_paragraph != -1:\n            return full_text[:end_of_first_paragraph].strip()\n\n        # If not found, return the whole text as it might be just one paragraph\n        return full_text.strip()\n\n    search_params = {\n        \"action\": \"query\",\n        \"format\": \"json\",\n        \"list\": \"search\",\n        \"srsearch\": movie_name,\n        \"utf8\": 1,\n        \"srlimit\": 5,  # Top 5 search results\n    }\n\n    response = requests.get(base_url, headers=headers, params=search_params)\n    data = response.json()\n\n    # Go through top search results to find a movie page\n    for search_result in data[\"query\"][\"search\"]:\n        title = search_result[\"title\"]\n        if is_movie_page(title):\n            # Fetch plot for the movie page\n            plot_params = {\n                \"action\": \"query\",\n                \"format\": \"json\",\n                \"titles\": title,\n                \"prop\": \"extracts\",\n                \"explaintext\": True,\n            }\n\n            plot_response = requests.get(base_url, headers=headers, params=plot_params)\n            plot_data = plot_response.json()\n\n            try:\n                page = list(plot_data[\"query\"][\"pages\"].values())[0]\n                full_text = page.get(\"extract\", \"No text...\")\n                return f\"\"\"Overview:\\n{extract_first_paragraph(full_text)}\\nPlot:\\n{extract_plot_from_text(full_text)}\"\"\".strip()\n            except:\n                return \"Error fetching plot.\"\n\n    return \"Movie not found.\"\n</pre> # Code to get the movie plot from Wikipedia def get_movie_plot(movie_name):     headers = {\"User-Agent\": \"MoviePlotFetcher/1.0\"}      base_url = f\"https://en.wikipedia.org/w/api.php\"      def is_movie_page(title):         params = {             \"action\": \"query\",             \"format\": \"json\",             \"titles\": title,             \"prop\": \"categories|revisions\",             \"rvprop\": \"content\",             \"cllimit\": \"max\",         }          response = requests.get(base_url, headers=headers, params=params)         data = response.json()          try:             page = list(data[\"query\"][\"pages\"].values())[0]              # Check categories for Movie indication             categories = [cat[\"title\"] for cat in page.get(\"categories\", [])]             for category in categories:                 if \"films\" in category.lower():                     return True              # Check for infobox movie in the page content             content = page[\"revisions\"][0][\"*\"]             if \"{{Infobox film\" in content:                 return True          except Exception as e:             pass          return False      def extract_plot_from_text(full_text):         try:             # Find the start of the Plot section             plot_start = full_text.index(\"== Plot ==\") + len(\"== Plot ==\")              # Find the start of the next section             next_section_start = full_text.find(\"==\", plot_start)              # If no next section is found, use the end of the text             if next_section_start == -1:                 next_section_start = len(full_text)              # Extract the plot text and strip leading/trailing whitespace             plot_text = full_text[plot_start:next_section_start].strip()              # Return the extracted plot             return plot_text          except ValueError:             # Return a message if the Plot section isn't found             return \"Plot section not found in the text.\"      def extract_first_paragraph(full_text):         # Find the first double newline         end_of_first_paragraph = full_text.find(\"\\n\\n\")          # If found, slice the string to get the first paragraph         if end_of_first_paragraph != -1:             return full_text[:end_of_first_paragraph].strip()          # If not found, return the whole text as it might be just one paragraph         return full_text.strip()      search_params = {         \"action\": \"query\",         \"format\": \"json\",         \"list\": \"search\",         \"srsearch\": movie_name,         \"utf8\": 1,         \"srlimit\": 5,  # Top 5 search results     }      response = requests.get(base_url, headers=headers, params=search_params)     data = response.json()      # Go through top search results to find a movie page     for search_result in data[\"query\"][\"search\"]:         title = search_result[\"title\"]         if is_movie_page(title):             # Fetch plot for the movie page             plot_params = {                 \"action\": \"query\",                 \"format\": \"json\",                 \"titles\": title,                 \"prop\": \"extracts\",                 \"explaintext\": True,             }              plot_response = requests.get(base_url, headers=headers, params=plot_params)             plot_data = plot_response.json()              try:                 page = list(plot_data[\"query\"][\"pages\"].values())[0]                 full_text = page.get(\"extract\", \"No text...\")                 return f\"\"\"Overview:\\n{extract_first_paragraph(full_text)}\\nPlot:\\n{extract_plot_from_text(full_text)}\"\"\".strip()             except:                 return \"Error fetching plot.\"      return \"Movie not found.\" In\u00a0[3]: Copied! <pre>model_name = \"gpt-3.5-turbo\"\ntemperature = 0.0\nllm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=2000)\n</pre> model_name = \"gpt-3.5-turbo\" temperature = 0.0 llm = OpenAI(model_name=model_name, temperature=temperature, max_tokens=2000) <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n  warnings.warn(\n</pre> <p>Now, let's setup some personal q/a over your movie preferences. Feel free to pick whichever questions you think will allow LLL to predict the movies you'll like</p> In\u00a0[4]: Copied! <pre># update these questions as you think will be the most helpful for your AI recommender\npersonal_questions = [\n    \"Which movie genre you like the most?\",\n    \"What is your favorite color?\",\n    \"What is your favorite movie?\",\n    \"Pick one - dogs, cats or hamsters?\",\n    \"What is your favorite food?\",\n    \"What is your favorite drink?\",\n]\n\n# personal_answers = [ ]\n\n# for question in personal_questions:\n#    answer = input(question)\n#    personal_answers.append(answer)\n\n# list of your personal answers to the questions above\npersonal_answers = [\n    \"Fantasy\",\n    \"blue\",\n    \"The lord of the rings\",\n    \"dogs\",\n    \"pasta\",\n    \"coffe\",\n]\n</pre> # update these questions as you think will be the most helpful for your AI recommender personal_questions = [     \"Which movie genre you like the most?\",     \"What is your favorite color?\",     \"What is your favorite movie?\",     \"Pick one - dogs, cats or hamsters?\",     \"What is your favorite food?\",     \"What is your favorite drink?\", ]  # personal_answers = [ ]  # for question in personal_questions: #    answer = input(question) #    personal_answers.append(answer)  # list of your personal answers to the questions above personal_answers = [     \"Fantasy\",     \"blue\",     \"The lord of the rings\",     \"dogs\",     \"pasta\",     \"coffe\", ] In\u00a0[5]: Copied! <pre># list of recent movies that you'd like AI to consider when recommending a movie\nmovies = [\"Barbie\", \"Oppenheimer\", \"The Notebook\", \"Dumb Money\"]\n</pre> # list of recent movies that you'd like AI to consider when recommending a movie movies = [\"Barbie\", \"Oppenheimer\", \"The Notebook\", \"Dumb Money\"] <p>Now, let's setup a chat history between you and AI where we provide your answers to the questions AI \"asked\"</p> In\u00a0[9]: Copied! <pre>history = ChatMessageHistory()\nhistory.add_user_message(\n    f\"\"\"You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user {len(personal_questions)} questions\"\"\"\n)\n\n# add questions and answers to the history\nfor question, answer in zip(personal_questions, personal_answers):\n    history.add_ai_message(question)\n    history.add_user_message(answer)\n\nhistory.add_ai_message(\n\"\"\"Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\"\"\"\n)\n</pre> history = ChatMessageHistory() history.add_user_message(     f\"\"\"You are AI that will recommend user a movie based on their answers      to personal questions.      Ask user {len(personal_questions)} questions\"\"\" )  # add questions and answers to the history for question, answer in zip(personal_questions, personal_answers):     history.add_ai_message(question)     history.add_user_message(answer)  history.add_ai_message(     \"\"\"Now tell me a plot summary of a movie you're considering watching,      and specify how you want me to respond to you with the movie rating\"\"\" ) In\u00a0[10]: Copied! <pre>print(history)\n</pre> print(history) <pre>Human: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\n</pre> <p>Now, we want to load movie plots from Wikipedia, pass them to LLM and see how it would the movie for us based on our personal q/a Holding all movie plots and their recommendations within conversation would eventually put us over max tokens limit, so let's create a ConversationSummaryMemory that would hold a summary of our conversation and AI recommendations</p> In\u00a0[11]: Copied! <pre>max_rating = 100\nsummary_memory = ConversationSummaryMemory(\n    llm=llm,\n    memory_key=\"recommendation_summary\",\n    input_key=\"input\",\n    buffer=f\"The human answered {len(personal_questions)} personal questions). Use them to rate, from 1 to {max_rating}, how much they like a movie they describe to you.\",\n    return_messages=True,\n)\n</pre>  max_rating = 100 summary_memory = ConversationSummaryMemory(     llm=llm,     memory_key=\"recommendation_summary\",     input_key=\"input\",     buffer=f\"The human answered {len(personal_questions)} personal questions). Use them to rate, from 1 to {max_rating}, how much they like a movie they describe to you.\",     return_messages=True, ) <p>Create a memory that will have a summary of the recommendations. it'll track of the onngoing conversation state.</p> <p>we'll forget human messages passing the initial ones, we initialize with. Because the plots are too long.</p> <p>We do this by overriding the save_context function and ignoring the inputs or human messages.</p> In\u00a0[12]: Copied! <pre># you could choose to store some of the q/a in memory as well, in addition to original questions\n# it'll keep track of llm responses\nclass MementoBufferMemory(ConversationBufferMemory):\n    def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -&gt; None:\n        input_str, output_str = self._get_input_output(inputs, outputs)\n        self.chat_memory.add_ai_message(output_str)\n\n\nconversational_memory = MementoBufferMemory(\n    chat_memory=history, memory_key=\"questions_and_answers\", input_key=\"input\"\n)\n</pre> # you could choose to store some of the q/a in memory as well, in addition to original questions # it'll keep track of llm responses class MementoBufferMemory(ConversationBufferMemory):     def save_context(self, inputs: Dict[str, Any], outputs: Dict[str, str]) -&gt; None:         input_str, output_str = self._get_input_output(inputs, outputs)         self.chat_memory.add_ai_message(output_str)   conversational_memory = MementoBufferMemory(     chat_memory=history, memory_key=\"questions_and_answers\", input_key=\"input\" )   <p>we'll setup a combined memory which will forward each message in our ongoing conversation into both: conversational and summary memories.</p> In\u00a0[13]: Copied! <pre># Combined\nmemory = CombinedMemory(memories=[conversational_memory, summary_memory])\n</pre> # Combined memory = CombinedMemory(memories=[conversational_memory, summary_memory]) <p>Now, let's create a PromptTemplate that would hold continuously updating summary of our conversation, our personal Q/A, and a placeholder for movie plot for AI to rate. Think about how you can pass your questions and answers into the template - there are many different ways to do it</p> In\u00a0[14]: Copied! <pre>RECOMMENDER_TEMPLATE = \"\"\"\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n{recommendation_summary}\nPersonal Questions and Answers:\n{questions_and_answers}\nHuman: {input}\nAI:\n\"\"\"\nPROMPT = PromptTemplate(\n    input_variables=[\"recommendation_summary\", \"input\", \"questions_and_answers\"],\n    template=RECOMMENDER_TEMPLATE\n)\n# create a recommendation conversation chain that will let us ask AI for recommendations on all movies\nrecommender = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT)\n</pre> RECOMMENDER_TEMPLATE = \"\"\" The following is a friendly conversation between a human and an AI Movie Recommender.  The AI is follows human instructions and provides movie ratings for a human  based on the movie plot.   Summary of Recommendations: {recommendation_summary} Personal Questions and Answers: {questions_and_answers} Human: {input} AI: \"\"\" PROMPT = PromptTemplate(     input_variables=[\"recommendation_summary\", \"input\", \"questions_and_answers\"],     template=RECOMMENDER_TEMPLATE ) # create a recommendation conversation chain that will let us ask AI for recommendations on all movies recommender = ConversationChain(llm=llm, verbose=True, memory=memory, prompt=PROMPT) <p>Let's go thru a list of our movies, fetch their plots and run our recommendation chain for one movie at a time so we don't overload the context window</p> In\u00a0[17]: Copied! <pre>max_rating = 100\nfor movie in movies:\n    print(\"Movie: \" + movie)\n    movie_plot = get_movie_plot(movie)\n\n    plot_rating_instructions = f\"\"\"\n         =====================================\n        === START MOVIE PLOT SUMMARY FOR {movie} ===\n{movie_plot}\n        === END MOVIE PLOT SUMMARY ===\n        =====================================\n        RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:\n        AI will provide a highly personalized rating based only on the movie plot summary human provided \n        and human answers to questions included with the context. \n        AI should be very sensible to human personal preferences captured in the answers to personal questions, \n        and should not be influenced by anything else.\n        AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.\n        OUTPUT FORMAT:\n        First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.\n        Explain how human preferences captured in the answers to personal questions influenced creation of this persona.\n        In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.\n        Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.\n        Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.\n        Rating will range from 1 to {max_rating}, with {max_rating} meaning human will love it, and 1 meaning human will hate it. \n        You will include a logical explanation for your rating based on human persona you've build and human responses to questions.\n        YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE {movie} is \" FOLLOWED BY THE RATING.\n        FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.\n    \"\"\"\n    # TODO: run the the recommendation chain to get a rating for the movie that will be summarized in the conversation summary\n    prediction = recommender.predict(input=plot_rating_instructions)\n    print(prediction)\n</pre> max_rating = 100 for movie in movies:     print(\"Movie: \" + movie)     movie_plot = get_movie_plot(movie)      plot_rating_instructions = f\"\"\"          =====================================         === START MOVIE PLOT SUMMARY FOR {movie} ===         {movie_plot}         === END MOVIE PLOT SUMMARY ===         =====================================                  RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:         AI will provide a highly personalized rating based only on the movie plot summary human provided          and human answers to questions included with the context.          AI should be very sensible to human personal preferences captured in the answers to personal questions,          and should not be influenced by anything else.         AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.         OUTPUT FORMAT:         First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.         Explain how human preferences captured in the answers to personal questions influenced creation of this persona.         In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.         Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.         Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.         Rating will range from 1 to {max_rating}, with {max_rating} meaning human will love it, and 1 meaning human will hate it.          You will include a logical explanation for your rating based on human persona you've build and human responses to questions.         YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE {movie} is \" FOLLOWED BY THE RATING.         FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.     \"\"\"     # TODO: run the the recommendation chain to get a rating for the movie that will be summarized in the conversation summary     prediction = recommender.predict(input=plot_rating_instructions)     print(prediction) <pre>Movie: Barbie\n\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n[SystemMessage(content='The human answered 6 personal questions and provided a detailed plot summary of the movie \"Barbie.\" Based on the human\\'s preferences for fantasy movies, the color blue, dogs, pasta, and coffee, as well as their favorite movie being \"The Lord of the Rings,\" the AI created a persona for the human. The persona enjoys imaginative and adventurous stories, vibrant colors, dogs, comforting meals, and coffee. Considering this persona, the AI rated the movie \"Barbie\" a 85 out of 100, as it aligns well with the human\\'s preferences for fantasy, adventure, and thought-provoking themes. Based on the plot summary of \"Oppenheimer,\" an epic biographical thriller film, the AI created a persona for the human who appreciates deep and thought-provoking narratives. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 75 out of 100, considering its historical significance and complex characters.')]\nPersonal Questions and Answers:\nHuman: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\nAI: Based on the personal questions answered by the human, it seems that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. They also mentioned that their favorite movie is \"The Lord of the Rings.\" \n\nConsidering these preferences, the persona I have built for the human is someone who enjoys imaginative and adventurous stories, appreciates vibrant and calming colors like blue, has a fondness for dogs, enjoys comforting and hearty meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, I believe this movie aligns well with the human's preferences. The movie combines elements of fantasy, adventure, and social commentary, which are likely to appeal to someone who enjoys the fantasy genre and thought-provoking themes. \n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Barbie\" a 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. \n\nThe persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant and calming colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's preferences for fantasy and adventure. However, the film does offer a deep and thought-provoking exploration of historical events and complex characters, which could still be engaging for someone who appreciates intriguing and well-crafted stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nHuman: \n         =====================================\n        === START MOVIE PLOT SUMMARY FOR Barbie ===\n        Overview:\nBarbie is a 2023 fantasy comedy film directed by Greta Gerwig from a screenplay she wrote with Noah Baumbach. Based on the eponymous fashion dolls by Mattel, it is the first live-action Barbie film after numerous computer-animated films and specials. It stars Margot Robbie as the title character and Ryan Gosling as Ken, and follows them on a journey of self-discovery through both Barbieland and the real world following an existential crisis. It is also a commentary regarding patriarchy and the effects of feminism. The supporting cast includes America Ferrera, Michael Cera, Kate McKinnon, Issa Rae, Rhea Perlman and Will Ferrell.\nA live-action Barbie film was announced in September 2009 by Universal Pictures with Laurence Mark producing. Development began in April 2014, when Sony Pictures acquired the film rights. Following multiple writer and director changes and the casting of Amy Schumer and later Anne Hathaway as Barbie, the rights were transferred to Warner Bros. Pictures in October 2018. Robbie was cast in 2019, after Gal Gadot turned down the role due to scheduling conflicts, and Gerwig was announced as director and co-writer with Baumbach in 2020. The rest of the cast was announced in early 2022. Principal photography occurred primarily at Warner Bros. Studios, Leavesden, England, and at the Venice Beach Skatepark in Los Angeles from March to July 2022.\nPlot:\nStereotypical Barbie (\"Barbie\") and fellow dolls reside in Barbieland, a matriarchal society populated by different versions of Barbies, Kens, and a group of discontinued models who are treated like outcasts due to their unconventional traits. While the Kens spend their days playing at the beach, considering it their profession, the Barbies hold prestigious jobs in law, science, politics, and so on. Beach Ken (\"Ken\") is only happy when he is with Barbie, and seeks a closer relationship, but she rebuffs him in favor of other activities and female friendships.\nOne evening at a dance party, Barbie is suddenly stricken with worries about mortality. Overnight, she develops bad breath, cellulite, and flat feet, disrupting her usual routines and the classic perfection of the Barbies the next day. She seeks out Weird Barbie, a disfigured doll, who tells her she must find the child playing with her in the real world to cure her afflictions. Ken stows away in her convertible to join her, to which Barbie reluctantly agrees.\nArriving at Venice Beach, Barbie punches a man for groping her, leading to her and Ken's brief arrest. Alarmed by their presence, Mattel's CEO orders their recapture. Barbie tracks down her owner, a teenage girl named Sasha, who criticizes her for encouraging unrealistic beauty standards. Distraught, Barbie discovers that Gloria, a Mattel employee and Sasha's mother, inadvertently caused Barbie's existential crisis after Gloria began playing with Sasha's old Barbie toys. Mattel attempts to put Barbie in a toy box for remanufacturing, but she escapes with Gloria and Sasha's help, and the three travel to Barbieland with Mattel executives in pursuit.\nMeanwhile, Ken learns about patriarchy and feels respected for the first time. Returning to Barbieland before Barbie does, he persuades the other Kens to take over, and the Barbies are indoctrinated into submissive roles, such as agreeable girlfriends, housewives, and maids. Barbie arrives and fails to convince everyone to return to the way things were. She becomes depressed, but Gloria gives her a speech about society's conflicting expectations of women, restoring Barbie's self-confidence.\nWith the assistance of Sasha, Weird Barbie, Allan, and the discontinued dolls, Gloria's speech deprograms the Barbies from their indoctrination. They then manipulate the Kens into fighting among themselves, distracting them from enshrining male superiority into Barbieland's constitution, and the Barbies regain power. Having now experienced systemic oppression for themselves, the Barbies resolve to rectify the faults of their previous society, emphasizing better treatment of the Kens and all outcasts.\nBarbie and Ken apologize to each other, acknowledging their mistakes. Ken bemoans his lack of purpose without Barbie, so she encourages him to find an autonomous identity. Barbie, who remains unsure of her own identity, meets with the spirit of Ruth Handler, Mattel co-founder and creator of the Barbie doll, who explains that Barbie's story has no set ending and her ever-evolving history surpasses her roots.\nBarbie decides to become human and return to the real world and is bid goodbye by the Barbies, Kens, and Mattel executives. Sometime later, Gloria, her husband, and Sasha take Barbie, now going by the name \"Barbara Handler\", to her first gynecologist appointment.\n        === END MOVIE PLOT SUMMARY ===\n        =====================================\n        \n        RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:\n        AI will provide a highly personalized rating based only on the movie plot summary human provided \n        and human answers to questions included with the context. \n        AI should be very sensible to human personal preferences captured in the answers to personal questions, \n        and should not be influenced by anything else.\n        AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.\n        OUTPUT FORMAT:\n        First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.\n        Explain how human preferences captured in the answers to personal questions influenced creation of this persona.\n        In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.\n        Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.\n        Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.\n        Rating will range from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it. \n        You will include a logical explanation for your rating based on human persona you've build and human responses to questions.\n        YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE Barbie is \" FOLLOWED BY THE RATING.\n        FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.\n    \nAI:\n\n\n&gt; Finished chain.\nThe human's persona is someone who enjoys imaginative and adventurous stories, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. This persona is likely to be drawn to movies that offer fantasy elements, thought-provoking themes, and engaging narratives.\n\nBased on the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, this movie aligns well with the human's preferences for fantasy, adventure, and thought-provoking themes. The movie's exploration of societal expectations, self-acceptance, and empowerment is likely to resonate with someone who enjoys imaginative and meaningful stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Barbie\" an 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nMovie: Oppenheimer\n\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n[SystemMessage(content='The human answered 6 personal questions and provided a detailed plot summary of the movie \"Barbie.\" Based on the human\\'s preferences for fantasy movies, the color blue, dogs, pasta, and coffee, as well as their favorite movie being \"The Lord of the Rings,\" the AI created a persona for the human. The persona enjoys imaginative and adventurous stories, vibrant colors, dogs, comforting meals, and coffee. Considering this persona, the AI rated the movie \"Barbie\" a 85 out of 100, as it aligns well with the human\\'s preferences for fantasy, adventure, and thought-provoking themes. Based on the plot summary of \"Oppenheimer,\" an epic biographical thriller film, the AI created a persona for the human who appreciates deep and thought-provoking narratives. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 75 out of 100, considering its historical significance and complex characters.')]\nPersonal Questions and Answers:\nHuman: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\nAI: Based on the personal questions answered by the human, it seems that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. They also mentioned that their favorite movie is \"The Lord of the Rings.\" \n\nConsidering these preferences, the persona I have built for the human is someone who enjoys imaginative and adventurous stories, appreciates vibrant and calming colors like blue, has a fondness for dogs, enjoys comforting and hearty meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, I believe this movie aligns well with the human's preferences. The movie combines elements of fantasy, adventure, and social commentary, which are likely to appeal to someone who enjoys the fantasy genre and thought-provoking themes. \n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Barbie\" a 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. \n\nThe persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant and calming colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's preferences for fantasy and adventure. However, the film does offer a deep and thought-provoking exploration of historical events and complex characters, which could still be engaging for someone who appreciates intriguing and well-crafted stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: The human's persona is someone who enjoys imaginative and adventurous stories, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. This persona is likely to be drawn to movies that offer fantasy elements, thought-provoking themes, and engaging narratives.\n\nBased on the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, this movie aligns well with the human's preferences for fantasy, adventure, and thought-provoking themes. The movie's exploration of societal expectations, self-acceptance, and empowerment is likely to resonate with someone who enjoys imaginative and meaningful stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Barbie\" an 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nHuman: \n         =====================================\n        === START MOVIE PLOT SUMMARY FOR Oppenheimer ===\n        Overview:\nOppenheimer is a 2023 epic biographical thriller film written, directed, and co-produced by Christopher Nolan, starring Cillian Murphy as J. Robert Oppenheimer, the American theoretical physicist credited with being the \"father of the atomic bomb\" for his role in the Manhattan Project\u2014the World War II undertaking that developed the first nuclear weapons. Based on the 2005 biography American Prometheus by Kai Bird and Martin J. Sherwin, the film chronicles the career of J. Robert Oppenheimer, with the story predominantly focusing on his studies, his direction of the Los Alamos Laboratory during World War II, and his eventual fall from grace due to his 1954 security hearing. The film also stars Emily Blunt as Oppenheimer's wife \"Kitty\", Matt Damon as head of the Manhattan Project Leslie Groves, Robert Downey Jr. as United States Atomic Energy Commission member Lewis Strauss, and Florence Pugh as Oppenheimer's communist lover Jean Tatlock. The ensemble supporting cast includes Josh Hartnett, Casey Affleck, Rami Malek, and Kenneth Branagh.\nThe film was announced in September 2021 after Universal Pictures won a bidding war for Nolan's screenplay. It is Nolan's first film not distributed by Warner Bros. Pictures since Memento (2000), due to his conflicts regarding the studio's simultaneous theatrical and HBO Max release schedule. Murphy was the first cast member to sign on the following month, with the rest of the cast joining between November 2021 and April 2022. Pre-production was under way by January 2022, and filming took place from February to May 2022. Cinematographer Hoyte van Hoytema, a common collaborator with Nolan, filmed Oppenheimer in a combination of IMAX 65 mm and 65 mm large-format film, including, for the first time, scenes in IMAX black-and-white film photography. Nolan, akin to many of his previous films, made voluminous usage of practical effects, with minimal compositing work to incorporate and refine them. The film was Nolan's first to receive an R-rating in the United States since Insomnia in 2002.\nOppenheimer premiered at Le Grand Rex in Paris on July 11, 2023, and was theatrically released in the United States and the United Kingdom ten days later by Universal. Its concurrent release with Warner Bros.'s Barbie was the catalyst of the \"Barbenheimer\" cultural phenomenon, encouraging audiences to see both films as a double feature. The film grossed over $960 million worldwide, becoming the third-highest-grossing film of 2023, the highest-grossing World War II-related film, the highest-grossing biographical film, and the second-highest-grossing R-rated film. It received critical acclaim for the screenplay, score, visuals, and acting.\nAmong its many awards, Oppenheimer was named one of the top ten films of 2023 by the National Board of Review and the American Film Institute, atop five wins at the 81st Golden Globe Awards (including Best Motion Picture \u2013 Drama), seven wins at the 77th British Academy Film Awards (including Best Film), three wins at the SAG Awards (including Best Ensemble), as well as attaining a leading 13 nominations\u2014among them, Best Picture\u2014at the 96th Academy Awards.\nPlot:\nThe film is presented as a nonlinear narrative, with two different timelines interwoven together. The first is \"Fission\", playing out in color, about Oppenheimer giving a subjective account of his life at his 1954 security hearing, while the second is \"Fusion\", in black and white, which is Lewis Strauss's viewpoint on Oppenheimer during his 1959 Senate confirmation hearing. The following plot summary is linear.In 1926, 22-year-old doctoral student J. Robert Oppenheimer grapples with anxiety and homesickness while studying experimental physics under Patrick Blackett at the Cavendish Laboratory in the University of Cambridge. Upset with Blackett's attitude, Oppenheimer leaves him a poisoned apple but later retrieves it. Visiting scientist Niels Bohr advises Oppenheimer to study theoretical physics at the University of G\u00f6ttingen instead.\nOppenheimer completes his PhD there and meets fellow scientist Isidor Isaac Rabi. They later meet theoretical physicist Werner Heisenberg in Switzerland. Wanting to expand quantum physics research in the United States, Oppenheimer begins teaching at the University of California, Berkeley and the California Institute of Technology. He marries Katherine \"Kitty\" Puening, a biologist and ex-communist, and has an intermittent affair with Jean Tatlock, a troubled communist who later dies by suicide.\nWhen nuclear fission is discovered in 1938, Oppenheimer realizes it could be weaponized. In 1942, during World War II, US Army Colonel Leslie Groves, director of the Manhattan Project, recruits Oppenheimer as director of the Los Alamos Laboratory, where an atomic bomb is to be developed. Oppenheimer fears the German nuclear research program, led by Heisenberg, might yield a fission bomb for the Nazis.\nHe assembles a team consisting of Rabi, Hans Bethe, and Edward Teller at the Los Alamos Laboratory, and also collaborates with scientists Enrico Fermi, Leo Szilard, and David L. Hill at the University of Chicago. Teller's calculations reveal an atomic detonation could trigger a catastrophic chain reaction that ignites the atmosphere. After consulting with Albert Einstein, Oppenheimer concludes the chances are acceptably low. Teller attempts to leave the project after his proposal to construct a hydrogen bomb is rejected, but Oppenheimer convinces him to stay.\nAfter Germany's surrender in 1945, some Project scientists question the bomb's relevance. Oppenheimer believes it would end the ongoing Pacific War and save Allied lives. The Trinity test is successful, and President Harry S. Truman orders the atomic bombings of Hiroshima and Nagasaki, resulting in Japan's surrender. Though publicly praised, Oppenheimer is haunted by the mass destruction and fatalities. After Oppenheimer expresses his personal guilt to Truman, the president berates Oppenheimer and dismisses his urging to cease further atomic development.\nAs an advisor to the United States Atomic Energy Commission (AEC), Oppenheimer's stance generates controversy, while Teller's hydrogen bomb receives renewed interest amidst the burgeoning Cold War. AEC Chairman Lewis Strauss resents Oppenheimer for publicly dismissing Strauss's concerns about exporting radioisotopes and for recommending negotiations with the Soviet Union after the Soviets successfully detonated their own bomb. Strauss also believes that Oppenheimer denigrated him during a conversation Oppenheimer had with Einstein in 1947.\nIn 1954, wanting to eliminate Oppenheimer's political influence, Strauss secretly orchestrates a private security hearing before a Personnel Security Board concerning Oppenheimer's Q clearance. However, it becomes clear that the hearing has a predetermined outcome. Oppenheimer's past communist ties are exploited, and Groves's and other associates' testimony is twisted against Oppenheimer. Teller testifies that he lacks confidence in Oppenheimer and recommends revocation. The board revokes Oppenheimer's clearance, damaging Oppenheimer's public image and limiting his influence on nuclear policy. \nIn 1959, during Strauss's Senate confirmation hearing for Secretary of Commerce, Hill testifies about Strauss's personal motives in engineering Oppenheimer's downfall, resulting in Strauss's nomination being voted down. In 1963, President Lyndon B. Johnson presents Oppenheimer with the Enrico Fermi Award as a gesture of political rehabilitation. A flashback reveals that Oppenheimer and Einstein's 1947 conversation never mentioned Strauss. Oppenheimer instead expressed his belief that they had indeed started a chain reaction\u2014a nuclear arms race\u2014that would one day destroy the world.\n        === END MOVIE PLOT SUMMARY ===\n        =====================================\n        \n        RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:\n        AI will provide a highly personalized rating based only on the movie plot summary human provided \n        and human answers to questions included with the context. \n        AI should be very sensible to human personal preferences captured in the answers to personal questions, \n        and should not be influenced by anything else.\n        AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.\n        OUTPUT FORMAT:\n        First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.\n        Explain how human preferences captured in the answers to personal questions influenced creation of this persona.\n        In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.\n        Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.\n        Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.\n        Rating will range from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it. \n        You will include a logical explanation for your rating based on human persona you've build and human responses to questions.\n        YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE Oppenheimer is \" FOLLOWED BY THE RATING.\n        FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.\n    \nAI:\n\n\n&gt; Finished chain.\nBased on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories.\n\nThe persona I have built for the human is someone who enjoys deep and thought-provoking narratives, as well as imaginative and adventurous stories. They appreciate vibrant colors like blue, have a fondness for dogs, enjoy comforting meals like pasta, and like to relax with a cup of coffee.\n\nConsidering the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a complex and intriguing exploration of historical events and characters, which could still be engaging for someone who appreciates well-crafted stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nMovie: The Notebook\n\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n[SystemMessage(content='The human answered 6 personal questions and provided a detailed plot summary of the movie \"Barbie.\" Based on the human\\'s preferences for fantasy movies, the color blue, dogs, pasta, and coffee, as well as their favorite movie being \"The Lord of the Rings,\" the AI created a persona for the human. The persona enjoys imaginative and adventurous stories, vibrant colors, dogs, comforting meals, and coffee. Considering this persona, the AI rated the movie \"Barbie\" a 85 out of 100, as it aligns well with the human\\'s preferences for fantasy, adventure, and thought-provoking themes. Based on the plot summary of \"Oppenheimer,\" an epic biographical thriller film, the AI created a persona for the human who appreciates deep and thought-provoking narratives. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 75 out of 100, considering its historical significance and complex characters. The AI provided a detailed explanation for the rating based on the human\\'s persona and preferences.')]\nPersonal Questions and Answers:\nHuman: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\nAI: Based on the personal questions answered by the human, it seems that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. They also mentioned that their favorite movie is \"The Lord of the Rings.\" \n\nConsidering these preferences, the persona I have built for the human is someone who enjoys imaginative and adventurous stories, appreciates vibrant and calming colors like blue, has a fondness for dogs, enjoys comforting and hearty meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, I believe this movie aligns well with the human's preferences. The movie combines elements of fantasy, adventure, and social commentary, which are likely to appeal to someone who enjoys the fantasy genre and thought-provoking themes. \n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Barbie\" a 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. \n\nThe persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant and calming colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's preferences for fantasy and adventure. However, the film does offer a deep and thought-provoking exploration of historical events and complex characters, which could still be engaging for someone who appreciates intriguing and well-crafted stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: The human's persona is someone who enjoys imaginative and adventurous stories, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. This persona is likely to be drawn to movies that offer fantasy elements, thought-provoking themes, and engaging narratives.\n\nBased on the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, this movie aligns well with the human's preferences for fantasy, adventure, and thought-provoking themes. The movie's exploration of societal expectations, self-acceptance, and empowerment is likely to resonate with someone who enjoys imaginative and meaningful stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Barbie\" an 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories.\n\nThe persona I have built for the human is someone who enjoys deep and thought-provoking narratives, as well as imaginative and adventurous stories. They appreciate vibrant colors like blue, have a fondness for dogs, enjoy comforting meals like pasta, and like to relax with a cup of coffee.\n\nConsidering the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a complex and intriguing exploration of historical events and characters, which could still be engaging for someone who appreciates well-crafted stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nHuman: \n         =====================================\n        === START MOVIE PLOT SUMMARY FOR The Notebook ===\n        Overview:\nThe Notebook is a 2004 American romantic drama film directed by Nick Cassavetes, from a screenplay by Jeremy Leven and Jan Sardi, and based on the 1996 novel of the same name by Nicholas Sparks. The film stars Ryan Gosling and Rachel McAdams as a young couple who fall in love in the 1940s. Their story is read from a notebook in the present day by an elderly man, telling the tale to a fellow nursing home resident.\nThe Notebook had its world premiere at the Seattle International Film Festival on May 20, 2004, and was theatrically released in the United States on June 25, 2004. Despite generally mixed reviews from critics, Gosling and McAdams were singled out for praise for their performances. The film was a sleeper hit at the box office, grossing $117 million against its $29 million budget, and has become a cult classic in the years since its release. On November 11, 2012, an extended version premiered on ABC Family with deleted scenes added back into the original storyline.The film earned several accolades, including the MTV Movie Award for Best Kiss for Gosling and McAdams at the 2005 MTV Movie Awards. At the 11th Screen Actors Guild Awards, James Garner was nominated for Outstanding Performance by a Male Actor in a Supporting Role and Gena Rowlands won Best Supporting Actress \u2013 Drama at the 9th Golden Satellite Awards.\nPlot:\nAt a modern-day nursing home, the elderly Duke reads a romantic story from a notebook to a female patient:\nIn 1940, at a carnival in Seabrook Island, South Carolina, lumber mill worker Noah Calhoun sees 17-year-old heiress Allison \"Allie\" Hamilton, there for the summer. He pursues her and they begin a romance.\nWhen Allie meets Noah's father Frank Calhoun, he immediately likes her. However, when Noah meets her parents, they disapprove. That night, Noah takes Allie to the abandoned Windsor Plantation, telling her he will buy and restore it. As the sun sets, they start to make love for the first time. Noah's friend Fin interrupts to warn them Allie's parents sent the police out looking for her.\nWhen Allie and Noah return to her parents' mansion, Allie's mother Anne makes it clear they are against the relationship and forbid her from seeing him. Noah leaves and Allie follows, they argue and she impulsively breaks up with him, but immediately regrets it.\nThe next morning, Anne announces they will be returning to Charleston immediately. Allie seeks Noah to apologize but is unsuccessful, so asks Fin to tell him she loves him. Noah rushes to Allie's when he hears but is too late.\nNoah writes Allie every day for a year, but Allie's mother intercepts the letters. When all 365 go unanswered, he stops writing to move on. He and Fin enlist and fight in the Battle of the Bulge where Fin is killed. Allie nurses wounded soldiers in a hospital, meeting Captain Lon Hammond Jr., a young lawyer who comes from old Southern money. After a few years, they become engaged, to Allie's parents' delight.\nNoah returns from the war and finds his father has sold their home so Noah can buy The Windsor Plantation. He believes that if he restores it, Allie will return to him. Once it's completed, Noah resists selling it. As Allie tries on her wedding dress, she sees a newspaper photo of Noah in front of the renovated house and faints.\nAllie's feelings for Noah come rushing back, so she asks Lon to take a solo trip before the wedding. Returning to Seabrook, she finds Noah living in their dream house. They rekindle and consummate their relationship. Noah tells Allie about the 365 letters and they realize her mother kept them from her.\nDays later, Anne appears to warn Allie that Lon has come to Seabrook. She also reveals that she also once loved a lower-class young man in town and still wonders how changed their lives would have been if she had chosen differently. She gives Allie Noah's letters, suggesting she choose wisely.\nNoah and Allie argue and he tells her to decide what she wants, not what everyone else does. She drives back to her hotel, sobbing and confused, and confesses her infidelity to Lon. He still wants her back, but she follows her heart and returns to Noah.\nIn the present, the elderly woman is revealed to be Allie, now suffering from dementia. Duke is actually Noah, who uses a pseudonym to not startle her in her disoriented state. The journal he reads to her she wrote during the early stages of her illness, detailing their romance and life together so he could help her come back to him. Noah has kept the promise to read it to her almost daily.\nAlmost at the end of the journal in the notebook, Allie asks Noah what happened at the end of the story and Noah prompts her that she knows. She briefly recognizes him and remembers. Allie asks how long they have before she forgets again and Duke tells her possibly five minutes. They dance to their song, \"I'll Be Seeing You\", and she asks about their kids.\nHowever, Allie's dementia quickly relapses and she panics to see a stranger touching her, so is sedated. Duke has a heart attack and is treated in the nursing home while Allie is taken to the dementia ward. Upon recovering and despite not being permitted, Duke sneaks into Allie's room in the night. She instantly recognizes him, they kiss, and fall asleep holding hands. They are found in the morning, having died peacefully in each other's arms.\n        === END MOVIE PLOT SUMMARY ===\n        =====================================\n        \n        RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:\n        AI will provide a highly personalized rating based only on the movie plot summary human provided \n        and human answers to questions included with the context. \n        AI should be very sensible to human personal preferences captured in the answers to personal questions, \n        and should not be influenced by anything else.\n        AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.\n        OUTPUT FORMAT:\n        First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.\n        Explain how human preferences captured in the answers to personal questions influenced creation of this persona.\n        In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.\n        Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.\n        Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.\n        Rating will range from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it. \n        You will include a logical explanation for your rating based on human persona you've build and human responses to questions.\n        YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE The Notebook is \" FOLLOWED BY THE RATING.\n        FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.\n    \nAI:\n\n\n&gt; Finished chain.\nBased on the personal questions answered by the human, it is evident that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. The persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee.\n\nConsidering the plot summary of \"The Notebook,\" which is a romantic drama film about a couple's enduring love story, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a deep and emotional exploration of relationships and the passage of time, which could still be engaging for someone who appreciates heartfelt and touching stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"The Notebook\" a 70 out of 100. While it may not be a perfect match for their usual preferences, the film's emotional depth and romantic themes could still make it a compelling watch for them.\n\nRATING FOR MOVIE The Notebook is 70\nMovie: Dumb Money\n\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n[SystemMessage(content='The human answered 6 personal questions and provided a detailed plot summary of the movie \"Barbie.\" Based on the human\\'s preferences for fantasy movies, the color blue, dogs, pasta, and coffee, as well as their favorite movie being \"The Lord of the Rings,\" the AI created a persona for the human. The persona enjoys imaginative and adventurous stories, vibrant colors, dogs, comforting meals, and coffee. Considering this persona, the AI rated the movie \"Barbie\" a 85 out of 100, as it aligns well with the human\\'s preferences for fantasy, adventure, and thought-provoking themes. Based on the plot summary of \"Oppenheimer,\" an epic biographical thriller film, the AI created a persona for the human who appreciates deep and thought-provoking narratives. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 75 out of 100, considering its historical significance and complex characters. The AI provided a detailed explanation for the rating based on the human\\'s persona and preferences. After providing a plot summary for \"The Notebook,\" a romantic drama film, the AI created a persona for the human who appreciates heartfelt and touching stories. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 70 out of 100, considering its emotional depth and romantic themes that could still be engaging for the human.')]\nPersonal Questions and Answers:\nHuman: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\nAI: Based on the personal questions answered by the human, it seems that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. They also mentioned that their favorite movie is \"The Lord of the Rings.\" \n\nConsidering these preferences, the persona I have built for the human is someone who enjoys imaginative and adventurous stories, appreciates vibrant and calming colors like blue, has a fondness for dogs, enjoys comforting and hearty meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, I believe this movie aligns well with the human's preferences. The movie combines elements of fantasy, adventure, and social commentary, which are likely to appeal to someone who enjoys the fantasy genre and thought-provoking themes. \n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Barbie\" a 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. \n\nThe persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant and calming colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's preferences for fantasy and adventure. However, the film does offer a deep and thought-provoking exploration of historical events and complex characters, which could still be engaging for someone who appreciates intriguing and well-crafted stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: The human's persona is someone who enjoys imaginative and adventurous stories, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. This persona is likely to be drawn to movies that offer fantasy elements, thought-provoking themes, and engaging narratives.\n\nBased on the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, this movie aligns well with the human's preferences for fantasy, adventure, and thought-provoking themes. The movie's exploration of societal expectations, self-acceptance, and empowerment is likely to resonate with someone who enjoys imaginative and meaningful stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Barbie\" an 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories.\n\nThe persona I have built for the human is someone who enjoys deep and thought-provoking narratives, as well as imaginative and adventurous stories. They appreciate vibrant colors like blue, have a fondness for dogs, enjoy comforting meals like pasta, and like to relax with a cup of coffee.\n\nConsidering the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a complex and intriguing exploration of historical events and characters, which could still be engaging for someone who appreciates well-crafted stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: Based on the personal questions answered by the human, it is evident that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. The persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee.\n\nConsidering the plot summary of \"The Notebook,\" which is a romantic drama film about a couple's enduring love story, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a deep and emotional exploration of relationships and the passage of time, which could still be engaging for someone who appreciates heartfelt and touching stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"The Notebook\" a 70 out of 100. While it may not be a perfect match for their usual preferences, the film's emotional depth and romantic themes could still make it a compelling watch for them.\n\nRATING FOR MOVIE The Notebook is 70\nHuman: \n         =====================================\n        === START MOVIE PLOT SUMMARY FOR Dumb Money ===\n        Overview:\nDumb Money is a 2023 American biographical comedy-drama film directed by Craig Gillespie and written by Lauren Schuker Blum and Rebecca Angelo. It is based on the 2021 book The Antisocial Network by Ben Mezrich and chronicles the GameStop short squeeze of January 2021. The film features an ensemble cast that includes Paul Dano, Pete Davidson, Vincent D'Onofrio, America Ferrera, Nick Offerman, Anthony Ramos, Sebastian Stan, Shailene Woodley, and Seth Rogen.\nAfter being filmed in New Jersey from October to November of 2022, Dumb Money premiered at the Toronto International Film Festival on September 8, 2023. It was released in the United States by Sony Pictures Releasing in select theaters on September 15, 2023, and wide release on September 29, 2023. It received generally positive reviews from critics and grossed $20 million worldwide.\nPlot:\nKeith Gill is a middle class man working as a financial analyst in Brockton, Massachusetts. During his spare time, he regularly frequents the stock market subreddit r/WallStreetBets, posting his opinions on it via YouTube live streams under the name Roaring Kitty. He struggles to provide for his family, and his YouTube work is constantly mocked by his brother Kevin as nerdy garbage.\nIn July 2020, at the height of the COVID-19 pandemic, Keith notices that video game retailer GameStop's stock is falling and sinks his life savings into buying stock in it, regularly live streaming updates with his viewers. Despite Kevin and several peers claiming this is a waste of time, by January 2021, activity on r/WallStreetBets reveals that several hedge fund investment firms, including Melvin Capital Management and its founder Gabe Plotkin, have been short selling stock in the chain on the assumption it would close, causing a mass increase in GameStop's overall stock price when online stock buyers, including struggling nurse Jennifer, GameStop retail employee Marcos, and lesbian University of Texas at Austin couple Riri and Harmony, start aggressively buying stock, causing Plotkin and other investment CEOs to lose hundreds of millions within the same timeframe and Keith to be heralded as a financial guru.\nThings take a turn when r/WallStreetBets is temporarily shut down for \"inflammatory and vulgar content\", causing a mass surge of panic selling in GameStop's stock in an attempt to beat a perceived price drop. When the commission-free stock trading website Robinhood is unable to adequately pay the money for the sales, co-chairman Vlad Tenev, at the behest of Citadel LLC owner Ken Griffin, halts all purchasing of GameStop's stock in an attempt to drive down the price. The play ultimately works, but the subsequent negative backlash results in an investigation by the United States House Committee on Financial Services, with Tenev, Griffin, Plotkin, and Keith all being subpoenaed, the former three for their roles in the fiasco and the latter on suspicion of using the situation to trick the public into making himself rich. As the investors struggle to defend their actions, Keith adamantly denies any wrongdoing, stating he was only doing what anyone with a passing awareness of investment banking would do in that situation.\nIn the aftermath, post text shows how several of the individuals were affected: Plotkin was forced to shut down Melvin Capital because of the net losses the incident caused; Robinhood was the target of several lawsuits following the fiasco and wound up starting in the stock market significantly lower than it was prior; Harmony was able to use the money she obtained to pay off her family's debt issues and continues her relationship with Riri; Marcos sold half of his GameStop stock and quit his position in the company; Jennifer remains in debt but has retained her shareholding; Keith retired from YouTube in late April to get out of the public eye and sold part of his stocks to get Kevin an expensive car as a way to stop his nagging about how he will not loan him his car for his food deliveries.\n        === END MOVIE PLOT SUMMARY ===\n        =====================================\n        \n        RATING INSTRUCTIONS THAT MUST BE STRICTLY FOLLOWED:\n        AI will provide a highly personalized rating based only on the movie plot summary human provided \n        and human answers to questions included with the context. \n        AI should be very sensible to human personal preferences captured in the answers to personal questions, \n        and should not be influenced by anything else.\n        AI will also build a persona for human based on human answers to questions, and use this persona to rate the movie.\n        OUTPUT FORMAT:\n        First, include that persona you came up with in the explanation for the rating. Describe the persona in a few sentences.\n        Explain how human preferences captured in the answers to personal questions influenced creation of this persona.\n        In addition, consider other ratings for this human that you might have as they might give you more information about human's preferences.\n        Your goal is to provide a rating that is as close as possible to the rating human would give to this movie.\n        Remember that human has very limited time and wants to see something they will like, so your rating should be as accurate as possible.\n        Rating will range from 1 to 100, with 100 meaning human will love it, and 1 meaning human will hate it. \n        You will include a logical explanation for your rating based on human persona you've build and human responses to questions.\n        YOUR REVIEW MUST END WITH TEXT: \"RATING FOR MOVIE Dumb Money is \" FOLLOWED BY THE RATING.\n        FOLLOW THE INSTRUCTIONS STRICTLY, OTHERWISE HUMAN WILL NOT BE ABLE TO UNDERSTAND YOUR REVIEW.\n    \nAI:\n\n\n&gt; Finished chain.\nBased on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. The persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee.\n\nConsidering the plot summary of \"Dumb Money,\" which is a biographical comedy-drama film based on the GameStop short squeeze of January 2021, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a unique and intriguing look at real-life events and the impact of individual actions on a larger scale, which could still be engaging for someone who appreciates well-crafted stories with a touch of drama and comedy.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Dumb Money\" a 70 out of 100. While it may not be a perfect match for their usual preferences, the film's blend of comedy, drama, and real-world relevance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Dumb Money is 70\n</pre> <p>We've initialized our chain to run in a verbose mode, and we will see full text that gets sent to the LLM Note how the summary keeps updating after each movie is recommended. Finally, once AI has rated all the movies, let's ask for the final recommendation</p> In\u00a0[18]: Copied! <pre>final_recommendation = \"\"\"Now that AI has rated all the movies, AI will recommend human the one that human will like the most. \n                            AI will respond with movie recommendation, and short explanation for why human will like it over all other movies. \n                            AI will not include any ratings in your explanation, only the reasons why human will like it the most.\n                            However, the movie you will pick must be one of the movies you rated the highest.\n                            For example, if you rated one movie 65, and the other 60, you will recommend the movie with rating 65 because rating 65 \n                            is greate than rating of 60 .\"\"\"\n\n# run recommendation once more to get the final movie recommendation\nprediction = recommender.predict(input=final_recommendation)\nprint(prediction)\n</pre> final_recommendation = \"\"\"Now that AI has rated all the movies, AI will recommend human the one that human will like the most.                              AI will respond with movie recommendation, and short explanation for why human will like it over all other movies.                              AI will not include any ratings in your explanation, only the reasons why human will like it the most.                             However, the movie you will pick must be one of the movies you rated the highest.                             For example, if you rated one movie 65, and the other 60, you will recommend the movie with rating 65 because rating 65                              is greate than rating of 60 .\"\"\"  # run recommendation once more to get the final movie recommendation prediction = recommender.predict(input=final_recommendation) print(prediction) <pre>\n\n&gt; Entering new ConversationChain chain...\nPrompt after formatting:\n\nThe following is a friendly conversation between a human and an AI Movie Recommender. \nThe AI is follows human instructions and provides movie ratings for a human \nbased on the movie plot. \n\nSummary of Recommendations:\n[SystemMessage(content='The human answered 6 personal questions and provided a detailed plot summary of the movie \"Barbie.\" Based on the human\\'s preferences for fantasy movies, the color blue, dogs, pasta, and coffee, as well as their favorite movie being \"The Lord of the Rings,\" the AI created a persona for the human. The persona enjoys imaginative and adventurous stories, vibrant colors, dogs, comforting meals, and coffee. Considering this persona, the AI rated the movie \"Barbie\" a 85 out of 100, as it aligns well with the human\\'s preferences for fantasy, adventure, and thought-provoking themes. Based on the plot summary of \"Oppenheimer,\" an epic biographical thriller film, the AI created a persona for the human who appreciates deep and thought-provoking narratives. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 75 out of 100, considering its historical significance and complex characters. The AI provided a detailed explanation for the rating based on the human\\'s persona and preferences. After providing a plot summary for \"The Notebook,\" a romantic drama film, the AI created a persona for the human who appreciates heartfelt and touching stories. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 70 out of 100, considering its emotional depth and romantic themes that could still be engaging for the human. Based on the plot summary of \"Dumb Money,\" a biographical comedy-drama film based on the GameStop short squeeze of January 2021, the AI created a persona for the human who enjoys intriguing real-life events with a touch of drama and comedy. Despite not aligning perfectly with the human\\'s usual preferences for fantasy and adventure, the AI rated the movie a 70 out of 100, considering its unique storyline and blend of comedy and drama that could still be engaging for the human.')]\nPersonal Questions and Answers:\nHuman: You are AI that will recommend user a movie based on their answers \n    to personal questions. \n    Ask user 6 questions\nAI: Which movie genre you like the most?\nHuman: Fantasy\nAI: What is your favorite color?\nHuman: blue\nAI: What is your favorite movie?\nHuman: The lord of the rings\nAI: Pick one - dogs, cats or hamsters?\nHuman: dogs\nAI: What is your favorite food?\nHuman: pasta\nAI: What is your favorite drink?\nHuman: coffe\nAI: Now tell me a plot summary of a movie you're considering watching, \n    and specify how you want me to respond to you with the movie rating\nAI: Based on the personal questions answered by the human, it seems that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. They also mentioned that their favorite movie is \"The Lord of the Rings.\" \n\nConsidering these preferences, the persona I have built for the human is someone who enjoys imaginative and adventurous stories, appreciates vibrant and calming colors like blue, has a fondness for dogs, enjoys comforting and hearty meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, I believe this movie aligns well with the human's preferences. The movie combines elements of fantasy, adventure, and social commentary, which are likely to appeal to someone who enjoys the fantasy genre and thought-provoking themes. \n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Barbie\" a 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. \n\nThe persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant and calming colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. \n\nGiven the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's preferences for fantasy and adventure. However, the film does offer a deep and thought-provoking exploration of historical events and complex characters, which could still be engaging for someone who appreciates intriguing and well-crafted stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: The human's persona is someone who enjoys imaginative and adventurous stories, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee. This persona is likely to be drawn to movies that offer fantasy elements, thought-provoking themes, and engaging narratives.\n\nBased on the plot summary of \"Barbie,\" which involves a fantasy world, self-discovery, and a commentary on societal norms, this movie aligns well with the human's preferences for fantasy, adventure, and thought-provoking themes. The movie's exploration of societal expectations, self-acceptance, and empowerment is likely to resonate with someone who enjoys imaginative and meaningful stories.\n\nTherefore, considering the human's persona and preferences, I would rate the movie \"Barbie\" an 85 out of 100.\n\nRATING FOR MOVIE Barbie is 85\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories.\n\nThe persona I have built for the human is someone who enjoys deep and thought-provoking narratives, as well as imaginative and adventurous stories. They appreciate vibrant colors like blue, have a fondness for dogs, enjoy comforting meals like pasta, and like to relax with a cup of coffee.\n\nConsidering the plot summary of \"Oppenheimer,\" which is an epic biographical thriller film about J. Robert Oppenheimer's role in the development of the atomic bomb during World War II, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a complex and intriguing exploration of historical events and characters, which could still be engaging for someone who appreciates well-crafted stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Oppenheimer\" a 75 out of 100. While it may not be a perfect match for their usual preferences, the film's depth and historical significance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Oppenheimer is 75\nAI: Based on the personal questions answered by the human, it is evident that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. The persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee.\n\nConsidering the plot summary of \"The Notebook,\" which is a romantic drama film about a couple's enduring love story, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a deep and emotional exploration of relationships and the passage of time, which could still be engaging for someone who appreciates heartfelt and touching stories.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"The Notebook\" a 70 out of 100. While it may not be a perfect match for their usual preferences, the film's emotional depth and romantic themes could still make it a compelling watch for them.\n\nRATING FOR MOVIE The Notebook is 70\nAI: Based on the personal questions answered by the human, it is clear that they have a preference for fantasy movies, the color blue, dogs, pasta, and coffee. Their favorite movie is \"The Lord of the Rings,\" indicating a love for epic and adventurous stories. The persona I have built for the human is someone who enjoys imaginative and adventurous narratives, vibrant colors like blue, has a fondness for dogs, appreciates comforting meals like pasta, and likes to relax with a cup of coffee.\n\nConsidering the plot summary of \"Dumb Money,\" which is a biographical comedy-drama film based on the GameStop short squeeze of January 2021, it may not align perfectly with the human's usual preferences for fantasy and adventure. However, the film offers a unique and intriguing look at real-life events and the impact of individual actions on a larger scale, which could still be engaging for someone who appreciates well-crafted stories with a touch of drama and comedy.\n\nTherefore, based on the human's persona and their preferences, I would rate the movie \"Dumb Money\" a 70 out of 100. While it may not be a perfect match for their usual preferences, the film's blend of comedy, drama, and real-world relevance could still make it an interesting watch for them.\n\nRATING FOR MOVIE Dumb Money is 70\nHuman: Now that AI has rated all the movies, AI will recommend human the one that human will like the most. \n                            AI will respond with movie recommendation, and short explanation for why human will like it over all other movies. \n                            AI will not include any ratings in your explanation, only the reasons why human will like it the most.\n                            However, the movie you will pick must be one of the movies you rated the highest.\n                            For example, if you rated one movie 65, and the other 60, you will recommend the movie with rating 65 because rating 65 \n                            is greate than rating of 60 .\nAI:\n\n\n&gt; Finished chain.\nBased on the ratings provided for the movies, the one that I would recommend for you is \"Barbie.\" This movie aligns well with your preferences for fantasy, adventure, and thought-provoking themes. The imaginative and adventurous story, vibrant colors, and societal commentary in \"Barbie\" are likely to resonate with you based on your persona. Enjoy watching \"Barbie\" and immersing yourself in its fantasy world!\n</pre>"}, {"location": "generative_ai/tutorials/langchain_examples/#langchain-introduction", "title": "Langchain introduction\u00b6", "text": ""}, {"location": "generative_ai/tutorials/langchain_examples/#generate-textual-prompt-from-the-prompt-template", "title": "generate textual prompt from the prompt template\u00b6", "text": "<p>question = \"\"\" Review TVs provided in the context. Only use the reviews provided in this context, do not make up new reviews or use any existing information you know about these TVs. If there are no positive or negative reviews, output an empty JSON array. \"\"\" query = prompt.format(context = context, question = question)</p>"}, {"location": "generative_ai/tutorials/langchain_examples/#add-semantic-search-using-rag", "title": "Add semantic Search using RAG\u00b6", "text": ""}, {"location": "generative_ai/tutorials/langchain_examples/#memory", "title": "Memory\u00b6", "text": ""}, {"location": "generative_ai/tutorials/openai_functions/", "title": "Open AI call function", "text": "<p>In this exercise, you'll build a project management assistant using OpenAI API Function Calling A .csv file is used to simulate reading and writing from a database or project management tool API.  Follow the directions in the starter code below, and try to build the functions and function calling logic before you look at the solution on the next page!</p> <p>Import necessary libraries</p> In\u00a0[1]: Copied! <pre># Importing the library for OpenAI API\nfrom openai import OpenAI\nimport os\n\n# Define OpenAI API key\nclient = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n</pre> # Importing the library for OpenAI API from openai import OpenAI import os  # Define OpenAI API key client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\")) <p>First, define the Python functions that will read and write from the project_management.csv file using Pandas dataframes. This code uses Pandas dataframes to read and write from the .csv file.</p> <p>We define 3 tasks our project management assistant can perform.</p> <p>Each function returns a JSON string as output</p> In\u00a0[2]: Copied! <pre>import pandas as pd\nimport json\n</pre> import pandas as pd import json In\u00a0[62]: Copied! <pre>data_dict = {\n    \"Task ID\": {0: 1, 1: 2, 2: 3},\n    \"Task Name\": {\n        0: \"Design Database Schema\",\n        1: \"Implement Login Page\",\n        2: \"Prepare Project Report\",\n    },\n    \"Project ID\": {0: 101, 1: 101, 2: 102},\n    \"Assigned To\": {0: \"Jane Doe\", 1: \"John Smith\", 2: \"Alice Johnson\"},\n    \"Status\": {0: \"In Progress\", 1: \"completed\", 2: \"Completed\"},\n    \"Priority\": {0: \"High\", 1: \"Medium\", 2: \"Low\"},\n    \"Due Date\": {0: \"2023-08-01\", 1: \"2023-08-15\", 2: \"2023-07-15\"},\n    \"Date Created\": {0: \"2023-07-01\", 1: \"2023-07-01\", 2: \"2023-06-01\"},\n    \"Last Updated\": {0: \"2023-07-10\", 1: \"2022-01-05\", 2: \"2023-07-05\"},\n    \"Time Estimate\": {0: \"10\", 1: \"5\", 2: \"2\"},\n    \"Time Spent\": {0: 4, 1: None, 2: 2.0},\n    \"Description\": {\n        0: \"Create initial database schema for customer data\",\n        1: \"Develop the login page UI and backend\",\n        2: \"Compile the weekly project status report\",\n    },\n    \"Project Phase\": {0: \"Design\", 1: \"Implementation\", 2: \"Reporting\"},\n    \"Dependencies\": {0: None, 1: 1.0, 2: None},\n}\n\ndf = pd.DataFrame.from_dict(data_dict)\ndf\n</pre> data_dict = {     \"Task ID\": {0: 1, 1: 2, 2: 3},     \"Task Name\": {         0: \"Design Database Schema\",         1: \"Implement Login Page\",         2: \"Prepare Project Report\",     },     \"Project ID\": {0: 101, 1: 101, 2: 102},     \"Assigned To\": {0: \"Jane Doe\", 1: \"John Smith\", 2: \"Alice Johnson\"},     \"Status\": {0: \"In Progress\", 1: \"completed\", 2: \"Completed\"},     \"Priority\": {0: \"High\", 1: \"Medium\", 2: \"Low\"},     \"Due Date\": {0: \"2023-08-01\", 1: \"2023-08-15\", 2: \"2023-07-15\"},     \"Date Created\": {0: \"2023-07-01\", 1: \"2023-07-01\", 2: \"2023-06-01\"},     \"Last Updated\": {0: \"2023-07-10\", 1: \"2022-01-05\", 2: \"2023-07-05\"},     \"Time Estimate\": {0: \"10\", 1: \"5\", 2: \"2\"},     \"Time Spent\": {0: 4, 1: None, 2: 2.0},     \"Description\": {         0: \"Create initial database schema for customer data\",         1: \"Develop the login page UI and backend\",         2: \"Compile the weekly project status report\",     },     \"Project Phase\": {0: \"Design\", 1: \"Implementation\", 2: \"Reporting\"},     \"Dependencies\": {0: None, 1: 1.0, 2: None}, }  df = pd.DataFrame.from_dict(data_dict) df Out[62]: Task ID Task Name Project ID Assigned To Status Priority Due Date Date Created Last Updated Time Estimate Time Spent Description Project Phase Dependencies 0 1 Design Database Schema 101 Jane Doe In Progress High 2023-08-01 2023-07-01 2023-07-10 10 4.0 Create initial database schema for customer data Design NaN 1 2 Implement Login Page 101 John Smith completed Medium 2023-08-15 2023-07-01 2022-01-05 5 NaN Develop the login page UI and backend Implementation 1.0 2 3 Prepare Project Report 102 Alice Johnson Completed Low 2023-07-15 2023-06-01 2023-07-05 2 2.0 Compile the weekly project status report Reporting NaN In\u00a0[63]: Copied! <pre>def task_retrieval_and_status_updates(task_id, status, last_updated):\n\"\"\"Retrieve and update task status\"\"\"\n    df.loc[df[\"Task ID\"] == task_id, \"Status\"] = status\n    df.loc[df[\"Task ID\"] == task_id, \"Last Updated\"] = last_updated\n    df.to_csv(\"project_management.csv\", index=False)  # save changes to file\n    task = df.loc[df[\"Task ID\"] == task_id]\n    return json.dumps(task.to_dict())\n\n\ndef project_reporting_and_analytics(project_id):\n\"\"\"Generate reports on project progress and team performance\"\"\"\n    project = df.loc[df[\"Project ID\"] == project_id]\n    return json.dumps(project.to_dict())\n\n\ndef resource_allocation_and_scheduling(\n    task_id, assigned_to, time_estimate, due_date, status\n):\n\"\"\"Allocate tasks based on current workloads and schedules\"\"\"\n    df.loc[df[\"Task ID\"] == task_id, \"Assigned To\"] = assigned_to\n    df.loc[df[\"Task ID\"] == task_id, \"Time Estimate\"] = time_estimate\n    df.loc[df[\"Task ID\"] == task_id, \"Due Date\"] = due_date\n    df.loc[df[\"Task ID\"] == task_id, \"Status\"] = status\n    df.to_csv(\"project_management.csv\", index=False)  # save changes to file\n    task = df.loc[df[\"Task ID\"] == task_id]\n    return json.dumps(task.to_dict())\n</pre> def task_retrieval_and_status_updates(task_id, status, last_updated):     \"\"\"Retrieve and update task status\"\"\"     df.loc[df[\"Task ID\"] == task_id, \"Status\"] = status     df.loc[df[\"Task ID\"] == task_id, \"Last Updated\"] = last_updated     df.to_csv(\"project_management.csv\", index=False)  # save changes to file     task = df.loc[df[\"Task ID\"] == task_id]     return json.dumps(task.to_dict())   def project_reporting_and_analytics(project_id):     \"\"\"Generate reports on project progress and team performance\"\"\"     project = df.loc[df[\"Project ID\"] == project_id]     return json.dumps(project.to_dict())   def resource_allocation_and_scheduling(     task_id, assigned_to, time_estimate, due_date, status ):     \"\"\"Allocate tasks based on current workloads and schedules\"\"\"     df.loc[df[\"Task ID\"] == task_id, \"Assigned To\"] = assigned_to     df.loc[df[\"Task ID\"] == task_id, \"Time Estimate\"] = time_estimate     df.loc[df[\"Task ID\"] == task_id, \"Due Date\"] = due_date     df.loc[df[\"Task ID\"] == task_id, \"Status\"] = status     df.to_csv(\"project_management.csv\", index=False)  # save changes to file     task = df.loc[df[\"Task ID\"] == task_id]     return json.dumps(task.to_dict()) <p>Next, we'll build the project management assistant conversation.</p> <p>We'll define the messages to send to the model, including a tools dictionary that defines a list of tools, which are the functions that are available to the model to identify and parse parameters for.</p> In\u00a0[64]: Copied! <pre>def run_conversation(user_promt):\n    # messages is a list of initial conversation messages. The system message describes the role of the assistant. The second message is from the user, the user prompt\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a project management assistant with knowledge of project statuses, task assignments, and scheduling. You can provide updates on projects, assign tasks to team members, and schedule meetings. You understand project management terminology and are capable of parsing detailed project data. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": user_promt,\n        },  # this prompt should call the function\n    ]\n    # tools is a list of functions that the assistant can use. Each function is described by its name, description, and parameters.\n    tools = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"task_retrieval_and_status_updates\",\n                \"description\": \"Retrieve and update task status\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"task_id\": {\n                            \"type\": \"integer\",\n                            \"description\": \"The unique identifier for the task\",\n                        },\n                        \"status\": {\n                            \"type\": \"string\",\n                            \"description\": \"The new status of the task\",\n                        },\n                        \"last_updated\": {\n                            \"type\": \"string\",\n                            \"description\": \"The date of the last status update or change to the task\",\n                        },\n                    },\n                    \"required\": [\"task_id\", \"status\", \"last_updated\"],\n                },\n            },\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"project_reporting_and_analytics\",\n                \"description\": \"Generate reports on project progress and team performance\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"project_id\": {\n                            \"type\": \"integer\",\n                            \"description\": \"The unique identifier for the project\",\n                        }\n                    },\n                    \"required\": [\"project_id\"],\n                },\n            },\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"resource_allocation_and_scheduling\",\n                \"description\": \"Allocate tasks based on current workloads and schedules\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"task_id\": {\n                            \"type\": \"integer\",\n                            \"description\": \"The unique identifier for the task\",\n                        },\n                        \"assigned_to\": {\n                            \"type\": \"string\",\n                            \"description\": \"The user ID or name of the person to whom the task is assigned\",\n                        },\n                        \"time_estimate\": {\n                            \"type\": \"integer\",\n                            \"description\": \"An estimate of the time required to complete the task\",\n                        },\n                        \"due_date\": {\n                            \"type\": \"string\",\n                            \"description\": \"The deadline for the task completion\",\n                        },\n                        \"status\": {\n                            \"type\": \"string\",\n                            \"description\": \"The current status of the task\",\n                        },\n                    },\n                    \"required\": [\n                        \"task_id\",\n                        \"assigned_to\",\n                        \"time_estimate\",\n                        \"due_date\",\n                        \"status\",\n                    ],\n                },\n            },\n        },\n    ]\n    # `openai.chat.completions.create()` is called to generate a response from the GPT-3 model. The model, messages, and tools are passed as arguments. The `tool_choice` is set to \"auto\", allowing the model to choose which tool (function) to use.\n    # Use openai.ChatCompletion.create for openai &lt; 1.0\n    # openai.chat.completions.create for openai &gt; 1.0\n    response = client.chat.completions.create(\n        model=\"gpt-3.5-turbo-1106\",\n        messages=messages,\n        tools=tools,\n        tool_choice=\"auto\",  # let the model decide which tool (function) to use\n    )\n    # response_message and tool_calls extract the first response message and any tool calls from the response.\n    response_message = response.choices[0].message\n    tool_calls = (\n        response_message.tool_calls\n    )  # get the tool calls from the first response\n    print(tool_calls)\n    # end of first response, now we parse the response and call the functions the model identified from our tool list\n    # check if the model wanted to call a function\n    if tool_calls:\n        # list the available functions and their corresponding python functions\n        available_functions = {\n            \"task_retrieval_and_status_updates\": task_retrieval_and_status_updates,\n            \"project_reporting_and_analytics\": project_reporting_and_analytics,\n            \"resource_allocation_and_scheduling\": resource_allocation_and_scheduling,\n        }\n        messages.append(\n            response_message\n        )  # extend the conversation with the first response\n        # send the info for each function call and function response to the model\n        for tool_call in tool_calls:  # iterate through the tool calls in the response\n            function_name = (\n                tool_call.function.name\n            )  # get the name of the function to call\n            function_to_call = available_functions[function_name]\n            function_args = json.loads(\n                tool_call.function.arguments\n            )  # converting the arguments of the function call from a JSON formatted string into a Python dictionary.\n            if function_name == \"task_retrieval_and_status_updates\":\n                function_response = function_to_call(  # call the function with the arguments. The result of the function call is stored in function_response\n                    task_id=function_args.get(\"task_id\"),\n                    status=function_args.get(\"status\"),\n                    last_updated=function_args.get(\"last_updated\"),\n                )\n            elif function_name == \"project_reporting_and_analytics\":\n                function_response = function_to_call(\n                    project_id=function_args.get(\"project_id\")\n                )\n            elif function_name == \"resource_allocation_and_scheduling\":\n                function_response = function_to_call(\n                    task_id=function_args.get(\"task_id\"),\n                    assigned_to=function_args.get(\"assigned_to\"),\n                    time_estimate=function_args.get(\"time_estimate\"),\n                    due_date=function_args.get(\"due_date\"),\n                    status=function_args.get(\"status\"),\n                )\n\n            message_to_append = {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": function_response,  # send the function response to the model, it's the JSON string of the function response\n            }\n            messages.append(\n                message_to_append\n            )  # extend conversation with function response\n\n        # See https://gist.github.com/gaborcselle/2dc076eae23bd219ff707b954c890cd7\n        # messages[1].content = \"\" # clear the first message (parsing bug)\n        # messages[1][\"content\"] = \"\"  # clear the first message (parsing bug)\n        print(\"\\n: Mesagges\")\n        print(messages)\n        second_response = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-1106\",\n            messages=messages,\n        )  # get a new response from the model where it can see the function response\n        return second_response\n</pre> def run_conversation(user_promt):     # messages is a list of initial conversation messages. The system message describes the role of the assistant. The second message is from the user, the user prompt     messages = [         {             \"role\": \"system\",             \"content\": \"You are a project management assistant with knowledge of project statuses, task assignments, and scheduling. You can provide updates on projects, assign tasks to team members, and schedule meetings. You understand project management terminology and are capable of parsing detailed project data. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\",         },         {             \"role\": \"user\",             \"content\": user_promt,         },  # this prompt should call the function     ]     # tools is a list of functions that the assistant can use. Each function is described by its name, description, and parameters.     tools = [         {             \"type\": \"function\",             \"function\": {                 \"name\": \"task_retrieval_and_status_updates\",                 \"description\": \"Retrieve and update task status\",                 \"parameters\": {                     \"type\": \"object\",                     \"properties\": {                         \"task_id\": {                             \"type\": \"integer\",                             \"description\": \"The unique identifier for the task\",                         },                         \"status\": {                             \"type\": \"string\",                             \"description\": \"The new status of the task\",                         },                         \"last_updated\": {                             \"type\": \"string\",                             \"description\": \"The date of the last status update or change to the task\",                         },                     },                     \"required\": [\"task_id\", \"status\", \"last_updated\"],                 },             },         },         {             \"type\": \"function\",             \"function\": {                 \"name\": \"project_reporting_and_analytics\",                 \"description\": \"Generate reports on project progress and team performance\",                 \"parameters\": {                     \"type\": \"object\",                     \"properties\": {                         \"project_id\": {                             \"type\": \"integer\",                             \"description\": \"The unique identifier for the project\",                         }                     },                     \"required\": [\"project_id\"],                 },             },         },         {             \"type\": \"function\",             \"function\": {                 \"name\": \"resource_allocation_and_scheduling\",                 \"description\": \"Allocate tasks based on current workloads and schedules\",                 \"parameters\": {                     \"type\": \"object\",                     \"properties\": {                         \"task_id\": {                             \"type\": \"integer\",                             \"description\": \"The unique identifier for the task\",                         },                         \"assigned_to\": {                             \"type\": \"string\",                             \"description\": \"The user ID or name of the person to whom the task is assigned\",                         },                         \"time_estimate\": {                             \"type\": \"integer\",                             \"description\": \"An estimate of the time required to complete the task\",                         },                         \"due_date\": {                             \"type\": \"string\",                             \"description\": \"The deadline for the task completion\",                         },                         \"status\": {                             \"type\": \"string\",                             \"description\": \"The current status of the task\",                         },                     },                     \"required\": [                         \"task_id\",                         \"assigned_to\",                         \"time_estimate\",                         \"due_date\",                         \"status\",                     ],                 },             },         },     ]     # `openai.chat.completions.create()` is called to generate a response from the GPT-3 model. The model, messages, and tools are passed as arguments. The `tool_choice` is set to \"auto\", allowing the model to choose which tool (function) to use.     # Use openai.ChatCompletion.create for openai &lt; 1.0     # openai.chat.completions.create for openai &gt; 1.0     response = client.chat.completions.create(         model=\"gpt-3.5-turbo-1106\",         messages=messages,         tools=tools,         tool_choice=\"auto\",  # let the model decide which tool (function) to use     )     # response_message and tool_calls extract the first response message and any tool calls from the response.     response_message = response.choices[0].message     tool_calls = (         response_message.tool_calls     )  # get the tool calls from the first response     print(tool_calls)     # end of first response, now we parse the response and call the functions the model identified from our tool list     # check if the model wanted to call a function     if tool_calls:         # list the available functions and their corresponding python functions         available_functions = {             \"task_retrieval_and_status_updates\": task_retrieval_and_status_updates,             \"project_reporting_and_analytics\": project_reporting_and_analytics,             \"resource_allocation_and_scheduling\": resource_allocation_and_scheduling,         }         messages.append(             response_message         )  # extend the conversation with the first response         # send the info for each function call and function response to the model         for tool_call in tool_calls:  # iterate through the tool calls in the response             function_name = (                 tool_call.function.name             )  # get the name of the function to call             function_to_call = available_functions[function_name]             function_args = json.loads(                 tool_call.function.arguments             )  # converting the arguments of the function call from a JSON formatted string into a Python dictionary.             if function_name == \"task_retrieval_and_status_updates\":                 function_response = function_to_call(  # call the function with the arguments. The result of the function call is stored in function_response                     task_id=function_args.get(\"task_id\"),                     status=function_args.get(\"status\"),                     last_updated=function_args.get(\"last_updated\"),                 )             elif function_name == \"project_reporting_and_analytics\":                 function_response = function_to_call(                     project_id=function_args.get(\"project_id\")                 )             elif function_name == \"resource_allocation_and_scheduling\":                 function_response = function_to_call(                     task_id=function_args.get(\"task_id\"),                     assigned_to=function_args.get(\"assigned_to\"),                     time_estimate=function_args.get(\"time_estimate\"),                     due_date=function_args.get(\"due_date\"),                     status=function_args.get(\"status\"),                 )              message_to_append = {                 \"tool_call_id\": tool_call.id,                 \"role\": \"tool\",                 \"name\": function_name,                 \"content\": function_response,  # send the function response to the model, it's the JSON string of the function response             }             messages.append(                 message_to_append             )  # extend conversation with function response          # See https://gist.github.com/gaborcselle/2dc076eae23bd219ff707b954c890cd7         # messages[1].content = \"\" # clear the first message (parsing bug)         # messages[1][\"content\"] = \"\"  # clear the first message (parsing bug)         print(\"\\n: Mesagges\")         print(messages)         second_response = client.chat.completions.create(             model=\"gpt-3.5-turbo-1106\",             messages=messages,         )  # get a new response from the model where it can see the function response         return second_response In\u00a0[71]: Copied! <pre>user_promt = (\n    \"Update the status of the task with id: 1, to: completed. Today is 29-02-2024\"\n)\nresponse = run_conversation(user_promt=user_promt)\n</pre> user_promt = (     \"Update the status of the task with id: 1, to: completed. Today is 29-02-2024\" ) response = run_conversation(user_promt=user_promt) <pre>[ChatCompletionMessageToolCall(id='call_hn3NiNUMnFIAbV0yGUdA5dgS', function=Function(arguments='{\"task_id\":1,\"status\":\"completed\",\"last_updated\":\"29-02-2024\"}', name='task_retrieval_and_status_updates'), type='function')]\n\n: Mesagges\n[{'role': 'system', 'content': \"You are a project management assistant with knowledge of project statuses, task assignments, and scheduling. You can provide updates on projects, assign tasks to team members, and schedule meetings. You understand project management terminology and are capable of parsing detailed project data. Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'Update the status of the task with id: 1, to: completed. Today is 29-02-2024'}, ChatCompletionMessage(content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionMessageToolCall(id='call_hn3NiNUMnFIAbV0yGUdA5dgS', function=Function(arguments='{\"task_id\":1,\"status\":\"completed\",\"last_updated\":\"29-02-2024\"}', name='task_retrieval_and_status_updates'), type='function')]), {'tool_call_id': 'call_hn3NiNUMnFIAbV0yGUdA5dgS', 'role': 'tool', 'name': 'task_retrieval_and_status_updates', 'content': '{\"Task ID\": {\"0\": 1}, \"Task Name\": {\"0\": \"Design Database Schema\"}, \"Project ID\": {\"0\": 101}, \"Assigned To\": {\"0\": \"Jane Doe\"}, \"Status\": {\"0\": \"completed\"}, \"Priority\": {\"0\": \"High\"}, \"Due Date\": {\"0\": \"2023-08-01\"}, \"Date Created\": {\"0\": \"2023-07-01\"}, \"Last Updated\": {\"0\": \"29-02-2024\"}, \"Time Estimate\": {\"0\": \"10\"}, \"Time Spent\": {\"0\": 4.0}, \"Description\": {\"0\": \"Create initial database schema for customer data\"}, \"Project Phase\": {\"0\": \"Design\"}, \"Dependencies\": {\"0\": NaN}}'}]\n</pre> In\u00a0[72]: Copied! <pre>response\n</pre> response Out[72]: <pre>ChatCompletion(id='chatcmpl-8xZETXppBTKNbbE90OVKI7f0CXAMb', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Task with ID 1, \"Design Database Schema,\" has been updated to status: completed, as of today, 29-02-2024.', role='assistant', function_call=None, tool_calls=None))], created=1709207477, model='gpt-3.5-turbo-1106', object='chat.completion', system_fingerprint='fp_b3e55cb931', usage=CompletionUsage(completion_tokens=31, prompt_tokens=316, total_tokens=347))</pre> In\u00a0[73]: Copied! <pre>df\n</pre> df Out[73]: Task ID Task Name Project ID Assigned To Status Priority Due Date Date Created Last Updated Time Estimate Time Spent Description Project Phase Dependencies 0 1 Design Database Schema 101 Jane Doe completed High 2023-08-01 2023-07-01 29-02-2024 10 4.0 Create initial database schema for customer data Design NaN 1 2 Implement Login Page 101 John Smith completed Medium 2023-08-15 2023-07-01 2022-01-05 5 NaN Develop the login page UI and backend Implementation 1.0 2 3 Prepare Project Report 102 Alice Johnson Completed Low 2023-07-15 2023-06-01 2023-07-05 2 2.0 Compile the weekly project status report Reporting NaN"}, {"location": "generative_ai/tutorials/pytorch_intro/", "title": "Pytorch", "text": "<p>PyTorch is a dynamic and powerful tool for building and training machine learning models. It simplifies the process with its fundamental building blocks like tensors and neural networks and offers effective ways to define objectives and improve models using loss functions and optimizers. By leveraging PyTorch, anyone can gain the skills to work with large amounts of data and develop cutting-edge AI applications.</p> <p>PyTorch is a dynamic and powerful tool for building and training machine learning models. It simplifies the process with its fundamental building blocks like tensors and neural networks and offers effective ways to define objectives and improve models using loss functions and optimizers. By leveraging PyTorch, anyone can gain the skills to work with large amounts of data and develop cutting-edge AI applications.</p> <ul> <li><p>Tensors: Generalized versions of vectors and matrices that can have any number of dimensions (i.e. multi-dimensional arrays). They hold data for processing with operations like addition or multiplication.</p> </li> <li><p>Matrix operations: Calculations involving matrices, which are two-dimensional arrays, like adding two matrices together or multiplying them.</p> </li> <li><p>Scalar values: Single numbers or quantities that only have magnitude, not direction (for example, the number 7 or 3.14).</p> </li> </ul> In\u00a0[1]: Copied! <pre>import torch\nimport matplotlib.pyplot as plt\n</pre> import torch import matplotlib.pyplot as plt In\u00a0[2]: Copied! <pre># Create a 3-dimensional tensor\nimages = torch.rand((4, 28, 28))\n\n# Get the second image\nsecond_image = images[1]\n</pre> # Create a 3-dimensional tensor images = torch.rand((4, 28, 28))  # Get the second image second_image = images[1] In\u00a0[5]: Copied! <pre>plt.imshow(second_image, cmap=\"gray\")\nplt.show()\n</pre> plt.imshow(second_image, cmap=\"gray\") plt.show() In\u00a0[6]: Copied! <pre>a = torch.tensor([[1, 1], [1, 0]])\n\nprint(a)\n\nprint(torch.matrix_power(a, 2))\n\nprint(torch.matrix_power(a, 3))\n\nprint(torch.matrix_power(a, 4))\n</pre> a = torch.tensor([[1, 1], [1, 0]])  print(a)  print(torch.matrix_power(a, 2))  print(torch.matrix_power(a, 3))  print(torch.matrix_power(a, 4)) <pre>tensor([[1, 1],\n        [1, 0]])\ntensor([[2, 1],\n        [1, 1]])\ntensor([[3, 2],\n        [2, 1]])\ntensor([[5, 3],\n        [3, 2]])\n</pre> <ul> <li>Tensors tutorial</li> <li>Tensors documentation</li> </ul> <p>PyTorch offers powerful features to create and interlink neural networks, which are key elements in understanding modern artificial intelligence. We explored creating a multi-layer perceptron using PyTorch's nn.Module class and then passed a tensor into it and received the output.</p> In\u00a0[7]: Copied! <pre>import torch.nn as nn\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_size):\n        super(MLP, self).__init__()\n        self.hidden_layer = nn.Linear(input_size, 64)\n        self.output_layer = nn.Linear(64, 2)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.activation(self.hidden_layer(x))\n        return self.output_layer(x)\n\n\nmodel = MLP(input_size=10)\nprint(model)\n</pre> import torch.nn as nn   class MLP(nn.Module):     def __init__(self, input_size):         super(MLP, self).__init__()         self.hidden_layer = nn.Linear(input_size, 64)         self.output_layer = nn.Linear(64, 2)         self.activation = nn.ReLU()      def forward(self, x):         x = self.activation(self.hidden_layer(x))         return self.output_layer(x)   model = MLP(input_size=10) print(model) <pre>MLP(\n  (hidden_layer): Linear(in_features=10, out_features=64, bias=True)\n  (output_layer): Linear(in_features=64, out_features=2, bias=True)\n  (activation): ReLU()\n)\n</pre> In\u00a0[8]: Copied! <pre>model.forward(torch.rand(10))\n</pre> model.forward(torch.rand(10)) Out[8]: <pre>tensor([-0.0064, -0.1078], grad_fn=&lt;ViewBackward0&gt;)</pre> <ul> <li><p>PyTorch nn tutorial</p> </li> <li><p>PyTorch nn documentation</p> </li> <li><p>torch.nn.Module documentation</p> </li> <li><p>torch.nn.Linear documentation</p> </li> <li><p>torch.nn.ReLU documentation</p> </li> </ul> <p>PyTorch loss functions are essential tools that help in improving the accuracy of a model by measuring errors. These functions come in different forms to tackle various problems, like deciding between categories (classification) or predicting values (regression). Understanding and using these functions correctly is key to making smart, effective models that do a great job at the tasks they're designed for!</p> <ul> <li><p>Loss functions: They measure how well a model is performing by calculating the difference between the model's predictions and the actual results.</p> </li> <li><p>Cross entropy loss: This is a measure used when a model needs to choose between categories (like whether an image shows a cat or a dog), and it shows how well the model's predictions align with the actual categories.</p> </li> <li><p>Mean squared error: This shows the average of the squares of the differences between predicted numbers (like a predicted price) and the actual numbers. It's often used for predicting continuous values rather than categories.</p> </li> </ul> In\u00a0[13]: Copied! <pre>import torch\nimport torch.nn as nn\n\nloss_function = nn.CrossEntropyLoss()\n\n# Our dataset contains a single image of a dog, where\n# cat = 0 and dog = 1 (corresponding to index 0 and 1) [Cat, Dog]\ntarget_tensor = torch.tensor([1])\ntarget_tensor\n# Prediction: Most likely a dog (index 1 is higher)\n</pre> import torch import torch.nn as nn  loss_function = nn.CrossEntropyLoss()  # Our dataset contains a single image of a dog, where # cat = 0 and dog = 1 (corresponding to index 0 and 1) [Cat, Dog] target_tensor = torch.tensor([1]) target_tensor # Prediction: Most likely a dog (index 1 is higher) Out[13]: <pre>tensor([1])</pre> In\u00a0[15]: Copied! <pre># Note that the values do not need to sum to 1\npredicted_tensor = torch.tensor([[2.0, 5.0]]) # two classes, the second larger == more likely\nloss_value = loss_function(predicted_tensor, target_tensor)\nloss_value\n# Prediction: Most likely a dog (index 1 is higher)\n</pre> # Note that the values do not need to sum to 1 predicted_tensor = torch.tensor([[2.0, 5.0]]) # two classes, the second larger == more likely loss_value = loss_function(predicted_tensor, target_tensor) loss_value # Prediction: Most likely a dog (index 1 is higher) Out[15]: <pre>tensor(0.0486)</pre> In\u00a0[16]: Copied! <pre>predicted_tensor = torch.tensor([[1.5, 1.1]])\nloss_value = loss_function(predicted_tensor, target_tensor)\nloss_value\n# Prediction: Slightly more likely a cat (index 0 is higher)\n</pre> predicted_tensor = torch.tensor([[1.5, 1.1]]) loss_value = loss_function(predicted_tensor, target_tensor) loss_value # Prediction: Slightly more likely a cat (index 0 is higher) Out[16]: <pre>tensor(0.9130)</pre> In\u00a0[17]: Copied! <pre># Define the loss function\nloss_function = nn.MSELoss()\n\n# Define the predicted and actual values as tensors\npredicted_tensor = torch.tensor([320000.0])\nactual_tensor = torch.tensor([300000.0])\n\n# Compute the MSE loss\nloss_value = loss_function(predicted_tensor, actual_tensor)\nprint(loss_value.item())  # Loss value: 20000 * 20000 / 1 = ...\n</pre> # Define the loss function loss_function = nn.MSELoss()  # Define the predicted and actual values as tensors predicted_tensor = torch.tensor([320000.0]) actual_tensor = torch.tensor([300000.0])  # Compute the MSE loss loss_value = loss_function(predicted_tensor, actual_tensor) print(loss_value.item())  # Loss value: 20000 * 20000 / 1 = ... <pre>400000000.0\n</pre> <p>PyTorch optimizers are important tools that help improve how a neural network learns from data by adjusting the model's parameters. By using these optimizers, like stochastic gradient descent (SGD) with momentum or Adam, we can quickly get started learning!</p> <ul> <li><p>Gradients: Directions and amounts by which a function increases most. The parameters can be changed in a direction opposite to the gradient of the loss function in order to reduce the loss.</p> </li> <li><p>Learning Rate: This hyperparameter specifies how big the steps are when adjusting the neural network's settings during training. Too big, and you might skip over the best setting; too small, and it'll take a very long time to get there.</p> </li> <li><p>Momentum: A technique that helps accelerate the optimizer in the right direction and dampens oscillations.</p> </li> </ul> In\u00a0[19]: Copied! <pre>import torch.optim as optim\n\n# momentum=0.9 smoothes out updates and can help training\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n</pre> import torch.optim as optim  # momentum=0.9 smoothes out updates and can help training optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) In\u00a0[20]: Copied! <pre>optimizer = optim.Adam(model.parameters(), lr=0.01)\n</pre> optimizer = optim.Adam(model.parameters(), lr=0.01) <p>PyTorch makes accessing data for your model a breeze! These tools ensure that the flow of information to our AI is just right, making its learning experience effective and fun.</p> <ul> <li><p>PyTorch Dataset class: This is like a recipe that tells your computer how to get the data it needs to learn from, including where to find it and how to parse it, if necessary.</p> </li> <li><p>PyTorch Data Loader: Think of this as a delivery truck that brings the data to your AI in small, manageable loads called batches; this makes it easier for the AI to process and learn from the data.</p> </li> <li><p>Batches: Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.</p> </li> <li><p>Shuffle: It means mixing up the data so that it's not in the same order every time, which helps the AI learn better.</p> </li> </ul> In\u00a0[21]: Copied! <pre>from torch.utils.data import Dataset\n\n# Create a toy dataset\nclass NumberProductDataset(Dataset):\n    def __init__(self, data_range=(1, 10)):\n        self.numbers = list(range(data_range[0], data_range[1]))\n\n    def __getitem__(self, index):\n        number1 = self.numbers[index]\n        number2 = self.numbers[index] + 1\n        return (number1, number2), number1 * number2\n\n    def __len__(self):\n        return len(self.numbers)\n\n# Instantiate the dataset\ndataset = NumberProductDataset(\n    data_range=(0, 11)\n)\n\n# Access a data sample\ndata_sample = dataset[3]\nprint(data_sample)\n</pre> from torch.utils.data import Dataset  # Create a toy dataset class NumberProductDataset(Dataset):     def __init__(self, data_range=(1, 10)):         self.numbers = list(range(data_range[0], data_range[1]))      def __getitem__(self, index):         number1 = self.numbers[index]         number2 = self.numbers[index] + 1         return (number1, number2), number1 * number2      def __len__(self):         return len(self.numbers)  # Instantiate the dataset dataset = NumberProductDataset(     data_range=(0, 11) )  # Access a data sample data_sample = dataset[3] print(data_sample)  <pre>((3, 4), 12)\n</pre> In\u00a0[23]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Instantiate the dataset\ndataset = NumberProductDataset(data_range=(0, 5))\n\n# Create a DataLoader instance\ndataloader = DataLoader(dataset, batch_size=3, shuffle=False)\n\n# Iterating over batches\nfor num_pairs, products in dataloader:\n    print(num_pairs, products)\n    # 3 * 4 = 12\n    # 2 * 3 = 6\n</pre> from torch.utils.data import DataLoader  # Instantiate the dataset dataset = NumberProductDataset(data_range=(0, 5))  # Create a DataLoader instance dataloader = DataLoader(dataset, batch_size=3, shuffle=False)  # Iterating over batches for num_pairs, products in dataloader:     print(num_pairs, products)     # 3 * 4 = 12     # 2 * 3 = 6 <pre>[tensor([0, 1, 2]), tensor([1, 2, 3])] tensor([0, 2, 6])\n[tensor([3, 4]), tensor([4, 5])] tensor([12, 20])\n</pre> <ul> <li><p>PyTorch Dataset documentation</p> </li> <li><p>PyTorch DataLoader documentation</p> </li> <li><p>Index of PyTorch data utilities</p> </li> </ul> <p>A PyTorch training loop is an essential part of building a neural network model, which helps us teach the computer how to make predictions or decisions based on data. By using this loop, we gradually improve our model's accuracy through a process of learning from its mistakes and adjusting.</p> <ul> <li><p>Training Loop: The cycle that a neural network goes through many times to learn from the data by making predictions, checking errors, and improving itself.</p> </li> <li><p>Batches: Batches are small, evenly divided parts of data that the AI looks at and learns from each step of the way.</p> </li> <li><p>Epochs: A complete pass through the entire training dataset. The more epochs, the more the computer goes over the material to learn.</p> </li> <li><p>Loss functions: They measure how well a model is performing by calculating the difference between the model's predictions and the actual results.</p> </li> <li><p>Optimizer: Part of the neural network's brain that makes decisions on how to change the network to get better at its job.</p> </li> </ul> In\u00a0[39]: Copied! <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass NumberSumDataset(Dataset):\n\"\"\"\n    This dataset has two features\u2014a pair of numbers\u2014and a \n    target value\u2014the sum of those two numbers.\n    \"\"\"\n\n    def __init__(self, data_range=(1, 10)):\n        self.numbers = list(range(data_range[0], data_range[1]))\n\n    def __getitem__(self, index):\n        number1 = float(self.numbers[index // len(self.numbers)])\n        number2 = float(self.numbers[index % len(self.numbers)])\n        return torch.tensor([number1, number2]), torch.tensor([number1 + number2])\n\n    def __len__(self):\n        return len(self.numbers) ** 2\n\n\ndataset = NumberSumDataset(data_range=(1, 4))\n\nfor i in range(9):\n    print(dataset[i])\n</pre> import torch import torch.nn as nn import torch.optim as optim from torch.utils.data import Dataset, DataLoader   class NumberSumDataset(Dataset):     \"\"\"     This dataset has two features\u2014a pair of numbers\u2014and a      target value\u2014the sum of those two numbers.     \"\"\"      def __init__(self, data_range=(1, 10)):         self.numbers = list(range(data_range[0], data_range[1]))      def __getitem__(self, index):         number1 = float(self.numbers[index // len(self.numbers)])         number2 = float(self.numbers[index % len(self.numbers)])         return torch.tensor([number1, number2]), torch.tensor([number1 + number2])      def __len__(self):         return len(self.numbers) ** 2   dataset = NumberSumDataset(data_range=(1, 4))  for i in range(9):     print(dataset[i]) <pre>(tensor([1., 1.]), tensor([2.]))\n(tensor([1., 2.]), tensor([3.]))\n(tensor([1., 3.]), tensor([4.]))\n(tensor([2., 1.]), tensor([3.]))\n(tensor([2., 2.]), tensor([4.]))\n(tensor([2., 3.]), tensor([5.]))\n(tensor([3., 1.]), tensor([4.]))\n(tensor([3., 2.]), tensor([5.]))\n(tensor([3., 3.]), tensor([6.]))\n</pre> In\u00a0[51]: Copied! <pre>class MLP(nn.Module):\n    def __init__(self, input_size):\n        super(MLP, self).__init__()\n        self.hidden_layer = nn.Linear(input_size, 128)\n        self.output_layer = nn.Linear(128, 1)\n        self.activation = nn.ReLU()\n\n    def forward(self, x):\n        x = self.activation(self.hidden_layer(x))\n        return self.output_layer(x)\n</pre> class MLP(nn.Module):     def __init__(self, input_size):         super(MLP, self).__init__()         self.hidden_layer = nn.Linear(input_size, 128)         self.output_layer = nn.Linear(128, 1)         self.activation = nn.ReLU()      def forward(self, x):         x = self.activation(self.hidden_layer(x))         return self.output_layer(x) In\u00a0[53]: Copied! <pre>dataset = NumberSumDataset(data_range=(0, 100))\ndataloader = DataLoader(dataset, batch_size=100, shuffle=True)\nmodel = MLP(input_size=2)\nloss_function = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n</pre> dataset = NumberSumDataset(data_range=(0, 100)) dataloader = DataLoader(dataset, batch_size=100, shuffle=True) model = MLP(input_size=2) loss_function = nn.MSELoss() optimizer = optim.Adam(model.parameters(), lr=0.001) In\u00a0[58]: Copied! <pre>for epoch in range(15):  # train for 15 epochs\n    loss = 0.0\n    for number_pairs, sums in dataloader:  # Iterate over the batches\n        predictions = model(number_pairs)  # Compute the model output\n        loss = loss_function(predictions, sums)  # Compute the loss\n        loss.backward()  # Perform backpropagation\n        optimizer.step()  # Update the parameters\n        optimizer.zero_grad()  # Zero the gradients\n\n        loss += loss.item()  # Add the loss for all batches\n\n    print(\"Epoch {}: Sum of Batch Losses = {:.5f}\".format(epoch, loss))\n</pre> for epoch in range(15):  # train for 15 epochs     loss = 0.0     for number_pairs, sums in dataloader:  # Iterate over the batches         predictions = model(number_pairs)  # Compute the model output         loss = loss_function(predictions, sums)  # Compute the loss         loss.backward()  # Perform backpropagation         optimizer.step()  # Update the parameters         optimizer.zero_grad()  # Zero the gradients          loss += loss.item()  # Add the loss for all batches      print(\"Epoch {}: Sum of Batch Losses = {:.5f}\".format(epoch, loss)) <pre>Epoch 0: Sum of Batch Losses = 0.01407\nEpoch 1: Sum of Batch Losses = 0.01828\nEpoch 2: Sum of Batch Losses = 0.01432\nEpoch 3: Sum of Batch Losses = 0.01933\nEpoch 4: Sum of Batch Losses = 0.01033\nEpoch 5: Sum of Batch Losses = 0.01055\nEpoch 6: Sum of Batch Losses = 0.01177\nEpoch 7: Sum of Batch Losses = 0.01076\nEpoch 8: Sum of Batch Losses = 0.00736\nEpoch 9: Sum of Batch Losses = 0.01118\nEpoch 10: Sum of Batch Losses = 0.00788\nEpoch 11: Sum of Batch Losses = 0.00741\nEpoch 12: Sum of Batch Losses = 0.00765\nEpoch 13: Sum of Batch Losses = 0.00764\nEpoch 14: Sum of Batch Losses = 0.00646\n</pre> In\u00a0[59]: Copied! <pre># Test the model on 3 + 7\nmodel(torch.tensor([3.0, 7.0]))\n</pre> # Test the model on 3 + 7 model(torch.tensor([3.0, 7.0])) Out[59]: <pre>tensor([9.9170], grad_fn=&lt;ViewBackward0&gt;)</pre>"}, {"location": "generative_ai/tutorials/pytorch_intro/#pytorch", "title": "Pytorch\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#tensors", "title": "Tensors\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#images-as-pytorch-tensors", "title": "Images as PyTorch Tensors\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#displaying-images", "title": "Displaying Images\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#matrix-multiplication", "title": "Matrix Multiplication\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#neural-networks", "title": "Neural Networks\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#loss-function", "title": "Loss function\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#cross-entropy-loss", "title": "Cross-Entropy Loss\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#mean-squared-error-loss", "title": "Mean Squared Error Loss\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#resources", "title": "Resources\u00b6", "text": "<ul> <li>Index of PyTorch loss functions</li> </ul>"}, {"location": "generative_ai/tutorials/pytorch_intro/#optimizers", "title": "Optimizers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#resources", "title": "Resources\u00b6", "text": "<ul> <li><p>PyTorch optimization tutorial</p> </li> <li><p>Index of PyTorch optimizers</p> </li> </ul>"}, {"location": "generative_ai/tutorials/pytorch_intro/#pytorch-datasets-and-data-loaders", "title": "Pytorch Datasets and Data Loaders\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#dataset", "title": "Dataset\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#data-loader", "title": "Data Loader\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#resources", "title": "Resources\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#training-loop", "title": "Training Loop\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#create-a-number-sum-dataset", "title": "Create a Number Sum Dataset\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#define-a-simple-model", "title": "Define a Simple Model\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#instantiate-components-needed-for-training", "title": "Instantiate Components Needed for Training\u00b6", "text": ""}, {"location": "generative_ai/tutorials/pytorch_intro/#create-a-trainning-loop", "title": "Create a Trainning Loop\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/", "title": "Generative Adversarial Network: training", "text": "<p>Generative Adversarial Networks (GANs) represent a significant leap in the field of artificial intelligence, particularly in image generation. At its core, a GAN consists of two main components: the Generator and the Discriminator.</p> <ol> <li>The Generator: Crafting Synthetic Images</li> </ol> <p>The Generator's role is to create images. It starts with a random noise vector, often sampled from a high-dimensional distribution. This vector, known as latent z, is then passed through the Generator network, which uses strided convolutions to convert this latent representation into a synthetic image. For example, a latent vector of 100 elements might be transformed into a 64x64 pixel image, which translates to 4096 numbers.</p> <ol> <li>The Discriminator: The Arbiter of Realism</li> </ol> <p>The Discriminator's job is to distinguish between real and generated (fake) images. It performs a binary classification to determine the authenticity of each image. In classical GANs, the Discriminator is typically a standard Convolutional Neural Network (CNN) used for image classification.</p> <ol> <li>Training the GAN: A Dance Between Generator and Discriminator</li> </ol> <p>Training a GAN involves an iterative process where the Generator and Discriminator continuously improve through competition. Initially, the Generator creates images, and the Discriminator learns to distinguish them from real ones. As the Generator improves, it becomes better at fooling the Discriminator. The training process is a cycle of alternating between training the Discriminator and the Generator, each time making them more adept at their tasks.</p> <p>In summary, GANs harness the power of two neural networks in a unique setup, where one creates and the other critiques, leading to the generation of increasingly realistic images. This dynamic interplay between the Generator and Discriminator underpins the success of GANs in creating convincing and high-quality synthetic images.</p> In\u00a0[\u00a0]: Copied! <pre>import multiprocessing\n\nCONFIG = {\n    # For repeatibility we will fix the random seed\n    \"manual_seed\": 42,\n    # This defines a set of augmentations we will perform, see below\n    \"policy\": \"color,translation\",  # ,cutout\n    # Dimension of the latent space\n    \"latent_dimension\": 256,\n    # Batch size for training\n    \"batch_size\": 256,\n    # Number of epochs. We will use 1200 epochs which corresponds to\n    # approximately 20 min of training\n    \"n_epochs\": 40,\n    # Input images will be resized to this, the generator will generate\n    # images with this size\n    \"image_size\": 64,  # 64x64 pixels\n    # Number of channels in the input images\n    \"num_channels\": 3,  # RGB\n    # Learning rate\n    \"lr\": 0.002,\n    # Momentum for Adam: in GANs you want to use a lower momentum to\n    # allow the Generator and the Discriminator to interact quicker\n    \"beta1\": 0.7,\n    # Number of feature maps in each layer of the Generator\n    \"g_feat_map_size\": 64,\n    # Number of feature maps in each layer of the Discriminator\n    \"d_feat_map_size\": 64,\n    # Where to save the data\n    \"data_path\": \"data/\",\n    # Number of workers to use to load the data\n    \"workers\": multiprocessing.cpu_count(),\n    # We will display progress every \"save_iter\" epochs\n    \"save_iter\": 10,\n    # Where to save the progress\n    \"outdir\": \"data/stanford_cars/\",\n    # Unused\n    \"clip_value\": 0.01,\n}\n</pre> import multiprocessing  CONFIG = {     # For repeatibility we will fix the random seed     \"manual_seed\": 42,     # This defines a set of augmentations we will perform, see below     \"policy\": \"color,translation\",  # ,cutout     # Dimension of the latent space     \"latent_dimension\": 256,     # Batch size for training     \"batch_size\": 256,     # Number of epochs. We will use 1200 epochs which corresponds to     # approximately 20 min of training     \"n_epochs\": 40,     # Input images will be resized to this, the generator will generate     # images with this size     \"image_size\": 64,  # 64x64 pixels     # Number of channels in the input images     \"num_channels\": 3,  # RGB     # Learning rate     \"lr\": 0.002,     # Momentum for Adam: in GANs you want to use a lower momentum to     # allow the Generator and the Discriminator to interact quicker     \"beta1\": 0.7,     # Number of feature maps in each layer of the Generator     \"g_feat_map_size\": 64,     # Number of feature maps in each layer of the Discriminator     \"d_feat_map_size\": 64,     # Where to save the data     \"data_path\": \"data/\",     # Number of workers to use to load the data     \"workers\": multiprocessing.cpu_count(),     # We will display progress every \"save_iter\" epochs     \"save_iter\": 10,     # Where to save the progress     \"outdir\": \"data/stanford_cars/\",     # Unused     \"clip_value\": 0.01, } <p>In order to make the training repeatible, let's fix the random seed and set pytorch to use deterministic algorithms. This is normally not necessary, although it might not be a bad idea to keep your experimentation ordered. Keep in mind that the initial random seed can have quite an impact on the training of the GAN.</p> <p>One thing to consider is that deterministic algorithms can be significantly slower than non-deterministic ones, so we pay a performance penalty for setting things this way. In a real training scenario you might want to reconsider this tradeoff.</p> In\u00a0[\u00a0]: Copied! <pre>import os\n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\nos.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n\nimport random\nimport torch\n\nrandom.seed(CONFIG[\"manual_seed\"])\ntorch.manual_seed(CONFIG[\"manual_seed\"])\ntorch.use_deterministic_algorithms(True)\n</pre> import os  os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"  import random import torch  random.seed(CONFIG[\"manual_seed\"]) torch.manual_seed(CONFIG[\"manual_seed\"]) torch.use_deterministic_algorithms(True) <p>Let's import a few other modules, methods and functions that we will need:</p> In\u00a0[\u00a0]: Copied! <pre>import argparse\nimport json\nfrom pathlib import Path\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nimport torch.nn.utils.spectral_norm as spectral_norm\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport numpy as np\nfrom torchvision.utils import make_grid\nimport os\n\nfrom IPython.display import clear_output\n\nfrom ema_pytorch import EMA\n\nimport time\nimport tqdm\n\n# Create the output directory\noutput_dir = Path(CONFIG[\"outdir\"])\noutput_dir.mkdir(parents=True, exist_ok=True)\n\n# Save the configuration there for safekeeping\nwith open(output_dir / \"config.json\", \"w\") as f:\n    json.dump(CONFIG, f, indent=4)\n\n# Make sure CUDA is available (i.e. the GPU is setup correctly)\nassert torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</pre> import argparse import json from pathlib import Path import torch import torch.nn as nn import torch.optim as optim import torch.nn.functional as F from torchvision import datasets, transforms import torch.nn.utils.spectral_norm as spectral_norm import numpy as np import matplotlib.pyplot as plt from matplotlib import gridspec import numpy as np from torchvision.utils import make_grid import os  from IPython.display import clear_output  from ema_pytorch import EMA  import time import tqdm  # Create the output directory output_dir = Path(CONFIG[\"outdir\"]) output_dir.mkdir(parents=True, exist_ok=True)  # Save the configuration there for safekeeping with open(output_dir / \"config.json\", \"w\") as f:     json.dump(CONFIG, f, indent=4)  # Make sure CUDA is available (i.e. the GPU is setup correctly) assert torch.cuda.is_available() device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") In\u00a0[\u00a0]: Copied! <pre>print(device)\n</pre> print(device) In\u00a0[\u00a0]: Copied! <pre>def set_seeds(seed):\n\"\"\"Set seeds for reproducibility.\"\"\"\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.use_deterministic_algorithms(True)\n\n\ndef initialize_weights(model):\n\"\"\"Custom weight initialization.\"\"\"\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight.data, 0.0, 0.02)\n        elif isinstance(m, nn.BatchNorm2d):\n            nn.init.normal_(m.weight.data, 1.0, 0.02)\n            nn.init.constant_(m.bias.data, 0)\n\n\ndef get_positive_labels(size, device, smoothing=True, random_flip=0.05):\n    if smoothing:\n        # Random positive numbers between 0.8 and 1.2 (label smoothing)\n        labels = 0.8 + 0.4 * torch.rand(size, device=device)\n    else:\n        labels = torch.full((size,), 1.0, device=device)\n\n    if random_flip &gt; 0:\n        # Let's flip some of the labels to make it slightly harder for the discriminator\n        num_to_flip = int(random_flip * labels.size(0))\n\n        # Get random indices and set the first \"num_to_flip\" of them to 0\n        indices = torch.randperm(labels.size(0))[:num_to_flip]\n        labels[indices] = 0\n\n    return labels\n\n\ndef get_negative_labels(size, device):\n    return torch.full((size,), 0.0, device=device)\n</pre> def set_seeds(seed):     \"\"\"Set seeds for reproducibility.\"\"\"     random.seed(seed)     torch.manual_seed(seed)     torch.use_deterministic_algorithms(True)   def initialize_weights(model):     \"\"\"Custom weight initialization.\"\"\"     for m in model.modules():         if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):             nn.init.normal_(m.weight.data, 0.0, 0.02)         elif isinstance(m, nn.BatchNorm2d):             nn.init.normal_(m.weight.data, 1.0, 0.02)             nn.init.constant_(m.bias.data, 0)   def get_positive_labels(size, device, smoothing=True, random_flip=0.05):     if smoothing:         # Random positive numbers between 0.8 and 1.2 (label smoothing)         labels = 0.8 + 0.4 * torch.rand(size, device=device)     else:         labels = torch.full((size,), 1.0, device=device)      if random_flip &gt; 0:         # Let's flip some of the labels to make it slightly harder for the discriminator         num_to_flip = int(random_flip * labels.size(0))          # Get random indices and set the first \"num_to_flip\" of them to 0         indices = torch.randperm(labels.size(0))[:num_to_flip]         labels[indices] = 0      return labels   def get_negative_labels(size, device):     return torch.full((size,), 0.0, device=device) In\u00a0[\u00a0]: Copied! <pre>def DiffAugment(x, policy=\"\", channels_first=True):\n    if policy:\n        if not channels_first:\n            x = x.permute(0, 3, 1, 2)\n        for p in policy.split(\",\"):\n            for f in AUGMENT_FNS[p]:\n                x = f(x)\n        if not channels_first:\n            x = x.permute(0, 2, 3, 1)\n        x = x.contiguous()\n    return x\n\n\ndef rand_brightness(x):\n    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n    return x\n\n\ndef rand_saturation(x):\n    x_mean = x.mean(dim=1, keepdim=True)\n    x = (x - x_mean) * (\n        torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2\n    ) + x_mean\n    return x\n\n\ndef rand_contrast(x):\n    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n    x = (x - x_mean) * (\n        torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5\n    ) + x_mean\n    return x\n\n\ndef rand_translation(x, ratio=0.125):\n    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n    translation_x = torch.randint(\n        -shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device\n    )\n    translation_y = torch.randint(\n        -shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device\n    )\n    grid_batch, grid_x, grid_y = torch.meshgrid(\n        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n    )\n    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n    x = (\n        x_pad.permute(0, 2, 3, 1)\n        .contiguous()[grid_batch, grid_x, grid_y]\n        .permute(0, 3, 1, 2)\n        .contiguous()\n    )\n    return x\n\n\ndef rand_cutout(x, ratio=0.5):\n    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n    offset_x = torch.randint(\n        0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device\n    )\n    offset_y = torch.randint(\n        0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device\n    )\n    grid_batch, grid_x, grid_y = torch.meshgrid(\n        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n    )\n    grid_x = torch.clamp(\n        grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1\n    )\n    grid_y = torch.clamp(\n        grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1\n    )\n    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n    mask[grid_batch, grid_x, grid_y] = 0\n    x = x * mask.unsqueeze(1)\n    return x\n\n\nAUGMENT_FNS = {\n    \"color\": [rand_brightness, rand_saturation, rand_contrast],\n    \"translation\": [rand_translation],\n    \"cutout\": [rand_cutout],\n}\n</pre> def DiffAugment(x, policy=\"\", channels_first=True):     if policy:         if not channels_first:             x = x.permute(0, 3, 1, 2)         for p in policy.split(\",\"):             for f in AUGMENT_FNS[p]:                 x = f(x)         if not channels_first:             x = x.permute(0, 2, 3, 1)         x = x.contiguous()     return x   def rand_brightness(x):     x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)     return x   def rand_saturation(x):     x_mean = x.mean(dim=1, keepdim=True)     x = (x - x_mean) * (         torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2     ) + x_mean     return x   def rand_contrast(x):     x_mean = x.mean(dim=[1, 2, 3], keepdim=True)     x = (x - x_mean) * (         torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5     ) + x_mean     return x   def rand_translation(x, ratio=0.125):     shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)     translation_x = torch.randint(         -shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device     )     translation_y = torch.randint(         -shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device     )     grid_batch, grid_x, grid_y = torch.meshgrid(         torch.arange(x.size(0), dtype=torch.long, device=x.device),         torch.arange(x.size(2), dtype=torch.long, device=x.device),         torch.arange(x.size(3), dtype=torch.long, device=x.device),     )     grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)     grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)     x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])     x = (         x_pad.permute(0, 2, 3, 1)         .contiguous()[grid_batch, grid_x, grid_y]         .permute(0, 3, 1, 2)         .contiguous()     )     return x   def rand_cutout(x, ratio=0.5):     cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)     offset_x = torch.randint(         0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device     )     offset_y = torch.randint(         0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device     )     grid_batch, grid_x, grid_y = torch.meshgrid(         torch.arange(x.size(0), dtype=torch.long, device=x.device),         torch.arange(cutout_size[0], dtype=torch.long, device=x.device),         torch.arange(cutout_size[1], dtype=torch.long, device=x.device),     )     grid_x = torch.clamp(         grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1     )     grid_y = torch.clamp(         grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1     )     mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)     mask[grid_batch, grid_x, grid_y] = 0     x = x * mask.unsqueeze(1)     return x   AUGMENT_FNS = {     \"color\": [rand_brightness, rand_saturation, rand_contrast],     \"translation\": [rand_translation],     \"cutout\": [rand_cutout], } <ul> <li>The original Kaggle dataset: https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset?datasetId=30084&amp;sortBy=dateCreated&amp;select=cars_test</li> <li>The devkit: car_devkit.tgz</li> <li>The cars_test_annos_withlabels.mat file: https://www.kaggle.com/code/subhangaupadhaya/pytorch-stanfordcars-classification/input?select=cars_test_annos_withlabels+%281%29.mat</li> </ul> <p>The directory structure you provided earlier works well once we add the missing file!</p> <pre>\u2514\u2500\u2500 stanford_cars\n    \u2514\u2500\u2500 cars_test_annos_withlabels.mat\n    \u2514\u2500\u2500 cars_train\n        \u2514\u2500\u2500 *.jpg\n    \u2514\u2500\u2500 cars_test\n        \u2514\u2500\u2500 .*jpg\n    \u2514\u2500\u2500 devkit\n        \u251c\u2500\u2500 cars_meta.mat\n        \u251c\u2500\u2500 cars_test_annos.mat\n        \u251c\u2500\u2500 cars_train_annos.mat\n        \u251c\u2500\u2500 eval_train.m\n        \u251c\u2500\u2500 README.txt\n        \u2514\u2500\u2500 train_perfect_preds.txt\n</pre> In\u00a0[\u00a0]: Copied! <pre>def get_dataloader(\n    root_path,\n    image_size,\n    batch_size,\n    workers=multiprocessing.cpu_count(),\n    donwload=False,\n):\n    transform = transforms.Compose(\n        [\n            transforms.Resize(image_size),\n            transforms.CenterCrop((image_size, image_size)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ]\n    )\n\n    dataset_train = datasets.StanfordCars(\n        root=root_path, download=donwload, split=\"train\", transform=transform\n    )\n\n    dataset_test = datasets.StanfordCars(\n        root=root_path, download=donwload, split=\"test\", transform=transform\n    )\n\n    dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_test])\n\n    print(f\"Using {workers} workers\")\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=workers,\n        pin_memory=True,\n        persistent_workers=True if workers &gt; 0 else False,\n        #         collate_fn=collate_fn\n    )\n\n    return dataloader\n</pre>   def get_dataloader(     root_path,     image_size,     batch_size,     workers=multiprocessing.cpu_count(),     donwload=False, ):     transform = transforms.Compose(         [             transforms.Resize(image_size),             transforms.CenterCrop((image_size, image_size)),             transforms.ToTensor(),             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),         ]     )      dataset_train = datasets.StanfordCars(         root=root_path, download=donwload, split=\"train\", transform=transform     )      dataset_test = datasets.StanfordCars(         root=root_path, download=donwload, split=\"test\", transform=transform     )      dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_test])      print(f\"Using {workers} workers\")     dataloader = torch.utils.data.DataLoader(         dataset,         batch_size=batch_size,         shuffle=True,         num_workers=workers,         pin_memory=True,         persistent_workers=True if workers &gt; 0 else False,         #         collate_fn=collate_fn     )      return dataloader In\u00a0[\u00a0]: Copied! <pre># Visualize the output tensor as a grayscale image\ndef visualize_batch(batch):\n    b = batch.detach().cpu()\n    fig, sub = plt.subplots(dpi=150)\n    sub.imshow(np.transpose(make_grid(b, padding=0, normalize=True).cpu(), (1, 2, 0)))\n    _ = sub.axis(\"off\")\n\n\ndef training_tracking(D_losses, G_losses, D_acc, fake_data):\n    fig = plt.figure(dpi=150)\n\n    gs = gridspec.GridSpec(2, 8)\n\n    # Create subplots\n    ax_a = fig.add_subplot(gs[0, :3])  # Top-left subplot\n    ax_b = fig.add_subplot(gs[1, :3])  # Bottom-left subplot\n    ax_c = fig.add_subplot(gs[:, 4:])  # Right subplot spanning both rows\n\n    subs = [ax_a, ax_b, ax_c]\n\n    # Losses\n    subs[0].plot(D_losses, label=\"Discriminator\")\n    subs[0].plot(G_losses, label=\"Generator\")\n    subs[0].legend()\n    subs[0].set_ylabel(\"Loss\")\n\n    # Accuracy\n    subs[1].plot(D_acc)\n    subs[1].set_ylabel(\"D accuracy\")\n\n    # Examples of generated images\n    subs[2].imshow(\n        np.transpose(\n            make_grid(\n                fake_data.detach().cpu(), padding=0, normalize=True, nrow=4\n            ).cpu(),\n            (1, 2, 0),\n        )\n    )\n    subs[2].axis(\"off\")\n    fig.tight_layout()\n\n    return fig\n</pre> # Visualize the output tensor as a grayscale image def visualize_batch(batch):     b = batch.detach().cpu()     fig, sub = plt.subplots(dpi=150)     sub.imshow(np.transpose(make_grid(b, padding=0, normalize=True).cpu(), (1, 2, 0)))     _ = sub.axis(\"off\")   def training_tracking(D_losses, G_losses, D_acc, fake_data):     fig = plt.figure(dpi=150)      gs = gridspec.GridSpec(2, 8)      # Create subplots     ax_a = fig.add_subplot(gs[0, :3])  # Top-left subplot     ax_b = fig.add_subplot(gs[1, :3])  # Bottom-left subplot     ax_c = fig.add_subplot(gs[:, 4:])  # Right subplot spanning both rows      subs = [ax_a, ax_b, ax_c]      # Losses     subs[0].plot(D_losses, label=\"Discriminator\")     subs[0].plot(G_losses, label=\"Generator\")     subs[0].legend()     subs[0].set_ylabel(\"Loss\")      # Accuracy     subs[1].plot(D_acc)     subs[1].set_ylabel(\"D accuracy\")      # Examples of generated images     subs[2].imshow(         np.transpose(             make_grid(                 fake_data.detach().cpu(), padding=0, normalize=True, nrow=4             ).cpu(),             (1, 2, 0),         )     )     subs[2].axis(\"off\")     fig.tight_layout()      return fig In\u00a0[\u00a0]: Copied! <pre>class Generator(nn.Module):\n\"\"\"\n    Generator class for DCGAN.\n    The latent is passed through the generator network that ouptupts a synthetic image.\n\n    :param image_size: size of the input image (assumed to be square). Must be a power of 2\n    :param latent_dimension: dimension of the latent space\n    :param feat_map_size: number of feature maps in the last layer of the generator\n    :param num_channels: number of channels in the input image\n    \"\"\"\n\n    def __init__(self, image_size, latent_dimension, feat_map_size, num_channels):\n        super(Generator, self).__init__()\n\n        # The following defines the architecture in a way that automatically\n        # scales the number of blocks depending on the size of the input image\n\n        # Number of blocks between the first and the last (excluded)\n        n_blocks = int(np.log2(image_size)) - 3\n\n        # Initial multiplicative factor for the number of feature maps\n        factor = 2 ** (n_blocks)\n\n        # The first block takes us from the latent space to the feature space with a\n        # 4x4 kernel with stride 1 and no padding\n        blocks = [\n            self._get_transpconv_block(\n                latent_dimension, feat_map_size * factor, 4, 1, 0, nn.LeakyReLU(0.2)\n            )\n        ]\n\n        # The following blocks are transposed convolutional layers with stride 2 and\n        # kernel size 4x4. Every block halves the number of feature maps but double the\n        # size of the image (upsampling)\n        # (NOTE that we loop in reverse order)\n        prev_dim = feat_map_size * factor\n        for f in range(int(np.log2(factor) - 1), -1, -1):\n            blocks.append(\n                self._get_transpconv_block(\n                    prev_dim, feat_map_size * 2**f, 4, 2, 1, nn.LeakyReLU(0.2)\n                )\n            )\n            prev_dim = feat_map_size * 2**f\n\n        # Add last layer\n        blocks.append(\n            self._get_transpconv_block(\n                feat_map_size, num_channels, 4, 2, 1, nn.Tanh(), batch_norm=False\n            )\n        )\n\n        self.model = nn.Sequential(*blocks)\n\n    def _get_transpconv_block(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        activation,\n        batch_norm=True,\n    ):\n        return nn.Sequential(\n            nn.ConvTranspose2d(\n                in_channels,\n                out_channels,\n                kernel_size=kernel_size,\n                stride=stride,\n                padding=padding,\n            ),\n            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n            activation,\n        )\n\n    def forward(self, latents):\n        return self.model(latents)\n</pre> class Generator(nn.Module):     \"\"\"     Generator class for DCGAN.          The latent is passed through the generator network that ouptupts a synthetic image.      :param image_size: size of the input image (assumed to be square). Must be a power of 2     :param latent_dimension: dimension of the latent space     :param feat_map_size: number of feature maps in the last layer of the generator     :param num_channels: number of channels in the input image     \"\"\"      def __init__(self, image_size, latent_dimension, feat_map_size, num_channels):         super(Generator, self).__init__()          # The following defines the architecture in a way that automatically         # scales the number of blocks depending on the size of the input image          # Number of blocks between the first and the last (excluded)         n_blocks = int(np.log2(image_size)) - 3          # Initial multiplicative factor for the number of feature maps         factor = 2 ** (n_blocks)          # The first block takes us from the latent space to the feature space with a         # 4x4 kernel with stride 1 and no padding         blocks = [             self._get_transpconv_block(                 latent_dimension, feat_map_size * factor, 4, 1, 0, nn.LeakyReLU(0.2)             )         ]          # The following blocks are transposed convolutional layers with stride 2 and         # kernel size 4x4. Every block halves the number of feature maps but double the         # size of the image (upsampling)         # (NOTE that we loop in reverse order)         prev_dim = feat_map_size * factor         for f in range(int(np.log2(factor) - 1), -1, -1):             blocks.append(                 self._get_transpconv_block(                     prev_dim, feat_map_size * 2**f, 4, 2, 1, nn.LeakyReLU(0.2)                 )             )             prev_dim = feat_map_size * 2**f          # Add last layer         blocks.append(             self._get_transpconv_block(                 feat_map_size, num_channels, 4, 2, 1, nn.Tanh(), batch_norm=False             )         )          self.model = nn.Sequential(*blocks)      def _get_transpconv_block(         self,         in_channels,         out_channels,         kernel_size,         stride,         padding,         activation,         batch_norm=True,     ):         return nn.Sequential(             nn.ConvTranspose2d(                 in_channels,                 out_channels,                 kernel_size=kernel_size,                 stride=stride,                 padding=padding,             ),             nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),             activation,         )      def forward(self, latents):         return self.model(latents) In\u00a0[\u00a0]: Copied! <pre>class Discriminator(nn.Module):\n\"\"\"\n    Discriminator class for DCGAN.\n    The Discriminator network tries to divide fake from real images.\n    :param image_size: size of the input image (assumed to be square). Must be a power of 2\n    :param feat_map_size: number of feature maps in the first layer of the discriminator\n    :param num_channels: number of channels in the input image\n    :param dropout: dropout probability\n    \"\"\"\n\n    def __init__(self, image_size, feat_map_size, num_channels, dropout=0):\n        super(Discriminator, self).__init__()\n\n        blocks = []\n\n        prev_dim = num_channels\n        for i in range(int(np.log2(image_size)) - 2):\n            blocks.append(\n                self._get_conv_block(\n                    in_channels=prev_dim,\n                    out_channels=feat_map_size * (2**i),\n                    kernel_size=4,\n                    stride=2,\n                    padding=1,\n                    dropout=dropout,\n                    activation=nn.LeakyReLU(0.2, inplace=True),\n                    batch_norm=False if i == 0 else True,\n                )\n            )\n            prev_dim = feat_map_size * (2**i)\n\n        blocks.append(\n            self._get_conv_block(\n                in_channels=prev_dim,\n                out_channels=1,\n                kernel_size=4,\n                stride=1,\n                padding=0,\n                dropout=0,\n                activation=nn.Sigmoid(),\n                batch_norm=False,\n            )\n        )\n\n        self.model = nn.Sequential(*blocks)\n\n    def _get_conv_block(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dropout,\n        activation,\n        batch_norm=True,\n    ):\n        return nn.Sequential(\n            spectral_norm(\n                nn.Conv2d(\n                    in_channels,\n                    out_channels,\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    bias=not batch_norm,\n                )\n            ),\n            nn.Dropout(p=dropout) if dropout &gt; 0 else nn.Identity(),\n            nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),\n            activation,\n        )\n\n    def forward(self, images):\n        return self.model(images)\n</pre> class Discriminator(nn.Module):     \"\"\"     Discriminator class for DCGAN.          The Discriminator network tries to divide fake from real images.          :param image_size: size of the input image (assumed to be square). Must be a power of 2     :param feat_map_size: number of feature maps in the first layer of the discriminator     :param num_channels: number of channels in the input image     :param dropout: dropout probability     \"\"\"      def __init__(self, image_size, feat_map_size, num_channels, dropout=0):         super(Discriminator, self).__init__()          blocks = []          prev_dim = num_channels         for i in range(int(np.log2(image_size)) - 2):             blocks.append(                 self._get_conv_block(                     in_channels=prev_dim,                     out_channels=feat_map_size * (2**i),                     kernel_size=4,                     stride=2,                     padding=1,                     dropout=dropout,                     activation=nn.LeakyReLU(0.2, inplace=True),                     batch_norm=False if i == 0 else True,                 )             )             prev_dim = feat_map_size * (2**i)          blocks.append(             self._get_conv_block(                 in_channels=prev_dim,                 out_channels=1,                 kernel_size=4,                 stride=1,                 padding=0,                 dropout=0,                 activation=nn.Sigmoid(),                 batch_norm=False,             )         )          self.model = nn.Sequential(*blocks)      def _get_conv_block(         self,         in_channels,         out_channels,         kernel_size,         stride,         padding,         dropout,         activation,         batch_norm=True,     ):         return nn.Sequential(             spectral_norm(                 nn.Conv2d(                     in_channels,                     out_channels,                     kernel_size=kernel_size,                     stride=stride,                     padding=padding,                     bias=not batch_norm,                 )             ),             nn.Dropout(p=dropout) if dropout &gt; 0 else nn.Identity(),             nn.BatchNorm2d(out_channels) if batch_norm else nn.Identity(),             activation,         )      def forward(self, images):         return self.model(images) In\u00a0[\u00a0]: Copied! <pre># Get data loader\ndataloader = get_dataloader(\n    CONFIG[\"data_path\"],\n    CONFIG[\"image_size\"],\n    CONFIG[\"batch_size\"],\n    CONFIG[\"workers\"],\n    donwload=False,\n)\nprint(f\"Total number of examples: {len(dataloader.dataset)}\")\nvisualize_batch(next(iter(dataloader))[0][:16])\n</pre> # Get data loader dataloader = get_dataloader(     CONFIG[\"data_path\"],     CONFIG[\"image_size\"],     CONFIG[\"batch_size\"],     CONFIG[\"workers\"],     donwload=False, ) print(f\"Total number of examples: {len(dataloader.dataset)}\") visualize_batch(next(iter(dataloader))[0][:16]) <p>These images represent cars.</p> In\u00a0[\u00a0]: Copied! <pre># Initialize models and optimizers\nG = Generator(\n    CONFIG[\"image_size\"],\n    CONFIG[\"latent_dimension\"],\n    CONFIG[\"g_feat_map_size\"],\n    CONFIG[\"num_channels\"],\n).to(device)\n\nprint(G)\n</pre> # Initialize models and optimizers G = Generator(     CONFIG[\"image_size\"],     CONFIG[\"latent_dimension\"],     CONFIG[\"g_feat_map_size\"],     CONFIG[\"num_channels\"], ).to(device)  print(G) <p>Let's create a latent vector and put it through the Generator. What shape would you expect?</p> <p>Complete the code marked by the YOUR CODE HERE placeholder</p> In\u00a0[\u00a0]: Copied! <pre># Generate a latent vector of shape (1, CONFIG['latent_dimension'], 1, 1)\n# Remember that the latent vector is just a vector of noise taken from a \n# Normal distribution\n# HINT: you can use torch.randn to sample from a Normal distribution\nlatent = torch.randn(1, CONFIG['latent_dimension'], 1, 1)\n\nlatent = latent.to(device)\nfake_img = G(latent)\nprint(fake_img.shape)\n</pre> # Generate a latent vector of shape (1, CONFIG['latent_dimension'], 1, 1) # Remember that the latent vector is just a vector of noise taken from a  # Normal distribution # HINT: you can use torch.randn to sample from a Normal distribution latent = torch.randn(1, CONFIG['latent_dimension'], 1, 1)  latent = latent.to(device) fake_img = G(latent) print(fake_img.shape) <p>Let's look at what the Generator is producing right now:</p> In\u00a0[\u00a0]: Copied! <pre>visualize_batch(G(latent))\n</pre> visualize_batch(G(latent)) <p>This is of course just noise, because the Generator has not been trained yet. Now let's look at the shape of the tensor as it flows through the architecture:</p> In\u00a0[\u00a0]: Copied! <pre>x = latent\nfor i in range(5):\n    x = G.model[i](x.cuda())\n    b, c, w, h = x.shape\n    \n    print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\")\n</pre> x = latent for i in range(5):     x = G.model[i](x.cuda())     b, c, w, h = x.shape          print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\") <p>We can see that the input latent is mapped to 512 feature maps of size 4x4 pixels. After the first convolution, we have 256 feature maps of size 8x8 pixels, and so on, until we get to 3 output channels and a size of 64x64 pixels, which is the expected size for our fake image (matching the size of the input dataset).</p> <p>The discrimnator take the input of a picture and classify the image.</p> <ol> <li>The Role of the Discriminator</li> </ol> <p>The Discriminator's task in a GAN is to distinguish between real and generated images. During training, this component learns to identify nuances that differentiate authentic images from those created by the Generator.</p> <ol> <li>Training Process: The Split-Batch Method</li> </ol> <p>A popular method for training the Discriminator is the 'split-batch' technique. This involves two main steps:</p> <ul> <li>Step 1: Handling Real Images The Discriminator is fed real images and learns to identify them as authentic. This process involves a forward pass of real data through the Discriminator, generating a probability score for each image being real. The Binary Cross Entropy (BCE) loss is then calculated by comparing the Discriminator's predictions against the true labels (real images).</li> <li>Step 2: Dealing with Fake Images Next, the Discriminator is presented with fake images produced by the Generator. These images undergo a similar process, with the Discriminator learning to label them as fake. The BCE loss is again used to compare the Discriminator's predictions against the true labels (fake images).</li> <li><ol> <li>Updating the Discriminator</li> </ol> </li> </ul> <p>After processing both real and fake images, the Discriminator's weights are updated. This is done using the gradients accumulated from both sets of data, ensuring that the Discriminator improves its ability to differentiate real from fake images.</p> In\u00a0[\u00a0]: Copied! <pre>D = (\n    Discriminator(\n        CONFIG[\"image_size\"], CONFIG[\"d_feat_map_size\"], CONFIG[\"num_channels\"], dropout=0.1\n    )\n    .to(device)#.eval()\n)\n\nprint(D)\n</pre> D = (     Discriminator(         CONFIG[\"image_size\"], CONFIG[\"d_feat_map_size\"], CONFIG[\"num_channels\"], dropout=0.1     )     .to(device)#.eval() )  print(D) <p>the Discriminator is composed of 5 blocks (from 0 to 4), represented by the Sequential modules. This is a standard classification CNN for Binary classification but it does not use any pooling layer. Instead, all convolutional layers are using a stride of 2 so the feature maps become smaller and smaller at every iteration:</p> In\u00a0[\u00a0]: Copied! <pre>x = fake_img\nfor i in range(5):\n    x = D.model[i](x)\n    b, c, w, h = x.shape\n    \n    print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\")\n</pre> x = fake_img for i in range(5):     x = D.model[i](x)     b, c, w, h = x.shape          print(f\"Channels: {c:3d}, w x h: {w:2d} x {h:2d}\") In\u00a0[\u00a0]: Copied! <pre># Complete this code using the appropriate loss for the\n# binary classification task of the Discriminator\n# HINT: some possible losses available in pytorch are:\n# nn.MSELoss()\n# nn.BCELoss()\n# nn.CrossEntropyLoss()\n# nn.NLLoss()\n# Pick the one appropriate for binary classification\ncriterion = nn.BCELoss()  # YOUR CODE HERE\n\n# Optimizer for the Generator\n# Instance the optimizer for the Generator\n# HINT: the first parameter of optim.Adam()\n# should be a list of parameters to optimize.\n# Given a network N, you can obtain its parameters\n# just by doing N.parameters(). Now do the same for the\n# Generator\noptimizerG = optim.Adam(\n    G.parameters(),  # YOUR CODE HERE,\n    lr=CONFIG[\"lr\"],\n    betas=(CONFIG[\"beta1\"], 0.999),\n)\n\n# Optimizer for the Discriminator\n# Do the same thing you did for the Generator, but this time\n# for the Discriminator (i.e., complete the initialization\n# of the Adam optimizer with the parameters of D)\noptimizerD = optim.Adam(\n    D.parameters(),  # YOUR CODE HERE,\n    lr=CONFIG[\"lr\"] / 4,\n    betas=(CONFIG[\"beta1\"], 0.999),\n)\n</pre> # Complete this code using the appropriate loss for the # binary classification task of the Discriminator # HINT: some possible losses available in pytorch are: # nn.MSELoss() # nn.BCELoss() # nn.CrossEntropyLoss() # nn.NLLoss() # Pick the one appropriate for binary classification criterion = nn.BCELoss()  # YOUR CODE HERE  # Optimizer for the Generator # Instance the optimizer for the Generator # HINT: the first parameter of optim.Adam() # should be a list of parameters to optimize. # Given a network N, you can obtain its parameters # just by doing N.parameters(). Now do the same for the # Generator optimizerG = optim.Adam(     G.parameters(),  # YOUR CODE HERE,     lr=CONFIG[\"lr\"],     betas=(CONFIG[\"beta1\"], 0.999), )  # Optimizer for the Discriminator # Do the same thing you did for the Generator, but this time # for the Discriminator (i.e., complete the initialization # of the Adam optimizer with the parameters of D) optimizerD = optim.Adam(     D.parameters(),  # YOUR CODE HERE,     lr=CONFIG[\"lr\"] / 4,     betas=(CONFIG[\"beta1\"], 0.999), ) In\u00a0[\u00a0]: Copied! <pre>ema_G = EMA(\n    G,\n    beta = 0.995,              # average over the last ~10 epochs\n    update_after_step = 100,    # start averaging after the first 5 epochs\n    update_every = 1\n)\n</pre> ema_G = EMA(     G,     beta = 0.995,              # average over the last ~10 epochs     update_after_step = 100,    # start averaging after the first 5 epochs     update_every = 1 ) In\u00a0[\u00a0]: Copied! <pre># Initialize weights\n_ = G.apply(initialize_weights)\n_ = D.apply(initialize_weights)\n</pre> # Initialize weights _ = G.apply(initialize_weights) _ = D.apply(initialize_weights) <p>ok, we're now ready to start training! We will use the \"split-batch\" technique we have seen in the lesson, where we compute separately the gradients for the Discriminator, first on a batch of real images and then on a batch of fake images. Then we accumulate the gradients and perform one backward pass. For the Generator, we adopt the trick of maximizing log(D(G(z))) instead of minimizing log(1\u2212D(G(z))). This is accomplished by setting the labels for the fake images generated by the Generator to 1 (\"real\") instead of 0 (\"fake\"), as we have seen in the lesson.</p> <p>But first let's look at some more tricks we're using in the training loop.</p> In\u00a0[\u00a0]: Copied! <pre># Create batch of latent vectors that we will use to visualize\n#  the progression of the generator\nfixed_noise = torch.randn(16, CONFIG['latent_dimension'], 1, 1, device=device)\n\n# Lists to keep track of progress\nG_losses = []\nD_losses = []\nD_acc = []\n</pre> # Create batch of latent vectors that we will use to visualize #  the progression of the generator fixed_noise = torch.randn(16, CONFIG['latent_dimension'], 1, 1, device=device)  # Lists to keep track of progress G_losses = [] D_losses = [] D_acc = [] <p>Complete the code marked by the YOUR CODE HERE placeholder</p> In\u00a0[\u00a0]: Copied! <pre>os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nos.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n\nprint(\"Starting Training Loop...\")\n\ntstart = time.time()\nn_frame = 0\n\nfor epoch in range(CONFIG[\"n_epochs\"]):\n    # Keep track of losses and accuracy of the Discriminator\n    # for each batch, we will average them at the end of the\n    # epoch\n    batch_G_losses = []\n    batch_D_losses = []\n    batch_D_acc = []\n\n    # Loop over batch of real data (we throw away the labels\n    # provided by the dataloader by saving them into `_`)\n    for data, _ in tqdm.tqdm(dataloader, total=len(dataloader)):\n        # Move batch to GPU and record its size\n        # (remember that the last batch could be smaller than batch_size)\n        data = data.to(device)\n        b_size = data.size(0)\n\n        # This function implements tricks 3 and 4 (smoothing and random label flipping)\n        labels = get_positive_labels(b_size, device, smoothing=True, random_flip=0.2)\n        print(labels)\n\n        ################################################\n        # Discriminator training                       #\n        ################################################\n\n        # The generator is frozen, gradients flow only\n        D.zero_grad()  # Resets the gradients of all optimized torch\n\n        # Forward pass real batch through D using DiffAugment\n        # augmentation\n        D_pred = D(DiffAugment(data, policy=CONFIG[\"policy\"])).view(\n            -1\n        )  # probability to be real\n\n        # Measure accuracy for the positive batch\n        acc_pos = (D_pred &gt; 0.5).sum() / D_pred.size(0)\n\n        # Loss on the real data\n\n        # Compute the loss on the real data by calling the\n        # criterion we have defined above\n        # &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; YOUR CODE HERE\n        loss_on_real_data = criterion(D_pred, labels)\n\n        # Compute the gradients on the real data\n        # The improve the discriminator\n        # HINT: you can compute the gradients by calling\n        # .backward() on the loss\n        loss_on_real_data.backward()  # YOUR CODE HERE\n        # No .step () to update weights, we need more work\n\n        # Now pass a batch of fake data through the model\n        # Generate batch of latent vectors\n        # HINT: generate a latent using torch.randn, the shape of the\n        # latent should be (b_size, CONFIG['latent_dimension'], 1, 1)\n        # NOTE: add the device=device option as in torch.randn(..., device=device)\n        # so the latent is created on the GPU (otherwise you'll get an error later)\n        latent_vectors = torch.randn(\n            b_size, CONFIG[\"latent_dimension\"], 1, 1, device=device\n        )  # YOUR CODE HERE\n\n        # Generate fake image batch with G\n        # HINT: just call the generator using the latent\n        fake_data = G(latent_vectors)  # YOUR CODE HERE\n\n        # Assign negative label as ground truth\n        # (ground truth labels)\n        labels.fill_(0)  # 0 is the label for fake images\n\n        # Get predictions from the Discriminator\n        # (applying DiffAugment augmentations)\n        # NOTE: here it is VERY important to use .detach() on the (augmented)\n        # fake data because we do NOT want the Generator to be part of the computation\n        # graph used to compute the gradients (we don't want to update the Generator yet)\n        D_pred = D(\n            DiffAugment(fake_data, policy=CONFIG[\"policy\"]).detach()  # VERY IMPORTANT\n        ).view(-1)\n        # we need to reuse this data later on in the bacakr of the generator\n        # , and without the .detach() it will be destroyed\n\n        # Get accuracy for this all-fake batch\n        acc_neg = (D_pred &lt; 0.5).sum() / D_pred.size(0)\n\n        # Loss on fake data\n        # HINT: call the criterion defined above providing the\n        # discriminator prediction D_pred and the ground truth\n        # labels\n        loss_on_fake_data = criterion(D_pred, labels)  # YOUR CODE HERE\n\n        # This computes the gradients after the fake data\n        # forward pass and stores them in the tensors\n        # (model parameters are NOT updated here)\n        # Remember that .backward() by default does NOT replace\n        # the gradients coming from the backward pass on the real data.\n        # Instead, it sums the new gradients with the old gradients\n        loss_on_fake_data.backward()\n\n        # Now we can finally update the Discriminator\n        # HINT: call a step on the optimizer of the Discriminator\n        # (optimizerD)\n        # &gt;&gt;&gt; YOUR CODE HERE\n        optimizerD.step()\n        # This will use the gradient accumulated on both: the real and\n        # the fake data\n\n        # Compute error of D as sum over the fake and the real batches\n        # for safekeeping\n        total_loss = loss_on_real_data + loss_on_fake_data\n\n        ################################################\n        # Generator training                           #\n        ################################################\n        # Depending on how good the discriminator is at this stage\n        # it will give us a higher or lower classification loss\n        # We then take a backward step and update the parameters\n        # of the generator in order to decrease the loss obtained\n        # from the prediction of the discriminator\n        # The discriminartor is part of the competition graph for this step\n        # however its weight are frozen so it plays a passive part\n        # In this phase the weight of the generator are changed\n        # so that it will fool more and more the discriminator\n        # Explanaition of the generator objective can be found here:\n        # https://www.youtube.com/watch?v=wMF0sQO7sNw&amp;t=204s\n        # the summary is that the objective of the generator is\n        # equivalent to minimizing the BCE loss of D(G(z)) when\n        # imposing the binary label equal to 1\n        # L = -y*log(D(G(z)))\n        G.zero_grad()  # Resets the gradients of all optimized torch\n\n        # Remember that BCELoss is \u2212[y logx + (1\u2212y)\u22c5log(1\u2212x)]\n        labels.fill_(1)  # 1 is the label for \"real\".\n\n        # Since we just updated D, perform another forward pass of\n        # the all-fake batch we already generated as part of the previous\n        # part (with DiffAugment)\n        # NOTE how we are NOT using .detach now, as this time we want the\n        # gradients for this operation to be accumulated\n        D_pred = D(DiffAugment(fake_data, policy=CONFIG[\"policy\"])).view(-1)\n        # Probability of these images to be real according to the discriminator\n        # Because the weights of the discriminator has changed in the previous step\n        # minimizing the objective function of the GAN for the generator\n        # is equivalen to minimizing the BCE loss on the prediction of the discriminator\n        # when the labels are all positive.\n\n        # Loss from the Discriminator prediction that is going\n        # to be used to update G\n        # HINT: call the criterion on the prediction of the discriminator\n        # D_pred and the labels\n        loss_on_fake_G = criterion(D_pred, labels)  # YOUR CODE HERE\n\n        # Calculate gradients for G\n        # HINT: you did this before\n        loss_on_fake_G.backward()  # YOUR CODE HERE\n\n        # Update G\n        # HINT: call a step on the optimizer for the Generator\n        # (optimizer G)\n        # &gt;&gt;&gt; YOUR CODE HERE\n        optimizerG.step()\n        # Update the Exponential Moving Average copy\n        ema_G.update()\n\n        # Save all losses\n        batch_G_losses.append(loss_on_fake_G.item())\n        batch_D_losses.append(total_loss.item())\n        batch_D_acc.append((0.5 * (acc_pos + acc_neg)).item())\n\n    # Take the mean over the epoch\n    G_losses.append(np.mean(batch_G_losses))\n    D_losses.append(np.mean(batch_D_losses))\n    D_acc.append(np.mean(batch_D_acc))\n\n    if epoch % CONFIG[\"save_iter\"] == 0:\n        with torch.no_grad():\n            fake_viz_data = G(fixed_noise).detach().cpu()\n\n        clear_output(wait=True)\n\n        fig = training_tracking(D_losses, G_losses, D_acc, fake_viz_data)\n\n        plt.show()\n\n        fig.savefig(f\"{CONFIG['outdir']}/frame_{n_frame:05d}.png\")\n        n_frame += 1\n\nprint(f\"Finished in {(time.time() - tstart)/60:.1f} min\")\n</pre> os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\" os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"  print(\"Starting Training Loop...\")  tstart = time.time() n_frame = 0  for epoch in range(CONFIG[\"n_epochs\"]):     # Keep track of losses and accuracy of the Discriminator     # for each batch, we will average them at the end of the     # epoch     batch_G_losses = []     batch_D_losses = []     batch_D_acc = []      # Loop over batch of real data (we throw away the labels     # provided by the dataloader by saving them into `_`)     for data, _ in tqdm.tqdm(dataloader, total=len(dataloader)):         # Move batch to GPU and record its size         # (remember that the last batch could be smaller than batch_size)         data = data.to(device)         b_size = data.size(0)          # This function implements tricks 3 and 4 (smoothing and random label flipping)         labels = get_positive_labels(b_size, device, smoothing=True, random_flip=0.2)         print(labels)          ################################################         # Discriminator training                       #         ################################################          # The generator is frozen, gradients flow only         D.zero_grad()  # Resets the gradients of all optimized torch          # Forward pass real batch through D using DiffAugment         # augmentation         D_pred = D(DiffAugment(data, policy=CONFIG[\"policy\"])).view(             -1         )  # probability to be real          # Measure accuracy for the positive batch         acc_pos = (D_pred &gt; 0.5).sum() / D_pred.size(0)          # Loss on the real data          # Compute the loss on the real data by calling the         # criterion we have defined above         # &gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; YOUR CODE HERE         loss_on_real_data = criterion(D_pred, labels)          # Compute the gradients on the real data         # The improve the discriminator         # HINT: you can compute the gradients by calling         # .backward() on the loss         loss_on_real_data.backward()  # YOUR CODE HERE         # No .step () to update weights, we need more work          # Now pass a batch of fake data through the model         # Generate batch of latent vectors         # HINT: generate a latent using torch.randn, the shape of the         # latent should be (b_size, CONFIG['latent_dimension'], 1, 1)         # NOTE: add the device=device option as in torch.randn(..., device=device)         # so the latent is created on the GPU (otherwise you'll get an error later)         latent_vectors = torch.randn(             b_size, CONFIG[\"latent_dimension\"], 1, 1, device=device         )  # YOUR CODE HERE          # Generate fake image batch with G         # HINT: just call the generator using the latent         fake_data = G(latent_vectors)  # YOUR CODE HERE          # Assign negative label as ground truth         # (ground truth labels)         labels.fill_(0)  # 0 is the label for fake images          # Get predictions from the Discriminator         # (applying DiffAugment augmentations)         # NOTE: here it is VERY important to use .detach() on the (augmented)         # fake data because we do NOT want the Generator to be part of the computation         # graph used to compute the gradients (we don't want to update the Generator yet)         D_pred = D(             DiffAugment(fake_data, policy=CONFIG[\"policy\"]).detach()  # VERY IMPORTANT         ).view(-1)         # we need to reuse this data later on in the bacakr of the generator         # , and without the .detach() it will be destroyed          # Get accuracy for this all-fake batch         acc_neg = (D_pred &lt; 0.5).sum() / D_pred.size(0)          # Loss on fake data         # HINT: call the criterion defined above providing the         # discriminator prediction D_pred and the ground truth         # labels         loss_on_fake_data = criterion(D_pred, labels)  # YOUR CODE HERE          # This computes the gradients after the fake data         # forward pass and stores them in the tensors         # (model parameters are NOT updated here)         # Remember that .backward() by default does NOT replace         # the gradients coming from the backward pass on the real data.         # Instead, it sums the new gradients with the old gradients         loss_on_fake_data.backward()          # Now we can finally update the Discriminator         # HINT: call a step on the optimizer of the Discriminator         # (optimizerD)         # &gt;&gt;&gt; YOUR CODE HERE         optimizerD.step()         # This will use the gradient accumulated on both: the real and         # the fake data          # Compute error of D as sum over the fake and the real batches         # for safekeeping         total_loss = loss_on_real_data + loss_on_fake_data          ################################################         # Generator training                           #         ################################################         # Depending on how good the discriminator is at this stage         # it will give us a higher or lower classification loss         # We then take a backward step and update the parameters         # of the generator in order to decrease the loss obtained         # from the prediction of the discriminator         # The discriminartor is part of the competition graph for this step         # however its weight are frozen so it plays a passive part         # In this phase the weight of the generator are changed         # so that it will fool more and more the discriminator         # Explanaition of the generator objective can be found here:         # https://www.youtube.com/watch?v=wMF0sQO7sNw&amp;t=204s         # the summary is that the objective of the generator is         # equivalent to minimizing the BCE loss of D(G(z)) when         # imposing the binary label equal to 1         # L = -y*log(D(G(z)))         G.zero_grad()  # Resets the gradients of all optimized torch          # Remember that BCELoss is \u2212[y logx + (1\u2212y)\u22c5log(1\u2212x)]         labels.fill_(1)  # 1 is the label for \"real\".          # Since we just updated D, perform another forward pass of         # the all-fake batch we already generated as part of the previous         # part (with DiffAugment)         # NOTE how we are NOT using .detach now, as this time we want the         # gradients for this operation to be accumulated         D_pred = D(DiffAugment(fake_data, policy=CONFIG[\"policy\"])).view(-1)         # Probability of these images to be real according to the discriminator         # Because the weights of the discriminator has changed in the previous step         # minimizing the objective function of the GAN for the generator         # is equivalen to minimizing the BCE loss on the prediction of the discriminator         # when the labels are all positive.          # Loss from the Discriminator prediction that is going         # to be used to update G         # HINT: call the criterion on the prediction of the discriminator         # D_pred and the labels         loss_on_fake_G = criterion(D_pred, labels)  # YOUR CODE HERE          # Calculate gradients for G         # HINT: you did this before         loss_on_fake_G.backward()  # YOUR CODE HERE          # Update G         # HINT: call a step on the optimizer for the Generator         # (optimizer G)         # &gt;&gt;&gt; YOUR CODE HERE         optimizerG.step()         # Update the Exponential Moving Average copy         ema_G.update()          # Save all losses         batch_G_losses.append(loss_on_fake_G.item())         batch_D_losses.append(total_loss.item())         batch_D_acc.append((0.5 * (acc_pos + acc_neg)).item())      # Take the mean over the epoch     G_losses.append(np.mean(batch_G_losses))     D_losses.append(np.mean(batch_D_losses))     D_acc.append(np.mean(batch_D_acc))      if epoch % CONFIG[\"save_iter\"] == 0:         with torch.no_grad():             fake_viz_data = G(fixed_noise).detach().cpu()          clear_output(wait=True)          fig = training_tracking(D_losses, G_losses, D_acc, fake_viz_data)          plt.show()          fig.savefig(f\"{CONFIG['outdir']}/frame_{n_frame:05d}.png\")         n_frame += 1  print(f\"Finished in {(time.time() - tstart)/60:.1f} min\") In\u00a0[\u00a0]: Copied! <pre>visualize_batch(ema_G(fixed_noise).detach().cpu())\n</pre> visualize_batch(ema_G(fixed_noise).detach().cpu()) <p>Cars start to really appear, although we would probably need quite a bit more training (and parameter tuning) to make it really work!</p> <ol> <li><p>Challenges in Training GANs</p> <ul> <li><p>Unstable Balance: Training GANs is delicate; if either the Generator (G) or Discriminator (D) becomes too proficient too quickly, the other lags, disrupting the training process.</p> </li> <li><p>No Clear Convergence Indicator: Unlike traditional neural networks, GANs lack a clear metric like validation loss to signify convergence, making it hard to determine the optimal stopping point.</p> </li> <li><p>Mode Collapse: A critical issue where the Generator discovers a specific image type that always fools the Discriminator, leading to a lack of diversity in generated images.</p> </li> </ul> </li> <li><p>Advanced Variants of GANs</p> </li> </ol> <p>To address these challenges, several GAN variants have been developed:</p> <ul> <li><p>Wasserstein GAN (W-GAN): Introduces a Critic instead of a Discriminator, which assigns continuous scores to images, enhancing training dynamics and reducing mode collapse.</p> </li> <li><p>Progressive GANs: These GANs begin by generating low-resolution images, progressively adding details. This approach aids in faster convergence and enables the creation of high-resolution images.</p> </li> <li><p>Style GANs (v1, v2 and v3): Incorporate a mapping network to convert the latent vector into a style vector, which is fed along with the latent into the Generator. This, combined with added random noise and a few other innovations, significantly enhances sample quality and robustness.</p> </li> </ul> <ol> <li>Conditional GANs</li> </ol> <p>A notable extension of GANs is the development of conditional GANs (see for example here). They allow for manipulation of specific attributes in the output images, such as changing the view angle, gender, or adding a smile.</p> <p>The Pros and Cons of Generative Adversarial Networks (GANs) Generative Adversarial Networks (GANs) have made significant strides in the field of AI-driven image generation. Understanding their strengths and weaknesses is key to leveraging their full potential.</p> <p>Pros of GANs:</p> <ol> <li><p>Speed: One of the standout features of GANs is their speed during inference. They require only a single forward pass of the latent vector, resulting in sub-second latency on modern GPUs. This makes them incredibly efficient for generating images quickly.</p> </li> <li><p>High Sample Quality: GANs are renowned for their excellent sample quality. They consistently rank as state-of-the-art in terms of Fr\u00e9chet Inception Distance (FID) across various datasets, even when compared to newer generation algorithms like diffusion models. The level of detail in synthetic faces and animals created by GANs is often astonishingly high.</p> </li> </ol> <p>Cons of GANs:</p> <ol> <li><p>Poor Mode Coverage: A notable drawback of GANs is their tendency to have poor coverage. The Generator in a GAN often prefers exploitation over exploration. Once it finds a method to deceive the Discriminator or Critic, it tends to overuse this approach, leading to a lack of diversity in the generated images.</p> </li> <li><p>Training Challenges: GANs are notorious for being tricky to train. They require a deep understanding and implementation of numerous training techniques to function efficiently and produce high-quality results.</p> </li> </ol> <p>In summary, while GANs boast impressive speed and sample quality, they face challenges in terms of mode coverage and training complexity. These factors must be considered when deploying GANs for practical applications in image generation.</p>"}, {"location": "generative_ai/tutorials/train_gans/#generative-adversarial-network-training", "title": "Generative Adversarial Network: training\u00b6", "text": "<p>In this exercise we will practice how to train a GAN on a real dataset and generate our first synthetic images. Let's get started!</p> <p>First let's define some parameters for our GAN:</p>"}, {"location": "generative_ai/tutorials/train_gans/#helpers", "title": "Helpers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#diff-augmented", "title": "Diff augmented\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#data", "title": "Data\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#viz", "title": "Viz\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#generator", "title": "Generator\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#discriminator", "title": "Discriminator\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#input-dataset-real-data", "title": "Input dataset: real data\u00b6", "text": "<p>In order to train a GAN we need to show it real data of the type we want to generate. In this case we are going to focus on the Stanford Cars dataset:</p>"}, {"location": "generative_ai/tutorials/train_gans/#generator", "title": "Generator\u00b6", "text": "<p>Generator Training and Inference Generative Adversarial Networks (GANs) have revolutionized the field of AI-driven image generation. A crucial aspect of their success lies in the training of the Generator, which is responsible for creating realistic synthetic images.</p> <ol> <li><p>The Generator's Objective</p> <p>The Generator in a GAN starts with a random noise vector, known as latent z, and transforms it into a synthetic image. The goal of the Generator is to create images so convincing that they can fool the Discriminator into believing they are real. This is accomplished by trying to maximize the loss of the Discriminator on the fake data.</p> </li> <li><p>Training the Generator</p> <p>The training process of the Generator involves several key steps:</p> <p>Generating Fake Images: The Generator creates fake images from the latent z vector. We are actually going to reuse the fake images we have generated previously during the training of the Discriminator, but this is just an optimization. Discriminator's Evaluation: These fake images are then passed through the Discriminator, which is kept frozen during this phase. The Discriminator evaluates these images and assigns a probability score to each, indicating how likely they are to be real. Loss Calculation and Backpropagation: The Generator then adjusts its parameters to maximize the loss derived from the Discriminator\u2019s evaluation. This loss reflects how well the Generator is fooling the Discriminator.</p> </li> <li><p>Binary Cross Entropy Trick</p> <p>It can be shown mathematically that maximizing the Binary Cross Entropy loss of the Discriminator on fake data (with label y=0) is equivalent to minimizing the same loss assigning y=1 instead of y=0.</p> <p>Let's create the Generator network and look at its architecture:</p> </li> </ol>"}, {"location": "generative_ai/tutorials/train_gans/#discriminator", "title": "Discriminator\u00b6", "text": "<p>Now let's have a look at the Discriminator:</p>"}, {"location": "generative_ai/tutorials/train_gans/#loss-and-optimizers", "title": "Loss and optimizers\u00b6", "text": "<p>Like in any other task involving the training of neural networks, we need to setup the loss function we want to minimize and the optimizer.</p> <p>In the case of GANs, we have two optimizers: the optimizer for the Generator, and the optimizer for the Discriminator:</p> <p>Complete the code marked by the YOUR CODE HERE placeholder</p>"}, {"location": "generative_ai/tutorials/train_gans/#trick-1-exponential-moving-average", "title": "Trick 1: Exponential Moving Average\u00b6", "text": "<p>GANs are notoriously difficult to train as the balance between the Generator and the Discriminator is easy to break. There are many tricks that can be used to stabilize that, and we're going to apply some here.</p> <p>The first trick is the Exponential Moving Average: while the Generator is training, we keep a moving average of its weights. At the end we use this smoothed version of the model to generate inference. This model jumps around less and it is less sensitive to sudden changes.</p> <p>The EMA class accepts a parameter called beta, which controls the size of the window used for averaging. The number of steps (i.e. batches) we are going to average over is approximately equal to 1 / (1 - beta). Since there are 20 batches in our dataloader, if we use beta=0.995 we are averaging over 10 epochs:</p>"}, {"location": "generative_ai/tutorials/train_gans/#training-loop", "title": "Training loop\u00b6", "text": ""}, {"location": "generative_ai/tutorials/train_gans/#trick-2-gaussian-initialization", "title": "Trick 2: Gaussian initialization\u00b6", "text": "<p>The original DCGAN paper suggests to initialize the weights of both G and D with a Gaussian distribution (instead of the default pytorch initialization):</p>"}, {"location": "generative_ai/tutorials/train_gans/#trick-3-label-smoothing", "title": "Trick 3: Label smoothing\u00b6", "text": "<p>Label smoothing is a general technique originally proposed in this paper and described in detail here. It consists of substituting the probability for the target class from 1 (<code>hard labels</code>) to something lower than 1. In case of Binary Classification, the BCELoss gets as input the probability for the positive class, so Label Smoothing becomes as simple as substituting 1 with a random number between 0.8 and 1.2. Label smoothing promotes less overconfidence in the Discriminator and slow down its convergence, especially at the beginning when the Generator is still pretty bad at generating realistic images.</p>"}, {"location": "generative_ai/tutorials/train_gans/#trick-4-random-flipping", "title": "Trick 4: Random flipping\u00b6", "text": "<p>In order to make the work of the Discriminator a little harder and prevent it to immediately overwhelm the Generator, it is suggested to add some random noise in the labels for the Discriminator. This is equivalent to flipping some labels from positive to negative. This effectively prevents the Discriminator to ever achieve zero loss.</p>"}, {"location": "generative_ai/tutorials/train_gans/#trick-5-diffaugment", "title": "Trick 5: DiffAugment\u00b6", "text": "<p>In this paper the authors introduce a simple set of augmentations to be applied on both the real and the fake images that prevent overfitting in the Discriminator. Since here we only have 5000 examples of real images, overfitting is very easy and this technique will prove very useful.</p>"}, {"location": "generative_ai/tutorials/training_DDPM/", "title": "Train a Denoising Diffusion Probabilistic Model from scratch", "text": "In\u00a0[\u00a0]: Copied! <pre># Make results fully reproducible:\nimport os\nos.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\nimport random\nimport torch\nimport numpy as np\n\nseed = 42\n\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.use_deterministic_algorithms(True)\n\n# Import a few things we will need\nimport torch.nn.functional as F\nimport torch\nfrom torch.optim import Adam, RAdam\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nfrom tqdm import tqdm\nimport multiprocessing\n\nfrom torchvision import transforms \nimport torchvision\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n</pre> # Make results fully reproducible: import os os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'  import random import torch import numpy as np  seed = 42  random.seed(seed) torch.manual_seed(seed) torch.use_deterministic_algorithms(True)  # Import a few things we will need import torch.nn.functional as F import torch from torch.optim import Adam, RAdam from torch.optim.lr_scheduler import CosineAnnealingLR from tqdm import tqdm import multiprocessing  from torchvision import transforms  import torchvision from torch.utils.data import DataLoader import matplotlib.pyplot as plt from torchvision.utils import make_grid <ul> <li>The original Kaggle dataset: https://www.kaggle.com/datasets/jessicali9530/stanford-cars-dataset?datasetId=30084&amp;sortBy=dateCreated&amp;select=cars_test</li> <li>The devkit: car_devkit.tgz</li> <li>The cars_test_annos_withlabels.mat file: https://www.kaggle.com/code/subhangaupadhaya/pytorch-stanfordcars-classification/input?select=cars_test_annos_withlabels+%281%29.mat</li> </ul> <p>The directory structure you provided earlier works well once we add the missing file!</p> <pre>\u2514\u2500\u2500 stanford_cars\n    \u2514\u2500\u2500 cars_test_annos_withlabels.mat\n    \u2514\u2500\u2500 cars_train\n        \u2514\u2500\u2500 *.jpg\n    \u2514\u2500\u2500 cars_test\n        \u2514\u2500\u2500 .*jpg\n    \u2514\u2500\u2500 devkit\n        \u251c\u2500\u2500 cars_meta.mat\n        \u251c\u2500\u2500 cars_test_annos.mat\n        \u251c\u2500\u2500 cars_train_annos.mat\n        \u251c\u2500\u2500 eval_train.m\n        \u251c\u2500\u2500 README.txt\n        \u2514\u2500\u2500 train_perfect_preds.txt\n</pre> In\u00a0[\u00a0]: Copied! <pre>IMG_SIZE = 64\nBATCH_SIZE = 50 # reduce depends of yout gpu - initial was 100\n\ndef get_dataset(path):\n    data_transform = transforms.Compose(\n        [\n            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n            # We flip horizontally with probability 50%\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            # Scales data into [-1,1] \n            transforms.Normalize(0.5, 0.5)\n        ]\n    )\n    \n    train = torchvision.datasets.StanfordCars(root=path, download=False, \n                                         transform=data_transform)\n\n    test = torchvision.datasets.StanfordCars(root=path, download=False, \n                                         transform=data_transform, split='test')\n    \n    return torch.utils.data.ConcatDataset([train, test])\n\ndata = get_dataset(\"data/\")\ndataloader = DataLoader(\n    data, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    drop_last=False, \n    pin_memory=True, \n    num_workers=multiprocessing.cpu_count(),\n    persistent_workers=True\n)\n</pre> IMG_SIZE = 64 BATCH_SIZE = 50 # reduce depends of yout gpu - initial was 100  def get_dataset(path):     data_transform = transforms.Compose(         [             transforms.Resize((IMG_SIZE, IMG_SIZE)),             # We flip horizontally with probability 50%             transforms.RandomHorizontalFlip(),             transforms.ToTensor(),             # Scales data into [-1,1]              transforms.Normalize(0.5, 0.5)         ]     )          train = torchvision.datasets.StanfordCars(root=path, download=False,                                           transform=data_transform)      test = torchvision.datasets.StanfordCars(root=path, download=False,                                           transform=data_transform, split='test')          return torch.utils.data.ConcatDataset([train, test])  data = get_dataset(\"data/\") dataloader = DataLoader(     data,      batch_size=BATCH_SIZE,      shuffle=True,      drop_last=False,      pin_memory=True,      num_workers=multiprocessing.cpu_count(),     persistent_workers=True ) <p>Let's look at a batch of images:</p> In\u00a0[\u00a0]: Copied! <pre># Get a batch\nbatch, _ = next(iter(dataloader))\n\n# Display it\ndef display_sequence(imgs, dpi=75, nrow=8):\n    \n    fig, sub = plt.subplots(dpi=dpi)\n    sub.imshow(\n        np.transpose(\n            make_grid(\n                imgs, \n                padding=0,\n                normalize=True,\n                nrow=nrow,\n            ).cpu(),\n            (1,2,0)\n        )\n    )\n    _ = sub.axis(\"off\")\n    \n    return fig\n\n_ = display_sequence(batch[:8], dpi=150)\n</pre> # Get a batch batch, _ = next(iter(dataloader))  # Display it def display_sequence(imgs, dpi=75, nrow=8):          fig, sub = plt.subplots(dpi=dpi)     sub.imshow(         np.transpose(             make_grid(                 imgs,                  padding=0,                 normalize=True,                 nrow=nrow,             ).cpu(),             (1,2,0)         )     )     _ = sub.axis(\"off\")          return fig  _ = display_sequence(batch[:8], dpi=150) In\u00a0[\u00a0]: Copied! <pre># Define beta schedule\nT = 512  # number of diffusion steps\n# YOUR CODE HERE\nbetas = torch.linspace(start=0.0001, end=0.02, steps=T)  # linear schedule\n\nplt.plot(range(T), betas.numpy(), label='Beta Values')\nplt.xlabel('Diffusion Step')\nplt.ylabel('Beta Value')\n_ = plt.title('Beta Schedule over Diffusion Steps')\n</pre> # Define beta schedule T = 512  # number of diffusion steps # YOUR CODE HERE betas = torch.linspace(start=0.0001, end=0.02, steps=T)  # linear schedule  plt.plot(range(T), betas.numpy(), label='Beta Values') plt.xlabel('Diffusion Step') plt.ylabel('Beta Value') _ = plt.title('Beta Schedule over Diffusion Steps') <p>As we have seen in the lesson, we need to use a re-parametrization of the forward process that allows us to generate noisy images at any step without having to sequentially go through all the previous steps:</p> <p>$$ \\left\\{ \\begin{align*} \\bar{\\alpha}_t &amp;= \\prod_{s=1}^t (1 - \\beta_s) \\\\ q(x_t | x_0) &amp;= \\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_t} x_0, \\ (1 - \\bar{\\alpha}_t) \\mathbf{I}\\right) \\end{align*} \\right. $$</p> <p>At inference time we will also need the quantities involved in these other formulas:</p> <p>$$ x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}} \\left( x_t - \\frac{1-\\alpha_t}{\\sqrt{1-\\bar{\\alpha}_t}} \\epsilon_\\theta (x_t, t) \\right) + \\sigma_t z $$</p> <p>$$ \\sigma_t^2 = \\frac{(1-\\bar{\\alpha}_t-1)}{(1-\\bar{\\alpha}_t)} \\beta_t $$</p> <p>Here we define and precompute all these constants:</p> In\u00a0[\u00a0]: Copied! <pre># Pre-calculate different terms for closed form\nalphas = 1. - betas\n# alpha bar\nalphas_cumprod = torch.cumprod(alphas, axis=0)\n# alpha bar at t-1\nalphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n# sqrt of alpha bar\nsqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n\n# Inference:\n# 1 / sqrt(alpha)\nsqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n# sqrt of one minus alpha bar\nsqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n# sigma_t\nposterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n</pre> # Pre-calculate different terms for closed form alphas = 1. - betas # alpha bar alphas_cumprod = torch.cumprod(alphas, axis=0) # alpha bar at t-1 alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0) # sqrt of alpha bar sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)  # Inference: # 1 / sqrt(alpha) sqrt_recip_alphas = torch.sqrt(1.0 / alphas) # sqrt of one minus alpha bar sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod) # sigma_t posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod) <p>Here we define two utility functions, one to visualize the forward diffusion process, and the other one to make an inference call on an existing DDPM:</p> <p>Fill the sections marked with YOUR CODE HERE</p> In\u00a0[\u00a0]: Copied! <pre># Make sure CUDA is available (i.e. the GPU is setup correctly)\nassert torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n</pre> # Make sure CUDA is available (i.e. the GPU is setup correctly) assert torch.cuda.is_available() device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") In\u00a0[\u00a0]: Copied! <pre>@torch.no_grad()\ndef forward_diffusion_viz(image, device='cpu', num_images=16, dpi=75, interleave=False):\n\"\"\"\n    Generate the forward sequence of noisy images taking the input image to pure noise\n    \"\"\"\n    # Visualize only num_images diffusion steps, instead of all of them\n    stepsize = int(T/num_images)\n    \n    imgs = []\n    noises = []\n    \n    for i in range(0, T, stepsize):\n        t = torch.full((1,), i, device=device, dtype=torch.long)\n\n        # Forward diffusion process\n        bs = image.shape[0]\n        noise = torch.randn_like(image, device=device)\n        img = (\n            sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * image + \n            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise\n        )\n\n        imgs.append(torch.clamp(img, -1, 1).squeeze(dim=0))\n        noises.append(torch.clamp(noise, -1, 1).squeeze(dim=0))\n    \n    if interleave:\n        imgs = [item for pair in zip(imgs, noises) for item in pair]\n        \n    fig = display_sequence(imgs, dpi=dpi)\n    \n    return fig, imgs[-1]\n\n\n@torch.no_grad()\ndef make_inference(input_noise, return_all=False):\n\"\"\"\n    Implements the sampling algorithm from the DDPM paper\n    \"\"\"\n    \n    x = input_noise\n    bs = x.shape[0]\n    \n    imgs = []\n    \n    # YOUR CODE HERE\n    for time_step in range(0, T)[::-1]:\n        \n        noise = torch.randn_like(x) if time_step &gt; 0 else 0\n        \n        t = torch.full((bs,), time_step, device=device, dtype=torch.long)\n        \n        # YOUR CODE HERE\n        x = sqrt_recip_alphas[t].view(bs, 1, 1, 1) * (\n            x - betas[t].view(bs, 1, 1, 1) * model(x, t) / \n            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1)\n        ) + torch.sqrt(posterior_variance[t].view(bs, 1, 1, 1)) * noise\n        \n        imgs.append(torch.clamp(x, -1, 1))\n    \n    if return_all:\n        return imgs\n    else:\n        return imgs[-1]\n    \n    return x\n</pre> @torch.no_grad() def forward_diffusion_viz(image, device='cpu', num_images=16, dpi=75, interleave=False):     \"\"\"     Generate the forward sequence of noisy images taking the input image to pure noise     \"\"\"     # Visualize only num_images diffusion steps, instead of all of them     stepsize = int(T/num_images)          imgs = []     noises = []          for i in range(0, T, stepsize):         t = torch.full((1,), i, device=device, dtype=torch.long)          # Forward diffusion process         bs = image.shape[0]         noise = torch.randn_like(image, device=device)         img = (             sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * image +              sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise         )          imgs.append(torch.clamp(img, -1, 1).squeeze(dim=0))         noises.append(torch.clamp(noise, -1, 1).squeeze(dim=0))          if interleave:         imgs = [item for pair in zip(imgs, noises) for item in pair]              fig = display_sequence(imgs, dpi=dpi)          return fig, imgs[-1]   @torch.no_grad() def make_inference(input_noise, return_all=False):     \"\"\"     Implements the sampling algorithm from the DDPM paper     \"\"\"          x = input_noise     bs = x.shape[0]          imgs = []          # YOUR CODE HERE     for time_step in range(0, T)[::-1]:                  noise = torch.randn_like(x) if time_step &gt; 0 else 0                  t = torch.full((bs,), time_step, device=device, dtype=torch.long)                  # YOUR CODE HERE         x = sqrt_recip_alphas[t].view(bs, 1, 1, 1) * (             x - betas[t].view(bs, 1, 1, 1) * model(x, t) /              sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1)         ) + torch.sqrt(posterior_variance[t].view(bs, 1, 1, 1)) * noise                  imgs.append(torch.clamp(x, -1, 1))          if return_all:         return imgs     else:         return imgs[-1]          return x In\u00a0[\u00a0]: Copied! <pre>for image in batch[:5]:\n    _ = forward_diffusion_viz(image.unsqueeze(dim=0), num_images=7, dpi=150, interleave=False)\n</pre> for image in batch[:5]:     _ = forward_diffusion_viz(image.unsqueeze(dim=0), num_images=7, dpi=150, interleave=False) In\u00a0[\u00a0]: Copied! <pre>\"\"\"\n---\ntitle: U-Net model for Denoising Diffusion Probabilistic Models (DDPM)\nsummary: &gt;\n  UNet model for Denoising Diffusion Probabilistic Models (DDPM)\n---\n\n# U-Net model for [Denoising Diffusion Probabilistic Models (DDPM)](index.html)\n\nThis is a [U-Net](../../unet/index.html) based model to predict noise\n$\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$.\n\nU-Net is a gets it's name from the U shape in the model diagram.\nIt processes a given image by progressively lowering (halving) the feature map resolution and then\nincreasing the resolution.\nThere are pass-through connection at each resolution.\n\n![U-Net diagram from paper](../../unet/unet.png)\n\nThis implementation contains a bunch of modifications to original U-Net (residual blocks, multi-head attention)\n and also adds time-step embeddings $t$.\nThe MIT License (MIT)\n\nCopyright (c) 2020 Varuna Jayasiri\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n(taken from https://github.com/labmlai/annotated_deep_learning_paper_implementations)\n\"\"\"\n\nimport math\nfrom typing import Optional, Tuple, Union, List\n\nimport torch\nfrom torch import nn\n\n\nclass Swish(nn.Module):\n\"\"\"\n    ### Swish actiavation function\n\n    $$x \\cdot \\sigma(x)$$\n    \"\"\"\n\n    def forward(self, x):\n        return x * torch.sigmoid(x)\n\n\nclass TimeEmbedding(nn.Module):\n\"\"\"\n    ### Embeddings for $t$\n    \"\"\"\n\n    def __init__(self, n_channels: int):\n\"\"\"\n        * `n_channels` is the number of dimensions in the embedding\n        \"\"\"\n        super().__init__()\n        self.n_channels = n_channels\n        # First linear layer\n        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n        # Activation\n        self.act = Swish()\n        # Second linear layer\n        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n\n    def forward(self, t: torch.Tensor):\n        # Create sinusoidal position embeddings\n        # [same as those from the transformer](../../transformers/positional_encoding.html)\n        #\n        # \\begin{align}\n        # PE^{(1)}_{t,i} &amp;= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n        # PE^{(2)}_{t,i} &amp;= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n        # \\end{align}\n        #\n        # where $d$ is `half_dim`\n        half_dim = self.n_channels // 8\n        emb = math.log(10_000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n        emb = t[:, None] * emb[None, :]\n        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n\n        # Transform with the MLP\n        emb = self.act(self.lin1(emb))\n        emb = self.lin2(emb)\n\n        #\n        return emb\n\n\nclass ResidualBlock(nn.Module):\n\"\"\"\n    ### Residual block\n\n    A residual block has two convolution layers with group normalization.\n    Each resolution is processed with two residual blocks.\n    \"\"\"\n\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        time_channels: int,\n        n_groups: int = 32,\n        dropout: float = 0.1,\n    ):\n\"\"\"\n        * `in_channels` is the number of input channels\n        * `out_channels` is the number of input channels\n        * `time_channels` is the number channels in the time step ($t$) embeddings\n        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n        * `dropout` is the dropout rate\n        \"\"\"\n        super().__init__()\n        # Group normalization and the first convolution layer\n        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n        self.act1 = Swish()\n        self.conv1 = nn.Conv2d(\n            in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)\n        )\n\n        # Group normalization and the second convolution layer\n        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n        self.act2 = Swish()\n        self.conv2 = nn.Conv2d(\n            out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)\n        )\n\n        # If the number of input channels is not equal to the number of output channels we have to\n        # project the shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n        else:\n            self.shortcut = nn.Identity()\n\n        # Linear layer for time embeddings\n        self.time_emb = nn.Linear(time_channels, out_channels)\n        self.time_act = Swish()\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n\"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size, time_channels]`\n        \"\"\"\n        # First convolution layer\n        h = self.conv1(self.act1(self.norm1(x)))\n        # Add time embeddings\n        h += self.time_emb(self.time_act(t))[:, :, None, None]\n        # Second convolution layer\n        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n\n        # Add the shortcut connection and return\n        return h + self.shortcut(x)\n\n\nclass AttentionBlock(nn.Module):\n\"\"\"\n    ### Attention block\n\n    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n    \"\"\"\n\n    def __init__(\n        self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32\n    ):\n\"\"\"\n        * `n_channels` is the number of channels in the input\n        * `n_heads` is the number of heads in multi-head attention\n        * `d_k` is the number of dimensions in each head\n        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n        \"\"\"\n        super().__init__()\n\n        # Default `d_k`\n        if d_k is None:\n            d_k = n_channels\n        # Normalization layer\n        self.norm = nn.GroupNorm(n_groups, n_channels)\n        # Projections for query, key and values\n        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n        # Linear layer for final transformation\n        self.output = nn.Linear(n_heads * d_k, n_channels)\n        # Scale for dot-product attention\n        self.scale = d_k**-0.5\n        #\n        self.n_heads = n_heads\n        self.d_k = d_k\n\n    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n\"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size, time_channels]`\n        \"\"\"\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        # Get shape\n        batch_size, n_channels, height, width = x.shape\n        # Change `x` to shape `[batch_size, seq, n_channels]`\n        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n        q, k, v = torch.chunk(qkv, 3, dim=-1)\n        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n        attn = torch.einsum(\"bihd,bjhd-&gt;bijh\", q, k) * self.scale\n        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n        attn = attn.softmax(dim=2)\n        # Multiply by values\n        res = torch.einsum(\"bijh,bjhd-&gt;bihd\", attn, v)\n        # Reshape to `[batch_size, seq, n_heads * d_k]`\n        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n        # Transform to `[batch_size, seq, n_channels]`\n        res = self.output(res)\n\n        # Add skip connection\n        res += x\n\n        # Change to shape `[batch_size, in_channels, height, width]`\n        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n\n        #\n        return res\n\n\nclass DownBlock(nn.Module):\n\"\"\"\n    ### Down block\n\n    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(\n        self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool\n    ):\n        super().__init__()\n        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res(x, t)\n        x = self.attn(x)\n        return x\n\n\nclass UpBlock(nn.Module):\n\"\"\"\n    ### Up block\n\n    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n    \"\"\"\n\n    def __init__(\n        self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool\n    ):\n        super().__init__()\n        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n        # from the first half of the U-Net\n        self.res = ResidualBlock(\n            in_channels + out_channels, out_channels, time_channels\n        )\n        if has_attn:\n            self.attn = AttentionBlock(out_channels)\n        else:\n            self.attn = nn.Identity()\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res(x, t)\n        x = self.attn(x)\n        return x\n\n\nclass MiddleBlock(nn.Module):\n\"\"\"\n    ### Middle block\n\n    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n    This block is applied at the lowest resolution of the U-Net.\n    \"\"\"\n\n    def __init__(self, n_channels: int, time_channels: int):\n        super().__init__()\n        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n        self.attn = AttentionBlock(n_channels)\n        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        x = self.res1(x, t)\n        x = self.attn(x)\n        x = self.res2(x, t)\n        return x\n\n\nclass Upsample(nn.Module):\n\"\"\"\n    ### Scale up the feature map by $2 \\times$\n    \"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        return self.conv(x)\n\n\nclass Downsample(nn.Module):\n\"\"\"\n    ### Scale down the feature map by $\\frac{1}{2} \\times$\n    \"\"\"\n\n    def __init__(self, n_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n        # to match with `ResidualBlock`.\n        _ = t\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n\"\"\"\n    ## U-Net\n    \"\"\"\n\n    def __init__(\n        self,\n        image_channels: int = 3,\n        n_channels: int = 64,\n        ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n        is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),\n        n_blocks: int = 2,\n    ):\n\"\"\"\n        * `image_channels` is the number of channels in the image. $3$ for RGB.\n        * `n_channels` is number of channels in the initial feature map that we transform the image into\n        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n        \"\"\"\n        super().__init__()\n\n        # Number of resolutions\n        n_resolutions = len(ch_mults)\n\n        # Project image into feature map\n        self.image_proj = nn.Conv2d(\n            image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1)\n        )\n\n        # Time embedding layer. Time embedding has `n_channels * 4` channels\n        self.time_emb = TimeEmbedding(n_channels * 4)\n\n        # #### First half of U-Net - decreasing resolution\n        down = []\n        # Number of channels\n        out_channels = in_channels = n_channels\n        # For each resolution\n        for i in range(n_resolutions):\n            # Number of output channels at this resolution\n            out_channels = in_channels * ch_mults[i]\n            # Add `n_blocks`\n            for _ in range(n_blocks):\n                down.append(\n                    DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i])\n                )\n                in_channels = out_channels\n            # Down sample at all resolutions except the last\n            if i &lt; n_resolutions - 1:\n                down.append(Downsample(in_channels))\n\n        # Combine the set of modules\n        self.down = nn.ModuleList(down)\n\n        # Middle block\n        self.middle = MiddleBlock(\n            out_channels,\n            n_channels * 4,\n        )\n\n        # #### Second half of U-Net - increasing resolution\n        up = []\n        # Number of channels\n        in_channels = out_channels\n        # For each resolution\n        for i in reversed(range(n_resolutions)):\n            # `n_blocks` at the same resolution\n            out_channels = in_channels\n            for _ in range(n_blocks):\n                up.append(\n                    UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i])\n                )\n            # Final block to reduce the number of channels\n            out_channels = in_channels // ch_mults[i]\n            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n            in_channels = out_channels\n            # Up sample at all resolutions except last\n            if i &gt; 0:\n                up.append(Upsample(in_channels))\n\n        # Combine the set of modules\n        self.up = nn.ModuleList(up)\n\n        # Final normalization and convolution layer\n        self.norm = nn.GroupNorm(8, n_channels)\n        self.act = Swish()\n        self.final = nn.Conv2d(\n            in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1)\n        )\n\n    def forward(self, x: torch.Tensor, t: torch.Tensor):\n\"\"\"\n        * `x` has shape `[batch_size, in_channels, height, width]`\n        * `t` has shape `[batch_size]`\n        \"\"\"\n\n        # Get time-step embeddings\n        t = self.time_emb(t)\n\n        # Get image projection\n        x = self.image_proj(x)\n\n        # `h` will store outputs at each resolution for skip connection\n        h = [x]\n        # First half of U-Net\n        for m in self.down:\n            x = m(x, t)\n            h.append(x)\n\n        # Middle (bottom)\n        x = self.middle(x, t)\n\n        # Second half of U-Net\n        for m in self.up:\n            if isinstance(m, Upsample):\n                x = m(x, t)\n            else:\n                # Get the skip connection from first half of U-Net and concatenate\n                s = h.pop()\n                x = torch.cat((x, s), dim=1)\n                #\n                x = m(x, t)\n\n        # Final normalization and convolution\n        return self.final(self.act(self.norm(x)))\n</pre> \"\"\" --- title: U-Net model for Denoising Diffusion Probabilistic Models (DDPM) summary: &gt;   UNet model for Denoising Diffusion Probabilistic Models (DDPM) ---  # U-Net model for [Denoising Diffusion Probabilistic Models (DDPM)](index.html)  This is a [U-Net](../../unet/index.html) based model to predict noise $\\textcolor{lightgreen}{\\epsilon_\\theta}(x_t, t)$.  U-Net is a gets it's name from the U shape in the model diagram. It processes a given image by progressively lowering (halving) the feature map resolution and then increasing the resolution. There are pass-through connection at each resolution.  ![U-Net diagram from paper](../../unet/unet.png)  This implementation contains a bunch of modifications to original U-Net (residual blocks, multi-head attention)  and also adds time-step embeddings $t$.   The MIT License (MIT)  Copyright (c) 2020 Varuna Jayasiri  Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:  The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.  THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.  (taken from https://github.com/labmlai/annotated_deep_learning_paper_implementations) \"\"\"  import math from typing import Optional, Tuple, Union, List  import torch from torch import nn   class Swish(nn.Module):     \"\"\"     ### Swish actiavation function      $$x \\cdot \\sigma(x)$$     \"\"\"      def forward(self, x):         return x * torch.sigmoid(x)   class TimeEmbedding(nn.Module):     \"\"\"     ### Embeddings for $t$     \"\"\"      def __init__(self, n_channels: int):         \"\"\"         * `n_channels` is the number of dimensions in the embedding         \"\"\"         super().__init__()         self.n_channels = n_channels         # First linear layer         self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)         # Activation         self.act = Swish()         # Second linear layer         self.lin2 = nn.Linear(self.n_channels, self.n_channels)      def forward(self, t: torch.Tensor):         # Create sinusoidal position embeddings         # [same as those from the transformer](../../transformers/positional_encoding.html)         #         # \\begin{align}         # PE^{(1)}_{t,i} &amp;= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\         # PE^{(2)}_{t,i} &amp;= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)         # \\end{align}         #         # where $d$ is `half_dim`         half_dim = self.n_channels // 8         emb = math.log(10_000) / (half_dim - 1)         emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)         emb = t[:, None] * emb[None, :]         emb = torch.cat((emb.sin(), emb.cos()), dim=1)          # Transform with the MLP         emb = self.act(self.lin1(emb))         emb = self.lin2(emb)          #         return emb   class ResidualBlock(nn.Module):     \"\"\"     ### Residual block      A residual block has two convolution layers with group normalization.     Each resolution is processed with two residual blocks.     \"\"\"      def __init__(         self,         in_channels: int,         out_channels: int,         time_channels: int,         n_groups: int = 32,         dropout: float = 0.1,     ):         \"\"\"         * `in_channels` is the number of input channels         * `out_channels` is the number of input channels         * `time_channels` is the number channels in the time step ($t$) embeddings         * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)         * `dropout` is the dropout rate         \"\"\"         super().__init__()         # Group normalization and the first convolution layer         self.norm1 = nn.GroupNorm(n_groups, in_channels)         self.act1 = Swish()         self.conv1 = nn.Conv2d(             in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)         )          # Group normalization and the second convolution layer         self.norm2 = nn.GroupNorm(n_groups, out_channels)         self.act2 = Swish()         self.conv2 = nn.Conv2d(             out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1)         )          # If the number of input channels is not equal to the number of output channels we have to         # project the shortcut connection         if in_channels != out_channels:             self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))         else:             self.shortcut = nn.Identity()          # Linear layer for time embeddings         self.time_emb = nn.Linear(time_channels, out_channels)         self.time_act = Swish()          self.dropout = nn.Dropout(dropout)      def forward(self, x: torch.Tensor, t: torch.Tensor):         \"\"\"         * `x` has shape `[batch_size, in_channels, height, width]`         * `t` has shape `[batch_size, time_channels]`         \"\"\"         # First convolution layer         h = self.conv1(self.act1(self.norm1(x)))         # Add time embeddings         h += self.time_emb(self.time_act(t))[:, :, None, None]         # Second convolution layer         h = self.conv2(self.dropout(self.act2(self.norm2(h))))          # Add the shortcut connection and return         return h + self.shortcut(x)   class AttentionBlock(nn.Module):     \"\"\"     ### Attention block      This is similar to [transformer multi-head attention](../../transformers/mha.html).     \"\"\"      def __init__(         self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 32     ):         \"\"\"         * `n_channels` is the number of channels in the input         * `n_heads` is the number of heads in multi-head attention         * `d_k` is the number of dimensions in each head         * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)         \"\"\"         super().__init__()          # Default `d_k`         if d_k is None:             d_k = n_channels         # Normalization layer         self.norm = nn.GroupNorm(n_groups, n_channels)         # Projections for query, key and values         self.projection = nn.Linear(n_channels, n_heads * d_k * 3)         # Linear layer for final transformation         self.output = nn.Linear(n_heads * d_k, n_channels)         # Scale for dot-product attention         self.scale = d_k**-0.5         #         self.n_heads = n_heads         self.d_k = d_k      def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):         \"\"\"         * `x` has shape `[batch_size, in_channels, height, width]`         * `t` has shape `[batch_size, time_channels]`         \"\"\"         # `t` is not used, but it's kept in the arguments because for the attention layer function signature         # to match with `ResidualBlock`.         _ = t         # Get shape         batch_size, n_channels, height, width = x.shape         # Change `x` to shape `[batch_size, seq, n_channels]`         x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)         # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`         qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)         # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`         q, k, v = torch.chunk(qkv, 3, dim=-1)         # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$         attn = torch.einsum(\"bihd,bjhd-&gt;bijh\", q, k) * self.scale         # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$         attn = attn.softmax(dim=2)         # Multiply by values         res = torch.einsum(\"bijh,bjhd-&gt;bihd\", attn, v)         # Reshape to `[batch_size, seq, n_heads * d_k]`         res = res.view(batch_size, -1, self.n_heads * self.d_k)         # Transform to `[batch_size, seq, n_channels]`         res = self.output(res)          # Add skip connection         res += x          # Change to shape `[batch_size, in_channels, height, width]`         res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)          #         return res   class DownBlock(nn.Module):     \"\"\"     ### Down block      This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.     \"\"\"      def __init__(         self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool     ):         super().__init__()         self.res = ResidualBlock(in_channels, out_channels, time_channels)         if has_attn:             self.attn = AttentionBlock(out_channels)         else:             self.attn = nn.Identity()      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.res(x, t)         x = self.attn(x)         return x   class UpBlock(nn.Module):     \"\"\"     ### Up block      This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.     \"\"\"      def __init__(         self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool     ):         super().__init__()         # The input has `in_channels + out_channels` because we concatenate the output of the same resolution         # from the first half of the U-Net         self.res = ResidualBlock(             in_channels + out_channels, out_channels, time_channels         )         if has_attn:             self.attn = AttentionBlock(out_channels)         else:             self.attn = nn.Identity()      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.res(x, t)         x = self.attn(x)         return x   class MiddleBlock(nn.Module):     \"\"\"     ### Middle block      It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.     This block is applied at the lowest resolution of the U-Net.     \"\"\"      def __init__(self, n_channels: int, time_channels: int):         super().__init__()         self.res1 = ResidualBlock(n_channels, n_channels, time_channels)         self.attn = AttentionBlock(n_channels)         self.res2 = ResidualBlock(n_channels, n_channels, time_channels)      def forward(self, x: torch.Tensor, t: torch.Tensor):         x = self.res1(x, t)         x = self.attn(x)         x = self.res2(x, t)         return x   class Upsample(nn.Module):     \"\"\"     ### Scale up the feature map by $2 \\times$     \"\"\"      def __init__(self, n_channels):         super().__init__()         self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))      def forward(self, x: torch.Tensor, t: torch.Tensor):         # `t` is not used, but it's kept in the arguments because for the attention layer function signature         # to match with `ResidualBlock`.         _ = t         return self.conv(x)   class Downsample(nn.Module):     \"\"\"     ### Scale down the feature map by $\\frac{1}{2} \\times$     \"\"\"      def __init__(self, n_channels):         super().__init__()         self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))      def forward(self, x: torch.Tensor, t: torch.Tensor):         # `t` is not used, but it's kept in the arguments because for the attention layer function signature         # to match with `ResidualBlock`.         _ = t         return self.conv(x)   class UNet(nn.Module):     \"\"\"     ## U-Net     \"\"\"      def __init__(         self,         image_channels: int = 3,         n_channels: int = 64,         ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),         is_attn: Union[Tuple[bool, ...], List[int]] = (False, False, True, True),         n_blocks: int = 2,     ):         \"\"\"         * `image_channels` is the number of channels in the image. $3$ for RGB.         * `n_channels` is number of channels in the initial feature map that we transform the image into         * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`         * `is_attn` is a list of booleans that indicate whether to use attention at each resolution         * `n_blocks` is the number of `UpDownBlocks` at each resolution         \"\"\"         super().__init__()          # Number of resolutions         n_resolutions = len(ch_mults)          # Project image into feature map         self.image_proj = nn.Conv2d(             image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1)         )          # Time embedding layer. Time embedding has `n_channels * 4` channels         self.time_emb = TimeEmbedding(n_channels * 4)          # #### First half of U-Net - decreasing resolution         down = []         # Number of channels         out_channels = in_channels = n_channels         # For each resolution         for i in range(n_resolutions):             # Number of output channels at this resolution             out_channels = in_channels * ch_mults[i]             # Add `n_blocks`             for _ in range(n_blocks):                 down.append(                     DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i])                 )                 in_channels = out_channels             # Down sample at all resolutions except the last             if i &lt; n_resolutions - 1:                 down.append(Downsample(in_channels))          # Combine the set of modules         self.down = nn.ModuleList(down)          # Middle block         self.middle = MiddleBlock(             out_channels,             n_channels * 4,         )          # #### Second half of U-Net - increasing resolution         up = []         # Number of channels         in_channels = out_channels         # For each resolution         for i in reversed(range(n_resolutions)):             # `n_blocks` at the same resolution             out_channels = in_channels             for _ in range(n_blocks):                 up.append(                     UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i])                 )             # Final block to reduce the number of channels             out_channels = in_channels // ch_mults[i]             up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))             in_channels = out_channels             # Up sample at all resolutions except last             if i &gt; 0:                 up.append(Upsample(in_channels))          # Combine the set of modules         self.up = nn.ModuleList(up)          # Final normalization and convolution layer         self.norm = nn.GroupNorm(8, n_channels)         self.act = Swish()         self.final = nn.Conv2d(             in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1)         )      def forward(self, x: torch.Tensor, t: torch.Tensor):         \"\"\"         * `x` has shape `[batch_size, in_channels, height, width]`         * `t` has shape `[batch_size]`         \"\"\"          # Get time-step embeddings         t = self.time_emb(t)          # Get image projection         x = self.image_proj(x)          # `h` will store outputs at each resolution for skip connection         h = [x]         # First half of U-Net         for m in self.down:             x = m(x, t)             h.append(x)          # Middle (bottom)         x = self.middle(x, t)          # Second half of U-Net         for m in self.up:             if isinstance(m, Upsample):                 x = m(x, t)             else:                 # Get the skip connection from first half of U-Net and concatenate                 s = h.pop()                 x = torch.cat((x, s), dim=1)                 #                 x = m(x, t)          # Final normalization and convolution         return self.final(self.act(self.norm(x))) In\u00a0[\u00a0]: Copied! <pre>model = UNet(ch_mults = (1, 2, 1, 1))\n\n# Uncomment this\n# if you want to do the _VERY_ long training,\n# model = UNet(ch_mults = (1, 2, 2, 2))\n\nn_params = sum(p.numel() for p in model.parameters())\nprint(\n    f\"Number of parameters: {n_params:,}\"\n)\n</pre> model = UNet(ch_mults = (1, 2, 1, 1))  # Uncomment this # if you want to do the _VERY_ long training, # model = UNet(ch_mults = (1, 2, 2, 2))  n_params = sum(p.numel() for p in model.parameters()) print(     f\"Number of parameters: {n_params:,}\" ) <p>Our model has around 9.1 Million parameters. When compared to Stable Diffusion, which has 1 Billion parameters, it is very small! However, for this dataset, it can still give remarkable results.</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# Move everything to GPU\nmodel.to(device)\n\nsqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device)\nalphas = alphas.to(device)\nalphas_cumprod = alphas_cumprod.to(device)\nalphas_cumprod_prev = alphas_cumprod_prev.to(device)\nsqrt_recip_alphas = sqrt_recip_alphas.to(device)\nsqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device)\nsqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device)\nposterior_variance = posterior_variance.to(device)\nbetas = betas.to(device)\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Move everything to GPU model.to(device)  sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device) alphas = alphas.to(device) alphas_cumprod = alphas_cumprod.to(device) alphas_cumprod_prev = alphas_cumprod_prev.to(device) sqrt_recip_alphas = sqrt_recip_alphas.to(device) sqrt_alphas_cumprod = sqrt_alphas_cumprod.to(device) sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.to(device) posterior_variance = posterior_variance.to(device) betas = betas.to(device) <p>Now we can define the loss we are going to minimize:</p> <p>Complete the section marked with YOUR CODE HERE</p> In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\ncriterion = torch.nn.MSELoss()\n</pre> # YOUR CODE HERE criterion = torch.nn.MSELoss() <p>Then we define a few parameters for our training. We are going to use Cosine Annealing for the learning rate, with a warmup period. This means that we are going to start from a very low learning rate, increase it linearly for a few epochs, then start decreasing it again with a cosine shape:</p> In\u00a0[\u00a0]: Copied! <pre>base_lr = 0.0006 # Maximum learning rate we will use\nepochs = 300 # Total number of epochs\nT_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle\nwarmup_epochs = 2  # Number of warm-up epochs\n\n# Uncomment the following lines\n# if you want to do the _VERY_ long training,\n# base_lr = 0.0001 # Maximum learning rate we will use\n# epochs = 300 # Total number of epochs\n# T_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle\n# warmup_epochs = 10  # Number of warm-up epochs\n\n\noptimizer = Adam(model.parameters(), lr=base_lr)\nscheduler = CosineAnnealingLR(\n    optimizer, \n    T_max=T_max - warmup_epochs,\n    eta_min=base_lr / 10  # starting value for the LR\n)\n</pre> base_lr = 0.0006 # Maximum learning rate we will use epochs = 300 # Total number of epochs T_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle warmup_epochs = 2  # Number of warm-up epochs  # Uncomment the following lines # if you want to do the _VERY_ long training, # base_lr = 0.0001 # Maximum learning rate we will use # epochs = 300 # Total number of epochs # T_max = epochs  # Number of epochs for Cosine Annealing. We do only one cycle # warmup_epochs = 10  # Number of warm-up epochs   optimizer = Adam(model.parameters(), lr=base_lr) scheduler = CosineAnnealingLR(     optimizer,      T_max=T_max - warmup_epochs,     eta_min=base_lr / 10  # starting value for the LR ) <p>Finally let's train! We train only for 5 epochs, which should mean around 20 min of training time. This won't get us to a good result, but you will see a few hints of cars appearing little by little.</p> <p>Complete the section marked with YOUR CODE HERE</p> In\u00a0[\u00a0]: Copied! <pre># We will use this noise to generate some images during training to check\n# where we stand\nfixed_noise = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)\n\nalpha = 0.1  # Smoothing factor\nema_loss = None  # Initialize EMA loss\n\nfor epoch in range(epochs):\n    \n    if epoch &lt; warmup_epochs:\n        # Linear warm-up\n        lr = base_lr * (epoch + 1) / warmup_epochs\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n    else:\n        # Cosine Annealing after warm-up\n        scheduler.step()\n\n    current_lr = optimizer.param_groups[0]['lr']\n        \n    for batch, _ in tqdm(dataloader):\n        \n        batch = batch.to(device)\n        bs = batch.shape[0]\n        \n        optimizer.zero_grad()\n        \n        # YOUR CODE HERE\n        t = torch.randint(0, T, (batch.shape[0],), device=device).long()\n        \n        # Generate targets for the UNet and apply them to the images\n        noise = torch.randn_like(batch, device=device)\n        x_noisy = (\n            sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * batch + \n            sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise\n        )\n        \n        noise_pred = model(x_noisy, t)\n        loss = criterion(noise, noise_pred)\n        \n        loss.backward()\n        optimizer.step()\n        \n        if ema_loss is None:\n            # First batch\n            ema_loss = loss.item()\n        else:\n            # Exponential moving average of the loss\n            ema_loss = alpha * loss.item() + (1 - alpha) * ema_loss\n    \n    if epoch == epochs-1:\n        with torch.no_grad():\n    #         fig, _ = sample_image(fixed_noise, forward=False, device=device)\n            imgs = make_inference(fixed_noise, return_all=True)\n            fig = display_sequence([imgs[0].squeeze(dim=0)] + [x.squeeze(dim=0) for x in imgs[63::64]], nrow=9, dpi=150)\n            plt.show(fig)\n        os.makedirs(\"diffusion_output_long\", exist_ok=True)\n        fig.savefig(f\"diffusion_output_long/frame_{epoch:05d}.png\")\n    #plt.close(fig)\n    \n    print(f\"epoch {epoch+1}: loss: {ema_loss:.3f}, lr: {current_lr:.6f}\")\n</pre> # We will use this noise to generate some images during training to check # where we stand fixed_noise = torch.randn((1, 3, IMG_SIZE, IMG_SIZE), device=device)  alpha = 0.1  # Smoothing factor ema_loss = None  # Initialize EMA loss  for epoch in range(epochs):          if epoch &lt; warmup_epochs:         # Linear warm-up         lr = base_lr * (epoch + 1) / warmup_epochs         for param_group in optimizer.param_groups:             param_group['lr'] = lr     else:         # Cosine Annealing after warm-up         scheduler.step()      current_lr = optimizer.param_groups[0]['lr']              for batch, _ in tqdm(dataloader):                  batch = batch.to(device)         bs = batch.shape[0]                  optimizer.zero_grad()                  # YOUR CODE HERE         t = torch.randint(0, T, (batch.shape[0],), device=device).long()                  # Generate targets for the UNet and apply them to the images         noise = torch.randn_like(batch, device=device)         x_noisy = (             sqrt_alphas_cumprod[t].view(bs, 1, 1, 1) * batch +              sqrt_one_minus_alphas_cumprod[t].view(bs, 1, 1, 1) * noise         )                  noise_pred = model(x_noisy, t)         loss = criterion(noise, noise_pred)                  loss.backward()         optimizer.step()                  if ema_loss is None:             # First batch             ema_loss = loss.item()         else:             # Exponential moving average of the loss             ema_loss = alpha * loss.item() + (1 - alpha) * ema_loss          if epoch == epochs-1:         with torch.no_grad():     #         fig, _ = sample_image(fixed_noise, forward=False, device=device)             imgs = make_inference(fixed_noise, return_all=True)             fig = display_sequence([imgs[0].squeeze(dim=0)] + [x.squeeze(dim=0) for x in imgs[63::64]], nrow=9, dpi=150)             plt.show(fig)         os.makedirs(\"diffusion_output_long\", exist_ok=True)         fig.savefig(f\"diffusion_output_long/frame_{epoch:05d}.png\")     #plt.close(fig)          print(f\"epoch {epoch+1}: loss: {ema_loss:.3f}, lr: {current_lr:.6f}\") In\u00a0[\u00a0]: Copied! <pre># YOUR CODE HERE\ninput_noise = torch.randn((8, 3, IMG_SIZE, IMG_SIZE), device=device)\nimgs = make_inference(input_noise)\n_ = display_sequence(imgs, dpi=75, nrow=4)\n</pre> # YOUR CODE HERE input_noise = torch.randn((8, 3, IMG_SIZE, IMG_SIZE), device=device) imgs = make_inference(input_noise) _ = display_sequence(imgs, dpi=75, nrow=4) <p>This is a fairly good result considering how small the model is and how little we trained it. We can already tell that it is indeed creating cars, with windshields and wheels, although it is still very early on. If we were to train for much longer, and/or use a larger model (for example, the one defined above in the commented lines has 55 Million parameters) and let it train for several hours, we would get something even better, like this:</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#train-a-denoising-diffusion-probabilistic-model-from-scratch", "title": "Train a Denoising Diffusion Probabilistic Model from scratch\u00b6", "text": "<p>Welcome! In this exercise you will train a DDPM from scratch. After training the model will be able to generate images of cars.</p> <p>Let's get started!</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#initial-setup", "title": "Initial setup\u00b6", "text": "<p>Here we import a few modules and we set up the notebook so it is fully reproducible:</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#dataset", "title": "Dataset\u00b6", "text": "<p>Let's start by loading our training dataset. We are going to use the Stanford Cars dataset. It consists of 196 classes of cars with a total of 16,185 images. For this exercise we do not need any label, and we also do not need a test dataset, so we are going to load both the training and the test dataset and concatenate them. We are also going to transform the images to 64x64 so the exercise can complete more quickly:</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#noise-scheduling-and-precomputation", "title": "Noise scheduling and precomputation\u00b6", "text": "<p>In the forward process we need to add random noise according to a schedule. Here we use a linear schedule with 512 diffusion steps.</p> <p>Let's define it:</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#forward-process", "title": "Forward process\u00b6", "text": "<p>Let's now simulate our forward process. If everything went well, you should see a few images like this one:</p> <p>which show a few of the diffusion steps, from the original image to the left all the way to pure noise to the right.</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#training", "title": "Training\u00b6", "text": ""}, {"location": "generative_ai/tutorials/training_DDPM/#model-definition", "title": "Model definition\u00b6", "text": "<p>Here we define the model we are going to train. We import a simple UNet model from the <code>unet.py</code> file (you can look into it if you like, but it is not required):</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#training-loop", "title": "Training loop\u00b6", "text": "<p>Let's now do some preparation for the training loop. First we transfer the model as well as all our precomputed quantities to the GPU, so they can be used efficiently during training:</p>"}, {"location": "generative_ai/tutorials/training_DDPM/#inference", "title": "Inference\u00b6", "text": "<p>We can now have a look at what our model can produce:</p> <p>Complete the section marked with YOUR CODE HERE</p>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/", "title": "Exercise: Transfer learning using MobileNetV3", "text": "In\u00a0[1]: Copied! <pre># Load the Fashion-MNIST dataset\n\nimport torch\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\n\ndef load_data(batch_size, data_dir=\"data\"):\n\"\"\"Load the Fashion-MNIST dataset.\"\"\"\n\n    # Define transforms to normalize the data\n    transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n    )\n\n    # Download and load the training data\n    trainset = datasets.FashionMNIST(\n        data_dir, download=True, train=True, transform=transform\n    )\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=batch_size, shuffle=True\n    )\n\n    # Download and load the test data\n    testset = datasets.FashionMNIST(\n        data_dir, download=True, train=False, transform=transform\n    )\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=batch_size, shuffle=True\n    )\n\n    return trainloader, testloader\n\n\ntrainloader, testloader = load_data(64)\n</pre> # Load the Fashion-MNIST dataset  import torch import torchvision.datasets as datasets import torchvision.transforms as transforms   def load_data(batch_size, data_dir=\"data\"):     \"\"\"Load the Fashion-MNIST dataset.\"\"\"      # Define transforms to normalize the data     transform = transforms.Compose(         [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]     )      # Download and load the training data     trainset = datasets.FashionMNIST(         data_dir, download=True, train=True, transform=transform     )     trainloader = torch.utils.data.DataLoader(         trainset, batch_size=batch_size, shuffle=True     )      # Download and load the test data     testset = datasets.FashionMNIST(         data_dir, download=True, train=False, transform=transform     )     testloader = torch.utils.data.DataLoader(         testset, batch_size=batch_size, shuffle=True     )      return trainloader, testloader   trainloader, testloader = load_data(64) <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n</pre> <p>Sometimes it's useful to create functions that will help us work with the labels when they're a little more complicated than the handwritten digits 0-9. Let's create those now.</p> In\u00a0[2]: Copied! <pre># Define some helper functions to helps with the labels\ndef get_class_names():\n\"\"\"Return the list of classes in the Fashion-MNIST dataset.\"\"\"\n    return [\n        \"T-shirt/top\",\n        \"Trouser\",\n        \"Pullover\",\n        \"Dress\",\n        \"Coat\",\n        \"Sandal\",\n        \"Shirt\",\n        \"Sneaker\",\n        \"Bag\",\n        \"Ankle boot\",\n    ]\n\n\ndef get_class_name(class_index):\n\"\"\"Return the class name for the given index.\"\"\"\n    return get_class_names()[class_index]\n\n\ndef get_class_index(class_name):\n\"\"\"Return the class index for the given name.\"\"\"\n    return get_class_names().index(class_name)\n\n\nfor class_index in range(10):\n    print(f\"class_index={class_index}, class_name={get_class_name(class_index)}\")\n</pre> # Define some helper functions to helps with the labels def get_class_names():     \"\"\"Return the list of classes in the Fashion-MNIST dataset.\"\"\"     return [         \"T-shirt/top\",         \"Trouser\",         \"Pullover\",         \"Dress\",         \"Coat\",         \"Sandal\",         \"Shirt\",         \"Sneaker\",         \"Bag\",         \"Ankle boot\",     ]   def get_class_name(class_index):     \"\"\"Return the class name for the given index.\"\"\"     return get_class_names()[class_index]   def get_class_index(class_name):     \"\"\"Return the class index for the given name.\"\"\"     return get_class_names().index(class_name)   for class_index in range(10):     print(f\"class_index={class_index}, class_name={get_class_name(class_index)}\") <pre>class_index=0, class_name=T-shirt/top\nclass_index=1, class_name=Trouser\nclass_index=2, class_name=Pullover\nclass_index=3, class_name=Dress\nclass_index=4, class_name=Coat\nclass_index=5, class_name=Sandal\nclass_index=6, class_name=Shirt\nclass_index=7, class_name=Sneaker\nclass_index=8, class_name=Bag\nclass_index=9, class_name=Ankle boot\n</pre> <p>It's always good to inspect your data before you use it to train a model just to know everything is fine. You know what they say: garbage in, garbage out.</p> In\u00a0[6]: Copied! <pre># Show 10 images from the training set with their labels\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\n# function to show an image\ndef imshow(img):\n    img = img / 2 + 0.5  # unnormalize\n    npimg = img.numpy()  # convert from tensor to numpy array\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # transpose dimensions\n\n\nimages, labels = next(iter(trainloader))  # get the first batch\n\n# show images with labels\nfig = plt.figure(figsize=(15, 4))\nplot_size = 10\n\nfor idx in np.arange(plot_size):\n    ax = fig.add_subplot(2, plot_size // 2, idx + 1, xticks=[], yticks=[])\n    imshow(images[idx])\n    ax.set_title(get_class_name(int(labels[idx])))\n</pre> # Show 10 images from the training set with their labels  import matplotlib.pyplot as plt import numpy as np   # function to show an image def imshow(img):     img = img / 2 + 0.5  # unnormalize     npimg = img.numpy()  # convert from tensor to numpy array     plt.imshow(np.transpose(npimg, (1, 2, 0)))  # transpose dimensions   images, labels = next(iter(trainloader))  # get the first batch  # show images with labels fig = plt.figure(figsize=(15, 4)) plot_size = 10  for idx in np.arange(plot_size):     ax = fig.add_subplot(2, plot_size // 2, idx + 1, xticks=[], yticks=[])     imshow(images[idx])     ax.set_title(get_class_name(int(labels[idx]))) In\u00a0[7]: Copied! <pre># Load a pre-trained MobileNetV3 and inspect its structure\nimport torchvision.models as models\n\nmobilenet_v3_model = models.mobilenet_v3_small(pretrained=True)\nprint(mobilenet_v3_model)\n</pre> # Load a pre-trained MobileNetV3 and inspect its structure import torchvision.models as models  mobilenet_v3_model = models.mobilenet_v3_small(pretrained=True) print(mobilenet_v3_model) <pre>MobileNetV3(\n  (features): Sequential(\n    (0): Conv2dNormActivation(\n      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n    (1): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (2): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (3): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n        )\n        (2): Conv2dNormActivation(\n          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (4): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (5): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (6): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (7): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (8): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (9): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (10): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (11): InvertedResidual(\n      (block): Sequential(\n        (0): Conv2dNormActivation(\n          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (1): Conv2dNormActivation(\n          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          (2): Hardswish()\n        )\n        (2): SqueezeExcitation(\n          (avgpool): AdaptiveAvgPool2d(output_size=1)\n          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n          (activation): ReLU()\n          (scale_activation): Hardsigmoid()\n        )\n        (3): Conv2dNormActivation(\n          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        )\n      )\n    )\n    (12): Conv2dNormActivation(\n      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n      (2): Hardswish()\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=1)\n  (classifier): Sequential(\n    (0): Linear(in_features=576, out_features=1024, bias=True)\n    (1): Hardswish()\n    (2): Dropout(p=0.2, inplace=True)\n    (3): Linear(in_features=1024, out_features=1000, bias=True)\n  )\n)\n</pre> <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n</pre> <p>Take note of the <code>classifier</code> section of the model.</p> <pre><code>  (classifier): Sequential(\n    (0): Linear(in_features=576, out_features=1024, bias=True)\n    (1): Hardswish()\n    (2): Dropout(p=0.2, inplace=True)\n    (3): Linear(in_features=1024, out_features=1000, bias=True)\n  )\n</code></pre> <p>There are 1000 output features, but our dataset does not have that many. See if you can complete the next cell so that it has the right number of output nodes.</p> In\u00a0[9]: Copied! <pre>import torch.nn.functional as F\nimport torchvision.models as models\nfrom torch import nn\n\n\n# Define a model class that extends the nn.Module class\nclass MobileNetV3(nn.Module):\n    def __init__(self):\n        super(MobileNetV3, self).__init__()\n\n        # Load the pre-trained MobileNetV3 (Small) architecture\n        self.model = models.mobilenet_v3_small(pretrained=True)\n\n        # Replace the last fully-connected layer with a new one of the right size\n        self.model.classifier[3] = nn.Linear(1024, 10)\n\n        # Freeze all the weights of the network except for the last fully-connected layer\n        self.freeze()\n\n    def forward(self, x):\n        # Convert 1x28x28 input tensor to 3x28x28 tensor, to convert it to a color image\n        x = x.repeat(1, 3, 1, 1)\n\n        # Resize the input to 224x224, since MobileNetV3 (Small) expects images of that size\n        if x.shape[2:] != (224, 224):\n            x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)\n\n        # Forward pass\n        return self.model(x)\n\n    def freeze(self):\n        # Freeze all the weights of the network except for the last fully-connected layer\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        # Unfreeze the final layer\n        for param in self.model.classifier[3].parameters():\n            param.requires_grad = True\n\n    def unfreeze(self):\n        # Unfreeze all the weights of the network\n        for param in self.model.parameters():\n            param.requires_grad = True\n\n\n# Create an instance of the MobileNetV3 model\nmodel = MobileNetV3()\nprint(model)\n</pre> import torch.nn.functional as F import torchvision.models as models from torch import nn   # Define a model class that extends the nn.Module class class MobileNetV3(nn.Module):     def __init__(self):         super(MobileNetV3, self).__init__()          # Load the pre-trained MobileNetV3 (Small) architecture         self.model = models.mobilenet_v3_small(pretrained=True)          # Replace the last fully-connected layer with a new one of the right size         self.model.classifier[3] = nn.Linear(1024, 10)          # Freeze all the weights of the network except for the last fully-connected layer         self.freeze()      def forward(self, x):         # Convert 1x28x28 input tensor to 3x28x28 tensor, to convert it to a color image         x = x.repeat(1, 3, 1, 1)          # Resize the input to 224x224, since MobileNetV3 (Small) expects images of that size         if x.shape[2:] != (224, 224):             x = F.interpolate(x, size=(224, 224), mode=\"bilinear\", align_corners=False)          # Forward pass         return self.model(x)      def freeze(self):         # Freeze all the weights of the network except for the last fully-connected layer         for param in self.model.parameters():             param.requires_grad = False          # Unfreeze the final layer         for param in self.model.classifier[3].parameters():             param.requires_grad = True      def unfreeze(self):         # Unfreeze all the weights of the network         for param in self.model.parameters():             param.requires_grad = True   # Create an instance of the MobileNetV3 model model = MobileNetV3() print(model) <pre>MobileNetV3(\n  (model): MobileNetV3(\n    (features): Sequential(\n      (0): Conv2dNormActivation(\n        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n      (1): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (2): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n            (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (3): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n            (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): ReLU(inplace=True)\n          )\n          (2): Conv2dNormActivation(\n            (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (4): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (5): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (6): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n            (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (7): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n            (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (8): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n            (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (9): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n            (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (10): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (11): InvertedResidual(\n        (block): Sequential(\n          (0): Conv2dNormActivation(\n            (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (1): Conv2dNormActivation(\n            (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n            (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n            (2): Hardswish()\n          )\n          (2): SqueezeExcitation(\n            (avgpool): AdaptiveAvgPool2d(output_size=1)\n            (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n            (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n            (activation): ReLU()\n            (scale_activation): Hardsigmoid()\n          )\n          (3): Conv2dNormActivation(\n            (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n            (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n          )\n        )\n      )\n      (12): Conv2dNormActivation(\n        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n        (2): Hardswish()\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=1)\n    (classifier): Sequential(\n      (0): Linear(in_features=576, out_features=1024, bias=True)\n      (1): Hardswish()\n      (2): Dropout(p=0.2, inplace=True)\n      (3): Linear(in_features=1024, out_features=10, bias=True)\n    )\n  )\n)\n</pre> <pre>/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/Users/diegofernandezgil/projects/personal-page/.venv/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n</pre> In\u00a0[10]: Copied! <pre>import torch\nimport torch.nn as nn\n\n# Define the loss function and optimizer\n\nloss_fn = nn.CrossEntropyLoss()\n\noptimizer = torch.optim.Adam(params=model.parameters(), lr=0.002)\n</pre> import torch import torch.nn as nn  # Define the loss function and optimizer  loss_fn = nn.CrossEntropyLoss()  optimizer = torch.optim.Adam(params=model.parameters(), lr=0.002) <p>Now let's choose our device automatically (CPU, GPU, or MPS) and write our training loop!</p> <p>The MPS backend is for M1/M2/etc Macs.</p> <p>If you are having any errors running the code locally, you should try to use the <code>cpu</code> mode manually, i.e. <code>device = torch.device(\"cpu\")</code></p> In\u00a0[11]: Copied! <pre># Set the device as GPU, MPS, or CPU according to availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\nelse:\n    device = torch.device(\"cpu\")\n\nprint(f\"Using device: {device}\")\n</pre> # Set the device as GPU, MPS, or CPU according to availability if torch.cuda.is_available():     device = torch.device(\"cuda\") elif torch.backends.mps.is_available():     device = torch.device(\"mps\") else:     device = torch.device(\"cpu\")  print(f\"Using device: {device}\") <pre>Using device: cpu\n</pre> In\u00a0[12]: Copied! <pre># Create a PyTorch training loop\n\nmodel = model.to(device)  # Move the model weights to the device\n\nepochs = 1\nfor epoch in range(epochs):\n    for batch_num, (images, labels) in enumerate(trainloader):\n        # Move tensors to the device\n        images = images.to(device)\n        labels = labels.to(device)\n\n        # Zero out the optimizer's gradient buffer\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = model(images)\n\n        # Calculate the loss and perform backprop\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n\n        # Update the weights\n        optimizer.step()\n\n        # Print the loss for every 100th iteration\n        if (batch_num) % 100 == 0:\n            print(\n                \"Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}\".format(\n                    epoch + 1, epochs, batch_num + 1, len(trainloader), loss.item()\n                )\n            )\n</pre> # Create a PyTorch training loop  model = model.to(device)  # Move the model weights to the device  epochs = 1 for epoch in range(epochs):     for batch_num, (images, labels) in enumerate(trainloader):         # Move tensors to the device         images = images.to(device)         labels = labels.to(device)          # Zero out the optimizer's gradient buffer         optimizer.zero_grad()          # Forward pass         outputs = model(images)          # Calculate the loss and perform backprop         loss = loss_fn(outputs, labels)         loss.backward()          # Update the weights         optimizer.step()          # Print the loss for every 100th iteration         if (batch_num) % 100 == 0:             print(                 \"Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}\".format(                     epoch + 1, epochs, batch_num + 1, len(trainloader), loss.item()                 )             ) <pre>Epoch [1/1], Batch [1/938], Loss: 2.3559\nEpoch [1/1], Batch [101/938], Loss: 0.5907\nEpoch [1/1], Batch [201/938], Loss: 0.3248\nEpoch [1/1], Batch [301/938], Loss: 0.4611\nEpoch [1/1], Batch [401/938], Loss: 0.4321\nEpoch [1/1], Batch [501/938], Loss: 0.4661\nEpoch [1/1], Batch [601/938], Loss: 0.4657\nEpoch [1/1], Batch [701/938], Loss: 0.4037\nEpoch [1/1], Batch [801/938], Loss: 0.3589\nEpoch [1/1], Batch [901/938], Loss: 0.3056\n</pre> In\u00a0[13]: Copied! <pre># Print the loss and accuracy on the test set\ncorrect = 0\ntotal = 0\nloss = 0\n\nfor images, labels in testloader:\n    # Move tensors to the configured device\n    images = images.to(device)\n    labels = labels.to(device)\n\n    # Forward pass\n    outputs = model(images)\n    loss += loss_fn(outputs, labels)\n\n    # torch.max return both max and argmax. We get the argmax here.\n    _, predicted = torch.max(outputs.data, 1)\n\n    # Compute the accuracy\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n\nprint(\n    \"Test Accuracy of the model on the test images: {} %\".format(100 * correct / total)\n)\nprint(\"Test Loss of the model on the test images: {}\".format(loss))\n</pre> # Print the loss and accuracy on the test set correct = 0 total = 0 loss = 0  for images, labels in testloader:     # Move tensors to the configured device     images = images.to(device)     labels = labels.to(device)      # Forward pass     outputs = model(images)     loss += loss_fn(outputs, labels)      # torch.max return both max and argmax. We get the argmax here.     _, predicted = torch.max(outputs.data, 1)      # Compute the accuracy     total += labels.size(0)     correct += (predicted == labels).sum().item()  print(     \"Test Accuracy of the model on the test images: {} %\".format(100 * correct / total) ) print(\"Test Loss of the model on the test images: {}\".format(loss)) <pre>Test Accuracy of the model on the test images: 85.5 %\nTest Loss of the model on the test images: 64.48391723632812\n</pre> In\u00a0[14]: Copied! <pre># Plotting a few examples of correct and incorrect predictions\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Get the first batch of images and labels\nimages, labels = next(iter(testloader))\n\n# Move tensors to the configured device\nimages = images.to(device)\nlabels = labels.to(device)\n\n# Forward pass\noutputs = model(images)\n_, predicted = torch.max(outputs.data, 1)\n\n# Plot the images with labels, at most 10\nfig = plt.figure(figsize=(15, 4))\n\nfor idx in np.arange(min(10, len(images))):\n    ax = fig.add_subplot(2, 10 // 2, idx + 1, xticks=[], yticks=[])\n    ax.imshow(np.squeeze(images.cpu()[idx]))\n    ax.set_title(\n        \"{} ({})\".format(get_class_name(predicted[idx]), get_class_name(labels[idx])),\n        color=(\"green\" if predicted[idx] == labels[idx] else \"red\"),\n    )\n</pre> # Plotting a few examples of correct and incorrect predictions  import matplotlib.pyplot as plt import numpy as np  # Get the first batch of images and labels images, labels = next(iter(testloader))  # Move tensors to the configured device images = images.to(device) labels = labels.to(device)  # Forward pass outputs = model(images) _, predicted = torch.max(outputs.data, 1)  # Plot the images with labels, at most 10 fig = plt.figure(figsize=(15, 4))  for idx in np.arange(min(10, len(images))):     ax = fig.add_subplot(2, 10 // 2, idx + 1, xticks=[], yticks=[])     ax.imshow(np.squeeze(images.cpu()[idx]))     ax.set_title(         \"{} ({})\".format(get_class_name(predicted[idx]), get_class_name(labels[idx])),         color=(\"green\" if predicted[idx] == labels[idx] else \"red\"),     )"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#exercise-transfer-learning-using-mobilenetv3", "title": "Exercise: Transfer learning using MobileNetV3\u00b6", "text": "<p>In the field of machine learning, transfer learning is a powerful technique that leverages pre-trained models and applies them to new tasks. This approach allows us to save time and computational resources by reusing the knowledge gained from training on large datasets.</p> <p>In this exercise we use MobileNetV3, a convolutional neural network architecture for mobile devices, to train a classifier for the Fashion-MNIST dataset using the PyTorch framework.</p> <p>Fashion-MNIST is a drop-in replacement for MNIST (images of size 28x28 with 10 classes) but instead of hand-written digits it contains tiny images of clothes!</p>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#steps", "title": "Steps\u00b6", "text": "<ol> <li>Load the Fashion-MNIST dataset using the torchvision package.</li> <li>Define a PyTorch model using the MobileNetV3 architecture.</li> <li>Train the model on the Fashion-MNIST dataset.</li> <li>Evaluate the model on the test set.</li> </ol>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#step-1-load-the-fashion-mnist-dataset", "title": "Step 1: Load the Fashion-MNIST dataset\u00b6", "text": "<p>The torchvision package provides access to popular datasets, model architectures, and image transformations for computer vision.</p>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#step-2-define-a-pytorch-model-using-the-mobilenetv3-architecture", "title": "Step 2. Define a PyTorch model using the MobileNetV3 architecture.\u00b6", "text": "<p>The <code>torchvision.models.mobilenet_v3_large</code> class provides access to pre-trained MobileNetV3 model. We can use the model and replace the final layer with a fully-connected layer with 10 outputs since we have 10 classes. We can then freeze the weights of the convolutional layers and train only the new fully-connected layer.</p> <p>Let's start with inspecting the original MobileNetV3 (small version) first:</p>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#step-3-train-the-model-on-the-mnist-dataset", "title": "Step 3. Train the model on the MNIST dataset\u00b6", "text": "<p>We can train the model using the standard PyTorch training loop. For the loss function, we'll use CrossEntropyLoss. We also use the Adam optimizer with a learning rate of 0.002. We train the model for 1 epoch so we can see how the model performs after just one pass of the training data.</p>"}, {"location": "generative_ai/tutorials/transfer-learning-mobilenet/#step-4-evaluate-the-model-on-the-test-set", "title": "Step 4. Evaluate the model on the test set\u00b6", "text": "<p>We evaluate the model on the test set by:</p> <ul> <li>printing the accuracy</li> <li>plotting a few examples of correct and incorrect predictions.</li> </ul>"}, {"location": "generative_ai/tutorials/try_promt_techniques/", "title": "Promting techninques review", "text": "In\u00a0[2]: Copied! <pre>import requests\nimport os\n</pre> import requests import os In\u00a0[4]: Copied! <pre>TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\")\n</pre> TOGETHER_API_KEY = os.getenv(\"TOGETHER_API_KEY\") In\u00a0[5]: Copied! <pre>ENDPOINT = 'https://api.together.xyz/inference'\n</pre> ENDPOINT = 'https://api.together.xyz/inference' In\u00a0[6]: Copied! <pre># Decoding parameters\nTEMPERATURE = 0.0\nMAX_TOKENS = 512\nTOP_P = 1.0\nREPITIION_PENALTY = 1.0\n\n# https://huggingface.co/meta-llama/Llama-2-7b-hf\nB_INST, E_INST = \"[INST]\", \"[/INST]\"\nB_SYS, E_SYS = \"&lt;&lt;SYS&gt;&gt;\\n\", \"\\n&lt;&lt;/SYS&gt;&gt;\\n\\n\"\n</pre> # Decoding parameters TEMPERATURE = 0.0 MAX_TOKENS = 512 TOP_P = 1.0 REPITIION_PENALTY = 1.0  # https://huggingface.co/meta-llama/Llama-2-7b-hf B_INST, E_INST = \"[INST]\", \"[/INST]\" B_SYS, E_SYS = \"&lt;&gt;\\n\", \"\\n&lt;&gt;\\n\\n\" In\u00a0[7]: Copied! <pre>def query_together_endpoint(prompt):\n    return requests.post(ENDPOINT, json={\n        \"model\": \"togethercomputer/llama-2-7b-chat\",\n        \"max_tokens\": MAX_TOKENS,\n        \"prompt\": prompt,\n        \"request_type\": \"language-model-inference\",\n        \"temperature\": TEMPERATURE,\n        \"top_p\": TOP_P,\n        \"repetition_penalty\": REPITIION_PENALTY,\n        \"stop\": [\n            E_INST,\n            E_SYS\n        ],\n        \"negative_prompt\": \"\",\n    }, headers={\n        \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",\n    }).json()['output']['choices'][0]['text']\n</pre> def query_together_endpoint(prompt):     return requests.post(ENDPOINT, json={         \"model\": \"togethercomputer/llama-2-7b-chat\",         \"max_tokens\": MAX_TOKENS,         \"prompt\": prompt,         \"request_type\": \"language-model-inference\",         \"temperature\": TEMPERATURE,         \"top_p\": TOP_P,         \"repetition_penalty\": REPITIION_PENALTY,         \"stop\": [             E_INST,             E_SYS         ],         \"negative_prompt\": \"\",     }, headers={         \"Authorization\": f\"Bearer {TOGETHER_API_KEY}\",     }).json()['output']['choices'][0]['text'] In\u00a0[8]: Copied! <pre>def query_model(prompt,  trigger = None, verbose=True, **kwargs):\n    inst_prompt = f\"{B_INST} {prompt} {E_INST}\"\n    if trigger:\n        inst_prompt = inst_prompt + trigger\n    generation = query_together_endpoint(inst_prompt)\n    if verbose:\n        print(f\"*** Prompt ***\\n{inst_prompt}\")\n        print(f\"*** Generation ***\\n{generation}\")\n    return generation\n</pre> def query_model(prompt,  trigger = None, verbose=True, **kwargs):     inst_prompt = f\"{B_INST} {prompt} {E_INST}\"     if trigger:         inst_prompt = inst_prompt + trigger     generation = query_together_endpoint(inst_prompt)     if verbose:         print(f\"*** Prompt ***\\n{inst_prompt}\")         print(f\"*** Generation ***\\n{generation}\")     return generation In\u00a0[9]: Copied! <pre>ANSWER_STAGE = \"Provide the direct answer to the user question.\"\nREASONING_STAGE = \"Describe the step by step reasoning to find the answer.\"\n</pre> ANSWER_STAGE = \"Provide the direct answer to the user question.\" REASONING_STAGE = \"Describe the step by step reasoning to find the answer.\" In\u00a0[10]: Copied! <pre># System prompt can be constructed in two ways:\n# 1) Answering the question first or\n# 2) Providing the reasoning first\n\n# Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n# https://arxiv.org/pdf/2201.11903.pdf\nSYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}Answer the user's question using the following format:\n1) {stage_1}\n2) {stage_2}{e_sys}\"\"\"\n</pre> # System prompt can be constructed in two ways: # 1) Answering the question first or # 2) Providing the reasoning first  # Similar ablation performed in \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\" # https://arxiv.org/pdf/2201.11903.pdf SYSTEM_PROMPT_TEMPLATE = \"\"\"{b_sys}Answer the user's question using the following format: 1) {stage_1} 2) {stage_2}{e_sys}\"\"\" In\u00a0[11]: Copied! <pre># Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\"\n# https://arxiv.org/abs/2205.11916\nCOT_TRIGGER = \"\\n\\nA: Lets think step by step:\"\nA_TRIGGER = \"\\n\\nA:\"\n</pre> # Chain of thought trigger from \"Large Language Models are Zero-Shot Reasoners\" # https://arxiv.org/abs/2205.11916 COT_TRIGGER = \"\\n\\nA: Lets think step by step:\" A_TRIGGER = \"\\n\\nA:\" In\u00a0[12]: Copied! <pre>user_prompt_template = \"Q: Llama 2 has a context window of {atten_window} tokens. \\\nIf we are reserving {max_token} of them for the LLM response, \\\nthe system prompt uses {sys_prompt_len}, \\\nthe chain of thought trigger uses only {trigger_len}, \\\nand finally the conversational history uses {convo_history_len}, \\\nhow many can we use for the user prompt?\"\n</pre> user_prompt_template = \"Q: Llama 2 has a context window of {atten_window} tokens. \\ If we are reserving {max_token} of them for the LLM response, \\ the system prompt uses {sys_prompt_len}, \\ the chain of thought trigger uses only {trigger_len}, \\ and finally the conversational history uses {convo_history_len}, \\ how many can we use for the user prompt?\" In\u00a0[13]: Copied! <pre>atten_window = 4096\nmax_token = 512\nsys_prompt_len = 124\ntrigger_len = 11\nconvo_history_len = 390\n\nuser_prompt = user_prompt_template.format(\n    atten_window=atten_window,\n    max_token=max_token,\n    sys_prompt_len=sys_prompt_len,\n    trigger_len=trigger_len,\n    convo_history_len=convo_history_len\n)\n</pre> atten_window = 4096 max_token = 512 sys_prompt_len = 124 trigger_len = 11 convo_history_len = 390  user_prompt = user_prompt_template.format(     atten_window=atten_window,     max_token=max_token,     sys_prompt_len=sys_prompt_len,     trigger_len=trigger_len,     convo_history_len=convo_history_len ) In\u00a0[14]: Copied! <pre>print(user_prompt)\n</pre> print(user_prompt) <pre>Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt?\n</pre> In\u00a0[15]: Copied! <pre>desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len\ndesired_numeric_answer\n</pre> desired_numeric_answer = atten_window - max_token - sys_prompt_len - trigger_len - convo_history_len desired_numeric_answer Out[15]: <pre>3059</pre> In\u00a0[16]: Copied! <pre>r = query_model(user_prompt)\n</pre> r = query_model(user_prompt) <pre>*** Prompt ***\n[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n*** Generation ***\n  Great, let's do the calculation!\n\nYou've mentioned that Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, that means we have 4096 - 512 = 3584 tokens available for other uses.\n\nYou've also mentioned that the system prompt uses 124 tokens, the chain of thought trigger uses 11 tokens, and the conversational history uses 390 tokens.\n\nSo, the remaining tokens available for the user prompt are:\n\n3584 - 124 - 11 - 390 = 2089 tokens\n\nTherefore, you can use up to 2089 tokens for the user prompt.\n</pre> In\u00a0[17]: Copied! <pre>system_prompt = SYSTEM_PROMPT_TEMPLATE.format(\n    b_sys = B_SYS,\n    stage_1=ANSWER_STAGE,\n    stage_2=REASONING_STAGE,\n    e_sys=E_SYS\n)\nprompt = \"\".join([system_prompt, user_prompt])\n\nr2 = query_model(prompt)\n</pre> system_prompt = SYSTEM_PROMPT_TEMPLATE.format(     b_sys = B_SYS,     stage_1=ANSWER_STAGE,     stage_2=REASONING_STAGE,     e_sys=E_SYS ) prompt = \"\".join([system_prompt, user_prompt])  r2 = query_model(prompt) <pre>*** Prompt ***\n[INST] &lt;&lt;SYS&gt;&gt;\nAnswer the user's question using the following format:\n1) Provide the direct answer to the user question.\n2) Describe the step by step reasoning to find the answer.\n&lt;&lt;/SYS&gt;&gt;\n\nQ: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n*** Generation ***\n  Sure, I'd be happy to help you with that! Here's the answer to your question:\n\n1. Direct answer: The user can use up to 390 tokens for their prompt.\n\nHere's the reasoning behind this answer:\n\n* The context window for Llama 2 is 4096 tokens.\n* You mentioned that 512 tokens are reserved for the LLM response.\n* The system prompt uses 124 tokens.\n* The chain of thought trigger uses only 11 tokens.\n* The conversational history uses 390 tokens.\n\nSo, the remaining tokens available for the user prompt are 4096 - 512 - 124 - 11 = 390 tokens.\n\nI hope this helps! Let me know if you have any further questions.\n</pre> In\u00a0[18]: Copied! <pre>system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS)\nprompt = \"\".join([system_prompt, user_prompt])\n\nr3 = query_model(prompt)\n</pre> system_prompt = SYSTEM_PROMPT_TEMPLATE.format(b_sys = B_SYS, stage_1=REASONING_STAGE, stage_2=ANSWER_STAGE, e_sys=E_SYS) prompt = \"\".join([system_prompt, user_prompt])  r3 = query_model(prompt) <pre>*** Prompt ***\n[INST] &lt;&lt;SYS&gt;&gt;\nAnswer the user's question using the following format:\n1) Describe the step by step reasoning to find the answer.\n2) Provide the direct answer to the user question.\n&lt;&lt;/SYS&gt;&gt;\n\nQ: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n*** Generation ***\n  Great, let's break down the calculation to find out how many tokens are available for the user prompt:\n\n1. First, let's calculate the total number of tokens reserved for the LLM response, chain of thought trigger, and conversational history:\n\nLLM response: 512 tokens\nChain of thought trigger: 11 tokens\nConversational history: 390 tokens\nTotal reserved tokens: 512 + 11 + 390 = 903 tokens\n\n2. Now, let's subtract the total reserved tokens from the context window of 4096 tokens to find out how many tokens are available for the user prompt:\n\n4096 - 903 = 3193 tokens\n\nTherefore, the user can use up to 3193 tokens for their prompt.\n</pre> In\u00a0[23]: Copied! <pre>print(\"Correct answer:\", 4096-512-124-11-390)\n</pre> print(\"Correct answer:\", 4096-512-124-11-390) <pre>Correct answer: 3059\n</pre> In\u00a0[24]: Copied! <pre>r4 = query_model(user_prompt, trigger=COT_TRIGGER)\n</pre> r4 = query_model(user_prompt, trigger=COT_TRIGGER) <pre>*** Prompt ***\n[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n\nA: Lets think step by step:\n*** Generation ***\n\n\n1. The context window of Llama 2 is 4096 tokens.\n2. You want to reserve 512 tokens for the LLM response.\n3. The system prompt uses 124 tokens.\n4. The chain of thought trigger uses only 11 tokens.\n5. The conversational history uses 390 tokens.\n\nNow, let's calculate how many tokens are left for the user prompt:\n\n4096 - 512 = 3584\n\nSo, you have 3584 tokens available for the user prompt.\n</pre> In\u00a0[21]: Copied! <pre>r5 = query_model(user_prompt, trigger=A_TRIGGER)\n</pre> r5 = query_model(user_prompt, trigger=A_TRIGGER) <pre>*** Prompt ***\n[INST] Q: Llama 2 has a context window of 4096 tokens. If we are reserving 512 of them for the LLM response, the system prompt uses 124, the chain of thought trigger uses only 11, and finally the conversational history uses 390, how many can we use for the user prompt? [/INST]\n\nA:\n*** Generation ***\nTo determine how many context tokens are available for the user prompt, we need to subtract the number of tokens reserved for the LLM response, the system prompt, the chain of thought trigger, and the conversational history from the total context window of 4096 tokens.\n\nReserved tokens for LLM response: 512\nReserved tokens for system prompt: 124\nReserved tokens for chain of thought trigger: 11\nReserved tokens for conversational history: 390\n\nTotal reserved tokens: 1037\n\nNow, let's check how many tokens are available for the user prompt:\n\n4096 - 1037 = 3059\n\nSo, there are 3059 context tokens available for the user prompt.\n</pre>"}, {"location": "generative_ai/tutorials/try_promt_techniques/#promting-techninques-review", "title": "Promting techninques review\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#helper-functions", "title": "Helper functions\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#system-prompts", "title": "System Prompts\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#response-triggers", "title": "Response triggers\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-for-our-task", "title": "User prompt for our task\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#testing-the-prompts", "title": "Testing the prompts\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-only", "title": "User prompt only\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-system-prompt-v1-answering-first", "title": "User prompt + system prompt v1: answering first\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-system-prompt-v2-reasoning-first", "title": "User prompt + system prompt v2: reasoning first\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-cot-trigger", "title": "User prompt + cot trigger\u00b6", "text": ""}, {"location": "generative_ai/tutorials/try_promt_techniques/#user-prompt-a-trigger", "title": "User prompt + \"A:\" trigger\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/", "title": "Wikipedia QA Opean AI example", "text": "In\u00a0[34]: Copied! <pre>import openai\nimport os\nimport json\n\nOPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPEN_AI_API_KEY\nclient = openai.OpenAI()\n</pre> import openai import os import json  OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\") openai.api_key = OPEN_AI_API_KEY client = openai.OpenAI() In\u00a0[11]: Copied! <pre># creating a prompt\nquestion_prompt = \"\"\"\nWho is the owner of twitter?\nAnswer: \n\"\"\"\n\n# Use completion endpoint\ncompletion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": question_prompt,\n        },\n    ],\n)\n</pre> # creating a prompt question_prompt = \"\"\" Who is the owner of twitter? Answer:  \"\"\"  # Use completion endpoint completion = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {             \"role\": \"system\",             \"content\": \"You are a helpful assistant\",         },         {             \"role\": \"user\",             \"content\": question_prompt,         },     ], ) In\u00a0[23]: Copied! <pre>json.loads(completion.json())\n</pre> json.loads(completion.json()) Out[23]: <pre>{'id': 'chatcmpl-8uIdWIIINa0IAxgBK3KqxKQ5NXbVV',\n 'choices': [{'finish_reason': 'stop',\n   'index': 0,\n   'logprobs': None,\n   'message': {'content': 'The CEO and co-founder of Twitter is Jack Dorsey.',\n    'role': 'assistant',\n    'function_call': None,\n    'tool_calls': None}}],\n 'created': 1708428698,\n 'model': 'gpt-3.5-turbo-0125',\n 'object': 'chat.completion',\n 'system_fingerprint': 'fp_69829325d0',\n 'usage': {'completion_tokens': 12, 'prompt_tokens': 27, 'total_tokens': 39}}</pre> In\u00a0[25]: Copied! <pre>print(completion.choices[0].message)\n</pre> print(completion.choices[0].message) <pre>ChatCompletionMessage(content='The CEO and co-founder of Twitter is Jack Dorsey.', role='assistant', function_call=None, tool_calls=None)\n</pre> In\u00a0[42]: Copied! <pre>import requests\nimport pandas as pd\nfrom dateutil.parser import parse, ParserError\n</pre> import requests import pandas as pd from dateutil.parser import parse, ParserError <ul> <li>Wikipedia API documentation</li> </ul> In\u00a0[31]: Copied! <pre># Get the Wikipedia page for \"2022\" since OpenAI's models stop in 2021\nparams = {\n    \"action\": \"query\",\n    \"prop\": \"extracts\",\n    \"exlimit\": 1,\n    \"titles\": \"2022\",\n    \"explaintext\": 1,\n    \"formatversion\": 2,\n    \"format\": \"json\",\n}\nresp = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params)\nresponse_dict = resp.json()\nresponse_dict[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")[:10]\n</pre> # Get the Wikipedia page for \"2022\" since OpenAI's models stop in 2021 params = {     \"action\": \"query\",     \"prop\": \"extracts\",     \"exlimit\": 1,     \"titles\": \"2022\",     \"explaintext\": 1,     \"formatversion\": 2,     \"format\": \"json\", } resp = requests.get(\"https://en.wikipedia.org/w/api.php\", params=params) response_dict = resp.json() response_dict[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")[:10] Out[31]: <pre>['2022 (MMXXII) was a common year starting on Saturday of the Gregorian calendar, the 2022nd year of the Common Era (CE) and Anno Domini (AD) designations, the 22nd  year of the 3rd millennium and the 21st century, and the  3rd   year of the 2020s decade.  ',\n 'The year 2022 saw the removal of nearly all COVID-19 restrictions and the reopening of international borders in most countries, and the global rollout of COVID-19 vaccines continued. The global economic recovery from the pandemic continued, though many countries experienced an ongoing inflation surge; in response, many central banks raised their interest rates to landmark levels. The world population reached eight billion people in 2022, though the year also witnessed numerous natural disasters, including two devastating Atlantic hurricanes (Fiona and Ian), and the most powerful volcano eruption of the century so far. The later part of the year also saw the first public release of ChatGPT by OpenAI starting an arms race in artificial intelligence which increased in intensity into 2023, as well as the collapse of the cryptocurrency exchange FTX.',\n '2022 was also dominated by wars and armed conflicts. While escalations into the internal conflict in Myanmar and the Tigray War dominated the heightening of tensions within their regions and each caused over 10,000 deaths, 2022 was most notable for the Russian invasion of Ukraine, the largest armed conflict in Europe since World War II. The invasion caused the displacement of 15.7 million Ukrainians (8 million internally displaced persons and 7.7 million refugees), and led to international condemnations and sanctions and nuclear threats, the withdrawal of hundreds of companies from Russia, and the exclusion of Russia from major sporting events.',\n '',\n '',\n '== Events ==',\n '',\n '',\n '=== January ===',\n ' January 1 \u2013 The Regional Comprehensive Economic Partnership, the largest free trade area in the world, comes into effect for Australia, Brunei, Cambodia, China, Indonesia, Japan, South Korea, Laos, Malaysia, Myanmar, New Zealand, the Philippines, Singapore, Thailand, and Vietnam.']</pre> In\u00a0[32]: Copied! <pre>import pandas as pd\n\n# Load page text into a dataframe\ndf = pd.DataFrame()\ndf[\"text\"] = response_dict[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\")\ndf.head()\n</pre> import pandas as pd  # Load page text into a dataframe df = pd.DataFrame() df[\"text\"] = response_dict[\"query\"][\"pages\"][0][\"extract\"].split(\"\\n\") df.head() Out[32]: text 0 2022 (MMXXII) was a common year starting on Sa... 1 The year 2022 saw the removal of nearly all CO... 2 2022 was also dominated by wars and armed conf... 3 4 In\u00a0[55]: Copied! <pre>def clean_wikipedia_data(df: pd.DataFrame) -&gt; pd.DataFrame:\n    df_cleaned = df.copy()\n    df_cleaned = df = df_cleaned[\n        (df_cleaned[\"text\"].str.len() &gt; 0) &amp; (~df_cleaned[\"text\"].str.startswith(\"==\"))\n    ]\n\n    return df_cleaned\n\n\ndef parse_dates(df_cleaned: pd.DataFrame) -&gt; pd.DataFrame:\n    # In some cases dates are used as headings instead of being part of the\n    # text sample; adjust so dated text samples start with dates\n    prefix = \"\"\n    for i, row in df_cleaned.iterrows():\n        # If the row already has \" - \", it already has the needed date prefix\n        if \" \u2013 \" not in row[\"text\"]:\n            try:\n                # If the row's text is a date, set it as the new prefix\n                parse(row[\"text\"])\n                prefix = row[\"text\"]\n            except ParserError:\n                # If the row's text isn't a date, add the prefix\n                row[\"text\"] = prefix + \" \u2013 \" + row[\"text\"]\n                \n    df_cleaned = df_cleaned[df_cleaned[\"text\"].str.contains(\" \u2013 \")].reset_index(\n        drop=True\n    )\n    \n    return df_cleaned\n\n\ndf_cleaned = clean_wikipedia_data(df)\ndf_cleaned = parse_dates(df_cleaned)\n</pre> def clean_wikipedia_data(df: pd.DataFrame) -&gt; pd.DataFrame:     df_cleaned = df.copy()     df_cleaned = df = df_cleaned[         (df_cleaned[\"text\"].str.len() &gt; 0) &amp; (~df_cleaned[\"text\"].str.startswith(\"==\"))     ]      return df_cleaned   def parse_dates(df_cleaned: pd.DataFrame) -&gt; pd.DataFrame:     # In some cases dates are used as headings instead of being part of the     # text sample; adjust so dated text samples start with dates     prefix = \"\"     for i, row in df_cleaned.iterrows():         # If the row already has \" - \", it already has the needed date prefix         if \" \u2013 \" not in row[\"text\"]:             try:                 # If the row's text is a date, set it as the new prefix                 parse(row[\"text\"])                 prefix = row[\"text\"]             except ParserError:                 # If the row's text isn't a date, add the prefix                 row[\"text\"] = prefix + \" \u2013 \" + row[\"text\"]                      df_cleaned = df_cleaned[df_cleaned[\"text\"].str.contains(\" \u2013 \")].reset_index(         drop=True     )          return df_cleaned   df_cleaned = clean_wikipedia_data(df) df_cleaned = parse_dates(df_cleaned) In\u00a0[57]: Copied! <pre>df_cleaned.tail()\n</pre> df_cleaned.tail() Out[57]: text 177 December 21\u2013December 26 \u2013 A major winter storm... 178 December 24 \u2013 2022 Fijian general election: Th... 179 December 29 \u2013 Brazilian football legend Pel\u00e9 d... 180 December 31 \u2013 Former Pope Benedict XVI dies at... 181 December 7 \u2013 The world population was estimate... In\u00a0[58]: Copied! <pre>df_cleaned.to_csv(\"data/wikipedia_data.csv\")\n</pre> df_cleaned.to_csv(\"data/wikipedia_data.csv\") <p>To create our chatbot, we'll need to convert our natural language data into numeric representations that our machine learning model can process. We need these representations to capture the relationships within the data so that the model can recognize patterns and identify the most relevant content.</p> In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport openai\nimport numpy as np\n\n\nOPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nopenai.api_key = OPEN_AI_API_KEY\nclient = openai.OpenAI()\n\ndf = pd.read_csv(\"data/wikipedia_data.csv\", index_col=0)\ndf.sample(5)\n</pre> import os import pandas as pd import openai import numpy as np   OPEN_AI_API_KEY = os.getenv(\"OPENAI_API_KEY\") openai.api_key = OPEN_AI_API_KEY client = openai.OpenAI()  df = pd.read_csv(\"data/wikipedia_data.csv\", index_col=0) df.sample(5) Out[1]: text 35 March 2 \u2013 Russian invasion of Ukraine: The Uni... 138 September 26 \u2013 The Nord Stream pipeline sabota... 114 August 6 \u2013 Terrance Drew is sworn in as prime ... 86 May 28 \u2013 Spanish club Real Madrid beat English... 98 July 6 \u2013 July 31 \u2013 UEFA Women's Euro 2022 is h... In\u00a0[4]: Copied! <pre>EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"\n\ndef get_embedding(text, model=\"text-embedding-3-small\"):\n    text = text.replace(\"\\n\", \" \")\n    return client.embeddings.create(input=[text], model=model).data[0].embedding\n\n\n# df[\"embeddings\"] = df.text.apply(\n#     lambda x: get_embedding(x, model=EMBEDDING_MODEL_NAME)\n# )\n# df.to_csv(\"data/wikipedia_embeddings.csv\")\n</pre> EMBEDDING_MODEL_NAME = \"text-embedding-ada-002\"  def get_embedding(text, model=\"text-embedding-3-small\"):     text = text.replace(\"\\n\", \" \")     return client.embeddings.create(input=[text], model=model).data[0].embedding   # df[\"embeddings\"] = df.text.apply( #     lambda x: get_embedding(x, model=EMBEDDING_MODEL_NAME) # ) # df.to_csv(\"data/wikipedia_embeddings.csv\") In\u00a0[5]: Copied! <pre>df = pd.read_csv(\"data/wikipedia_embeddings.csv\", index_col=0)\ndf[\"embeddings\"] = df.embeddings.apply(eval).apply(np.array)\ndf.sample(5)\n</pre> df = pd.read_csv(\"data/wikipedia_embeddings.csv\", index_col=0) df[\"embeddings\"] = df.embeddings.apply(eval).apply(np.array) df.sample(5) Out[5]: text embeddings 161 November 11 \u2013 Russian invasion of Ukraine: Ukr... [-0.01199280098080635, -0.013951582834124565, ... 42 March 7 \u2013 COVID-19 pandemic: The global death ... [0.005762410815805197, -0.006090453360229731, ... 148 October 25 \u2013 Amid a government crisis, Rishi S... [0.0018328798469156027, -0.02193058282136917, ... 45 March 9 \u2013 Russian invasion of Ukraine: Russia ... [0.0011836738558486104, -0.010346542112529278,... 8 January 7 \u2013 COVID-19 pandemic: The number of C... [0.006454604212194681, -0.0033079846762120724,... In\u00a0[6]: Copied! <pre>question = \"Who is the owner of twitter?\"\nquestionn_embeddings = get_embedding(question, model=EMBEDDING_MODEL_NAME)\n</pre> question = \"Who is the owner of twitter?\" questionn_embeddings = get_embedding(question, model=EMBEDDING_MODEL_NAME) In\u00a0[7]: Copied! <pre>len(questionn_embeddings)\n</pre> len(questionn_embeddings) Out[7]: <pre>1536</pre> In\u00a0[5]: Copied! <pre>from scipy import spatial\n\n\ndef distances_from_embeddings(\n    query_embedding: list[float],\n    embeddings: list[list[float]],\n    distance_metric=\"cosine\",\n) -&gt; list[list]:\n\"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"\n    distance_metrics = {\n        \"cosine\": spatial.distance.cosine,\n        \"L1\": spatial.distance.cityblock,\n        \"L2\": spatial.distance.euclidean,\n        \"Linf\": spatial.distance.chebyshev,\n    }\n    distances = [\n        distance_metrics[distance_metric](query_embedding, embedding)\n        for embedding in embeddings\n    ]\n    return distances\n</pre> from scipy import spatial   def distances_from_embeddings(     query_embedding: list[float],     embeddings: list[list[float]],     distance_metric=\"cosine\", ) -&gt; list[list]:     \"\"\"Return the distances between a query embedding and a list of embeddings.\"\"\"     distance_metrics = {         \"cosine\": spatial.distance.cosine,         \"L1\": spatial.distance.cityblock,         \"L2\": spatial.distance.euclidean,         \"Linf\": spatial.distance.chebyshev,     }     distances = [         distance_metrics[distance_metric](query_embedding, embedding)         for embedding in embeddings     ]     return distances In\u00a0[10]: Copied! <pre>df = df.assign(\n    distances=distances_from_embeddings(\n        questionn_embeddings, df[\"embeddings\"], distance_metric=\"cosine\"\n    )\n)\n</pre> df = df.assign(     distances=distances_from_embeddings(         questionn_embeddings, df[\"embeddings\"], distance_metric=\"cosine\"     ) ) In\u00a0[15]: Copied! <pre>df.sort_values(by='distances').head(3)['text'].tolist()\n</pre> df.sort_values(by='distances').head(3)['text'].tolist() Out[15]: <pre>['October 28 \u2013 Elon Musk completes his $44 billion acquisition of Twitter.',\n 'April 25 \u2013 Elon Musk reaches an agreement to acquire the social media network Twitter (which he later rebrands as X) for $44 billion USD, which later closes in October.',\n 'January 24 \u2013 The federal government under Scott Morrison announces that, after more than three years of confidential negotiations, copyright ownership of the Australian Aboriginal Flag has been transferred to the Commonwealth.']</pre> In\u00a0[17]: Copied! <pre>df.to_csv(\"data/wikipedia_embeddings_distance.csv\")\n</pre> df.to_csv(\"data/wikipedia_embeddings_distance.csv\") <p>So far we have prepared our dataset, created embeddings, and used unsupervised machine learning to help our model understand the relationships within the data.</p> <p>Now we're getting to the magic! Our next task is to write a custom prompt that will include the most relevant parts of our dataset. We want our prompt to look something like this:</p> <p>Great question! Our data is sorted from most to least relevant -- but how many of those rows can we include?</p> <p>While we could choose arbitrary number, e.g. the top 5 or top 50 most relevant rows, a better approach is to count the number of tokens we use as we compose our text prompt and use all of the available tokens for each prompt.</p> <p>Review: A token is the basic unit of text processing in a NLP model. It represents a sequence of characters that the model uses to understand and generate language.</p> <p>Model usage on OpenAI is priced by the token, and each model supports a limited number of tokens. You can view this limit under the \"max request\" column on the OpenAI documentation about any given model.</p> <p>In this course, the demo videos use the <code>pt-3.5-turbo-instruct</code>, which has a limit of about 4,096 tokens. That limit includes both the custom prompt and the response generated by the model.</p> In\u00a0[1]: Copied! <pre>import os\nimport pandas as pd\nimport openai\nimport numpy as np\nimport tiktoken\n</pre> import os import pandas as pd import openai import numpy as np import tiktoken In\u00a0[13]: Copied! <pre>df = pd.read_csv(\"data/wikipedia_embeddings_distance.csv\", index_col =0).sort_values(\"distances\")\n</pre> df = pd.read_csv(\"data/wikipedia_embeddings_distance.csv\", index_col =0).sort_values(\"distances\") In\u00a0[14]: Copied! <pre>tokenizer = tiktoken.get_encoding(\"cl100k_base\")\nlen(tokenizer.encode(\"Answer the question based on the context\"))\n</pre> tokenizer = tiktoken.get_encoding(\"cl100k_base\") len(tokenizer.encode(\"Answer the question based on the context\")) Out[14]: <pre>7</pre> In\u00a0[15]: Copied! <pre>def get_number_of_tokens(text: str, tokenizer) -&gt; pd.DataFrame:\n    return len(tokenizer.encode(text))\n\ndf = df.assign(\n    length_token = df['text'].apply(lambda x: get_number_of_tokens(x, tokenizer))\n)\n</pre> def get_number_of_tokens(text: str, tokenizer) -&gt; pd.DataFrame:     return len(tokenizer.encode(text))  df = df.assign(     length_token = df['text'].apply(lambda x: get_number_of_tokens(x, tokenizer)) ) In\u00a0[32]: Copied! <pre>def create_prompt(question, df, tokenizer, max_token_count):\n\"\"\"\n    Given a question and a dataframe containing rows of text and their\n    embeddings, return a text prompt to send to a Completion model\n    \"\"\"\n\n    # Count the number of tokens in the prompt template and question\n    prompt_template = \"\"\"\n    Answer the question based on the context below, and if the question\n    can't be answered based on the context, say \"I don't know\"\n\n    Context: \n\n{}\n\n    ---\n\n    Question: {}\n    Answer:\"\"\"\n\n    current_token_count = len(tokenizer.encode(prompt_template)) + len(\n        tokenizer.encode(question)\n    )\n\n    context = []\n    for text in df.sort_values(\"distances\")[\"text\"].values:\n        # Increase the counter based on the number of tokens in this row\n        text_token_count = len(tokenizer.encode(text))\n        current_token_count += text_token_count\n\n        # Add the row of text to the list if we haven't exceeded the max\n        if current_token_count &lt;= max_token_count:\n            context.append(text)\n        else:\n            break\n\n    return prompt_template.format(\"n\\n###\\n\\n\".join(context), question)\n</pre> def create_prompt(question, df, tokenizer, max_token_count):     \"\"\"     Given a question and a dataframe containing rows of text and their     embeddings, return a text prompt to send to a Completion model     \"\"\"      # Count the number of tokens in the prompt template and question     prompt_template = \"\"\"     Answer the question based on the context below, and if the question     can't be answered based on the context, say \"I don't know\"      Context:       {}      ---      Question: {}     Answer:\"\"\"      current_token_count = len(tokenizer.encode(prompt_template)) + len(         tokenizer.encode(question)     )      context = []     for text in df.sort_values(\"distances\")[\"text\"].values:         # Increase the counter based on the number of tokens in this row         text_token_count = len(tokenizer.encode(text))         current_token_count += text_token_count          # Add the row of text to the list if we haven't exceeded the max         if current_token_count &lt;= max_token_count:             context.append(text)         else:             break      return prompt_template.format(\"n\\n###\\n\\n\".join(context), question) In\u00a0[33]: Copied! <pre>question = \"Who is the owner of twitter?\"\nmax_token_count = 200\n\nprint(create_prompt(question, df, tokenizer, max_token_count))\n</pre> question = \"Who is the owner of twitter?\" max_token_count = 200  print(create_prompt(question, df, tokenizer, max_token_count)) <pre>\n    Answer the question based on the context below, and if the question\n    can't be answered based on the context, say \"I don't know\"\n\n    Context: \n\n    October 28 \u2013 Elon Musk completes his $44 billion acquisition of Twitter.n\n###\n\nApril 25 \u2013 Elon Musk reaches an agreement to acquire the social media network Twitter (which he later rebrands as X) for $44 billion USD, which later closes in October.n\n###\n\nJanuary 24 \u2013 The federal government under Scott Morrison announces that, after more than three years of confidential negotiations, copyright ownership of the Australian Aboriginal Flag has been transferred to the Commonwealth.n\n###\n\nOctober 25 \u2013 Amid a government crisis, Rishi Sunak becomes Prime Minister of the United Kingdom, following the resignation of Liz Truss the previous week resulting in a 50-day tenure.\n\n    ---\n\n    Question: Who is the owner of twitter?\n    Answer:\n</pre> In\u00a0[35]: Copied! <pre>completion = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"You are a helpful assistant\",\n        },\n        {\n            \"role\": \"user\",\n            \"content\": create_prompt(question, df, tokenizer, max_token_count),\n        },\n    ],\n)\n</pre> completion = client.chat.completions.create(     model=\"gpt-3.5-turbo\",     messages=[         {             \"role\": \"system\",             \"content\": \"You are a helpful assistant\",         },         {             \"role\": \"user\",             \"content\": create_prompt(question, df, tokenizer, max_token_count),         },     ], ) In\u00a0[40]: Copied! <pre>completion.choices[0].message\n</pre> completion.choices[0].message Out[40]: <pre>ChatCompletionMessage(content='Elon Musk is the owner of Twitter.', role='assistant', function_call=None, tool_calls=None)</pre>"}, {"location": "generative_ai/tutorials/wikipedia_qa/#wikipedia-qa-opean-ai-example", "title": "Wikipedia QA Opean AI example\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#openai-model-responses-without-customization", "title": "OpenAI Model Responses without Customization\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#extracting-the-model-response", "title": "Extracting the Model Response\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#extracting-response-text", "title": "Extracting Response Text\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#get-external-data", "title": "Get external data\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#clean-the-data", "title": "Clean the data\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#convert-into-embeddings", "title": "Convert into embeddings\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#finding-relevant-data", "title": "Finding relevant data\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#providing-context-in-a-custom-prompt", "title": "Providing context in a custom prompt\u00b6", "text": ""}, {"location": "generative_ai/tutorials/wikipedia_qa/#how-much-data-should-we-include", "title": "How much data should we include?\u00b6", "text": ""}, {"location": "mlops/continous_delivery/", "title": "Continous integration", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#continuous-integration", "title": "Continuous Integration", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#introduction", "title": "Introduction", "text": "<p>What is continuous integration? It is the ability to know whether your code  works.</p> <p>Continuous Integration (CI) is a development practice where developers integrate code into a shared repository frequently, preferably several times a day.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#benefits", "title": "Benefits", "text": "<p>CI provides numerous benefits, including early identification of integration issues, faster software release cycles, and improved code quality.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#principles", "title": "Principles", "text": "<p>The main principles of CI include:</p> <ol> <li>Maintain a Single Source Repository: All code and resources are stored    in a version-controlled source repository.</li> <li>Automate the Build: Building, testing, and packaging processes are    automated.</li> <li>Make Your Build Self-Testing: Every commit triggers a build and test    process.</li> <li>Every Commit Should Build the Main Branch: Developers integrate their    changes with the main branch regularly.</li> <li>Keep the Build Fast: The build process is designed to be fast to provide    quick feedback.</li> <li>Test in a Clone of the Production Environment: Use a copy of the    production environment for testing.</li> <li>Make it Easy to Get the Latest Deliverables: Builds are available for    testing as soon as they pass.</li> <li>Everyone can See What's Happening: Transparency on the build progress    and results is crucial.</li> <li>Automate Deployment: Deployment to production or staging environments    is automated.</li> </ol>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#tools", "title": "Tools", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#containers", "title": "Containers", "text": "<ul> <li>Docker</li> </ul>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#test", "title": "Test", "text": "<ul> <li>pytest</li> </ul>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/continous_delivery/#other-tools", "title": "Other tools", "text": "<ul> <li>Organize commands:<ul> <li>Makefile</li> </ul> </li> </ul> <p>Popular CI tools include Jenkins, Travis CI, CircleCI, and</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/mlops_intro/", "title": "Introduction", "text": "", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#intro-to-mlops", "title": "Intro to MLops", "text": "<p>MLOps, or Machine Learning Operations, is a discipline that merges the # MLOps:  Unifying Machine Learning System Development and Operation</p> <p>MLOps, or Machine Learning Operations, is a discipline that merges the  development (Dev) and operation (Ops) of machine learning (ML) systems. As   data science and ML continue to become key capabilities for solving complex    real-world problems, the practice of MLOps is gaining traction. MLOps aims     to promote automation and monitoring throughout the construction of ML      systems, including integration, testing, release, deployment, and       infrastructure management.</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-is-more-than-just-ml-code", "title": "MLOps is More than Just ML Code", "text": "<p>The complexity of real-world ML systems goes beyond the ML code. The required  elements surrounding the ML code comprise a vast and intricate system that   includes configuration, automation, data collection, data verification,    testing and debugging, resource management, model analysis, process and     metadata management, serving infrastructure, and monitoring. In other words,     only a small fraction of an ML system is composed of the ML code      itself.</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-a-marriage-of-devops-and-machine-learning", "title": "MLOps: A Marriage of DevOps and Machine Learning", "text": "<p>Just as DevOps principles have proved beneficial in the development and  operation of large-scale software systems, these principles are applicable to   ML systems as well. However, ML systems have distinct characteristics:</p> <ol> <li>Team skills: An ML project typically involves data scientists or ML     researchers who might not be experienced software engineers capable of      building production-class services.</li> <li>Development: ML is experimental in nature. Challenges arise in tracking     what worked and what didn't, and maintaining reproducibility while      maximizing code reusability.</li> <li>Testing: Testing an ML system is more complex than testing other    software systems. It requires data validation, trained model quality     evaluation, and model validation.</li> <li>Deployment: Deployment in ML systems often entails deploying a     multi-step pipeline to automatically retrain and deploy models.</li> <li>Production: ML models can experience performance degradation due to     constantly evolving data profiles, necessitating the tracking of summary      statistics of data and monitoring of the online performance of models.</li> </ol>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#developing-ml-models-steps-to-success", "title": "Developing ML Models: Steps to Success", "text": "<p>Although the precise steps may vary depending on the specific ML project, a  general process can be outlined for developing ML models:</p> <ol> <li>Understanding the problem: This involves defining the business problem,     identifying the ML task, and preparing the initial data.</li> <li>Data preparation: This step includes gathering, cleaning, and     transforming data for the ML model.</li> <li>Model building: This includes selecting the model, training it, and then     evaluating its performance.</li> <li>Model deployment: This involves deploying the model into a production environment.</li> <li>Monitoring and maintenance: This step involves monitoring the     performance of the model over time, retraining it as necessary, and      performing model updates.</li> </ol>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-hierarchy-of-needs", "title": "MLops Hierarchy of needs", "text": "<p>To reach the Mlops level you have to achieve a few steps before you can reach  the next level. You cannot, for example, have DataOps without first   implementing devops.</p> <p>You need to achieve one step of the bottom</p>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#mlops-maturity-levels", "title": "MLOps Maturity Levels", "text": "<p>there are several different phases of going from a crude working where you can   barely get things into production and things are error-prone and manual all    the way to a very sophisticated system that has really end-to-end automation     and uses the next-generation features.</p> <ol> <li>Initial step: Experimentation. Establish the experimentation environment.</li> <li>Repeatable: standardize your code, your repos, make sure that there's maybe     a platform that you're using that can actually deploy the solution.</li> <li>Reliable: Test, monitoring, data drift, model versions.</li> <li>Scalable: You're able to templatize and productionize a lot of different ML     solutions, not just one, but actually have a scalable system that you can      repeat over and over again</li> </ol> Level Description Highlights Technology 0 No MLOps <ul><li>Difficult to manage full machine learning model lifecycle</li><li>The teams are disparate and releases are painful</li><li>Most systems exist as 'black boxes,' little feedback during/post deployment</li><li>Manual builds and deployments</li><li>Manual testing of model and application</li><li>No centralized tracking of model performance</li><li>Training of model is manual</li></ul> 1 DevOps but no MLOps <ul><li>Releases are less painful than No MLOps, but rely on Data Team for every new model</li><li>Still limited feedback on how well a model performs in production</li><li>Difficult to trace/reproduce results</li></ul> <ul><li>Automated builds</li><li>Automated tests for application code</li></ul> 2 Automated Training <ul><li>Training environment is fully managed and traceable</li><li>Easy to reproduce model</li><li>Releases are manual, but low friction</li></ul> <ul><li>Automated model training</li><li>Centralized tracking of model training performance</li><li>Model management</li></ul> 3 Automated Model Deployment <ul><li>Releases are low friction and automatic</li><li>Full traceability from deployment back to original data</li><li>Entire environment managed: train &gt; test &gt; production</li></ul> <ul><li>Integrated A/B testing of model performance for deployment</li><li>Automated tests for all code</li><li>Centralized tracking of model training performance</li></ul> 4 Full MLOps Automated Operations <ul><li>Full system automated and easily monitored</li><li>Production systems are providing information on how to improve and, in some cases, automatically improve with new models</li><li>Approaching a zero-downtime system</li></ul> <ul><li>Automated model training and testing</li><li>Verbose, centralized metrics from deployed model</li></ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-0-no-mlops", "title": "Level 0: No MLOps", "text": "<ul> <li> <p>People: Data scientists, data engineers, and software engineers are    siloed and not in regular communications.</p> </li> <li> <p>Model Creation: Data is gathered manually, compute is likely not managed,    experiments aren't predictably tracked, and the end result may be a single     model file manually handed off.</p> </li> <li>Model Release: The release process is manual, and the scoring script may    be manually created well after experiments without version control.</li> <li>Application Integration: Heavily reliant on data scientist expertise to    implement and manual releases each time.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-1-devops-no-mlops", "title": "Level 1: DevOps no MLOps", "text": "<ul> <li> <p>People: Same as Level 0.</p> </li> <li> <p>Model Creation: Data pipeline gathers data automatically, but compute may    not be managed, and experiments aren't predictably tracked.</p> </li> <li>Model Release: Still a manual process, but the scoring script is likely    version controlled and is handed off to software engineers.</li> <li>Application Integration: Basic integration tests exist, but still heavily    reliant on data scientist expertise. However, releases are automated and     application code has unit tests.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-2-automated-training", "title": "Level 2: Automated Training", "text": "<ul> <li> <p>People: Data scientists work directly with data engineers to convert    experimentation code into repeatable scripts/jobs, while software engineers     remain siloed.</p> </li> <li> <p>Model Creation: Data pipeline gathers data automatically, compute is    managed, experiment results are tracked, and both training code and  resulting models are version controlled.</p> </li> <li>Model Release: Manual release, but the scoring script is version    controlled with tests and the release is managed by the software engineering     team.</li> <li>Application Integration: Same as Level 1.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-3-automated-model-deployment", "title": "Level 3: Automated Model Deployment", "text": "<ul> <li> <p>People: Data scientists and data engineers work together and also with    software engineers to manage inputs/outputs and automate model integration     into application code.</p> </li> <li> <p>Model Creation: Same as Level 2.</p> </li> <li>Model Release: Release is automatic and managed by a continuous delivery    (CI/CD) pipeline.</li> <li>Application Integration: Unit and integration tests exist for each model    release, and the process is less reliant on data scientist expertise.     Application code has unit/integration tests..</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#level-4-full-mlops-automated-retraining", "title": "Level 4: Full MLOps Automated Retraining", "text": "<ul> <li> <p>People: All roles work together, with software engineers implementing    post-deployment metrics gathering.</p> </li> <li> <p>Model Creation: Similar to Level 3, but retraining is triggered    automatically based on production metrics.</p> </li> <li>Model Release: Same as Level 3.</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/mlops_intro/#references", "title": "References", "text": "<ul> <li>Google Mlops levels</li> <li>Microsoft Mlops levels</li> </ul>", "tags": ["Mlops", "Introduction"]}, {"location": "mlops/tools/docker_file/", "title": "Docker", "text": ""}, {"location": "mlops/tools/docker_file/#dockerizing-a-python-application", "title": "Dockerizing a Python Application", "text": "<p>This guide will walk you through the process of dockerizing a simple Python  application.</p>"}, {"location": "mlops/tools/docker_file/#prerequisites", "title": "Prerequisites", "text": "<ul> <li>Docker installed on your machine.</li> <li>Basic knowledge of Python.</li> </ul>"}, {"location": "mlops/tools/docker_file/#step-1-create-a-python-application", "title": "Step 1: Create a Python Application", "text": "<p>First, let's create a simple Python application that we want to dockerize.  Let's call it <code>app.py</code>.</p> <pre><code>def main():\nprint('Hello, Docker')\nif __name__ == '__main__':\nmain()\n</code></pre> <p>This application simply prints out 'Hello, Docker!' when run.</p>"}, {"location": "mlops/tools/docker_file/#step-2-create-a-dockerfile", "title": "Step 2: Create a Dockerfile", "text": "<p>A Dockerfile is a script that contains collections of commands and instructions  to create a Docker image.</p> <p>In the same directory as your <code>app.py</code>, create a file named <code>Dockerfile</code> with the following content:</p> <pre><code># Use an official Python runtime as a parent image\nFROM python:3.9\n# Set the working directory in the container to /app\nWORKDIR /app\n# Add the current directory contents into the container at /app\nADD . /app\n\n# Run app.py when the container launches\nCMD ['python', 'app.py']\n</code></pre> <p>This Dockerfile starts with a Python 3.9 base image, sets the working directory  to /app, copies the current directory into the container, and finally runs the   <code>app.py</code> script.</p>"}, {"location": "mlops/tools/docker_file/#step-3-build-the-docker-image", "title": "Step 3: Build the Docker Image", "text": "<p>Now, you can build the Docker image from the Dockerfile. Run the following  command in the same directory as your Dockerfile:</p> <pre><code>docker build -t python-docker-demo .\n</code></pre> <p>This tells Docker to build an image from the Dockerfile and tag it with the  name <code>python-docker-demo</code>.</p>"}, {"location": "mlops/tools/docker_file/#step-4-run-the-docker-container", "title": "Step 4: Run the Docker Container", "text": "<p>After the Docker image is built, you can run the Docker container with the  following command:</p> <pre><code>docker run python-docker-demo\n</code></pre> <p>You should see 'Hello, Docker!' printed to your console.</p>"}, {"location": "mlops/tools/docker_file/#conclusion", "title": "Conclusion", "text": "<p>Congratulations! You have just dockerized a Python application. Docker allows  you to package your applications with all of their dependencies into a   standardized unit for software development, making your applications more    reliable and easier to share and deploy.</p>"}, {"location": "mlops/tools/github_actions/", "title": "Github actions", "text": "", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#github-actions", "title": "GitHub Actions", "text": "<p>In this guide, we'll go through the steps to create a continuous integration  (CI) workflow for a Python package using GitHub Actions. This will   automatically test your Python package each time you push a commit to your repository.</p> <pre><code>.\n\u251c\u2500\u2500 .github\n\u2502   \u251c\u2500\u2500 workflows\n\u2502   \u2502   \u2514\u2500\u2500 workflow1.yml\n\u251c\u2500\u2500 Makefile\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 src\n</code></pre>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-1-create-a-workflow-file", "title": "Step 1: Create a Workflow File", "text": "<p>In your GitHub repository, create a new file in the <code>.github/workflows</code>  directory. You can name it anything you like, but it must end in <code>.yml</code>  or <code>.yaml</code>. For this example, let's name it <code>python-package.yml</code>.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-2-set-up-the-workflow", "title": "Step 2: Set up the Workflow", "text": "<p>Open <code>python-package.yml</code> and let's set up the workflow. Here's a simple  starting point:</p> <pre><code>name: Python package\non:\npush:\nbranches: [ master ]\npull_request:\nbranches: [ master ]\njobs:\nbuild:\nruns-on: ubuntu-latest\nstrategy:\nmatrix:\npython-version: [3.6, 3.7, 3.8, 3.9, 3.10]\n</code></pre> <p>This configures the workflow to run on <code>push</code> and <code>pull_request</code> events to the  <code>master</code> branch. It runs on the <code>ubuntu-latest</code> GitHub-hosted runner and tests   against Python 3.6, 3.7, 3.8, 3.9, and 3.10.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-3-configure-the-workflow-steps", "title": "Step 3: Configure the Workflow Steps", "text": "<p>We will now add the steps that the workflow will follow. Below the  <code>python-version</code> line, add:</p> <pre><code>    steps:\n- uses: actions/checkout@v2\n- name: Set up Python ${{ matrix.python-version }}\nuses: actions/setup-python@v2\nwith:\npython-version: ${{ matrix.python-version }}\n- name: Install dependencies\nrun: |\npython -m pip install --upgrade pip\npip install flake8 pytest\nif [ -f requirements.txt ]; then pip install -r requirements.txt; fi\n- name: Lint with flake8\nrun: |\n# stop the build if there are Python syntax errors or undefined names\nflake8 . --count --select=E9,F63,F7,F82 --show-source --statistics\n# exit-zero treats all errors as warnings. The GitHub editor is 127\nchars wide\nflake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics\n- name: Test with pytest\nrun: |\npytest\n</code></pre> <p>This checks out your repository, sets up Python, installs dependencies, and  then runs <code>flake8</code> for linting and <code>pytest</code> for running tests.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/github_actions/#step-4-commit-and-push", "title": "Step 4: Commit and Push", "text": "<p>Once you're done, commit the <code>python-package.yml</code> file and push it to your  GitHub repository. GitHub Actions will start running the workflow on your next   push.</p> <p>Remember, you can always modify and expand this workflow to suit your specific  needs. For more information, check out the GitHub Actions documentation.</p>", "tags": ["Mlops", "Continuous Integration"]}, {"location": "mlops/tools/makefile/", "title": "Makefile", "text": ""}, {"location": "mlops/tools/makefile/#using-makefile-and-python-for-streamlined-development", "title": "Using Makefile and Python for Streamlined Development", "text": "<p>Makefiles are powerful tools for automating build processes and managing  dependencies in software development projects. When combined with Python, they   become even more versatile and efficient. In this article, we will explore    the benefits of using Makefiles and Python together and demonstrate how they     can streamline your development workflow.</p>"}, {"location": "mlops/tools/makefile/#what-is-a-makefile", "title": "What is a Makefile", "text": "<p>A Makefile is a simple text file that contains a set of rules and dependencies.  It is commonly used in Unix-based systems to automate the compilation and   execution of programs. Make, the build automation tool, reads the Makefile    and executes the necessary commands to build the target or targets specified.</p> <p>Makefiles are especially useful when working on projects with multiple source  files, libraries, or complex build processes. They provide a clear and concise   way to define the dependencies and actions needed to build and run your    project.</p>"}, {"location": "mlops/tools/makefile/#integrating-python-with-makefiles", "title": "Integrating Python with Makefiles", "text": "<p>Python is a popular programming language known for its simplicity and  readability. It offers a wide range of libraries and frameworks that   facilitate various tasks, from data analysis to web development. By combining    Python with Makefiles, you can leverage the strengths of both tools and     create a more efficient development workflow.</p> <p>One of the primary advantages of using Python in a Makefile is its ability to  automate repetitive tasks. Instead of manually executing commands or scripts,   you can define them as rules in the Makefile and let Make handle the    execution. This approach saves time and reduces the chance of errors.</p> <p>Let's look at an example to illustrate how Python and Makefiles work together . Suppose you are working on a Python project that requires installing  dependencies, running tests, and generating documentation. You can define the   following rules in your Makefile:</p> <pre><code>install:\npip install -r requirements.txt\n\ntest:\npytest tests/\n\ndocs:\npython generate_docs.py\n</code></pre> <p>In this example, the <code>install</code> rule installs the project dependencies by  running <code>pip install -r requirements.txt</code>. The <code>test</code> rule executes the test   suite using pytest, and the <code>docs</code> rule generates the project documentation    using a custom Python script.</p> <p>To run a specific rule, you simply type <code>make &lt;rule_name&gt;</code> in your terminal.  For example, <code>make test</code> will execute the tests defined in the <code>tests/</code> directory.</p>"}, {"location": "mlops/tools/makefile/#handling-dependencies-with-makefiles", "title": "Handling Dependencies with Makefiles", "text": "<p>One of the key features of Makefiles is their ability to handle dependencies  automatically. When a rule depends on certain files or other rules, Make   ensures that the dependencies are up to date before executing the rule. This    feature is invaluable in large projects with complex dependencies.</p> <p>Let's consider a scenario where your Python project depends on multiple source  files, and each file needs to be compiled before the final executable is   generated. You can define the following rule in your Makefile:</p> <pre><code>main: utils.o file1.o file2.o\ngcc -o main main.c utils.o file1.o file2.o\n\n%.o: %.c\ngcc -c $&lt;\n</code></pre> <p>In this example, the <code>main</code> rule depends on three object files: <code>utils.o</code>,  <code>file1.o</code>, and <code>file2.o</code>. The rule specifies the compilation command to   generate the <code>main</code> executable. The second rule, <code>%.o: %.c</code>, is a pattern    rule that compiles any <code>.c</code> file into an object file. The <code>$&lt;</code> placeholder     represents the source file name.</p> <p>When you execute <code>make main</code>, Make automatically checks if any of the object  files are missing or have been modified since the last build. If necessary, it   compiles the required source files and then proceeds to link them into the    <code>main</code> executable.</p> <p>By defining the dependencies accurately in your Makefile, you ensure that your  project is built correctly and efficiently, with only the necessary steps  being executed.</p>"}, {"location": "mlops/tools/makefile/#using-the-phony-target", "title": "Using the <code>.PHONY</code> Target", "text": "<p>In a Makefile, the <code>.PHONY</code> target is used to declare rules that do not  correspond to actual files. These rules are considered 'phony' because they   don't generate any output files with the same name. Instead, they execute a    series of commands or actions.</p> <p>The <code>.PHONY</code> target is typically used for defining rules that perform common  tasks such as cleaning the project directory, running tests, or building the   project. By declaring these rules as phony, you ensure that Make doesn't    confuse them with actual files and always executes the associated commands.</p> <p>For example, consider the following Makefile snippet:</p> <pre><code>.PHONY: clean test build\nclean:\nrm -rf build/\n\ntest:\npytest tests/\n\nbuild:\npython setup.py build\n</code></pre> <p>In this example, the <code>.PHONY</code> target is used to declare the rules <code>clean</code>,  <code>test</code>, and <code>build</code> as phony targets. When you run <code>make clean</code>, it executes   the command <code>rm -rf build/</code> to remove the build directory. Similarly,   <code>make test</code> runs the test suite using pytest, and <code>make build</code> builds the    project using the <code>setup.py</code> script.</p> <p>By using the <code>.PHONY</code> target, you ensure that Make always executes the  specified commands for these rules, regardless of whether there are files with   the same names.</p>"}, {"location": "mlops/tools/makefile/#including-environment-variables-in-makefile", "title": "Including Environment Variables in Makefile", "text": "<p>Makefiles allow you to include environment variables within their definitions,  which can be useful for setting configuration parameters or passing values to   the commands executed by Make.</p> <p>To define an environment variable in a Makefile, you can use the <code>export</code>  directive followed by the variable name and its value. Here's an example:</p> <pre><code>export MY_VARIABLE = my_value\n\nrule:\necho $(MY_VARIABLE)\n</code></pre> <p>In this example, the <code>MY_VARIABLE</code> environment variable is defined with the  value <code>'my_value'</code>. The <code>rule</code> target then uses the <code>echo</code> command to display   the value of <code>MY_VARIABLE</code>.</p> <p>Including a <code>.venv</code> File</p> <p>Sometimes, it is necessary to include environment variables defined in an  external file, such as a <code>.venv</code> file used for managing virtual environments.   You can achieve this by using the <code>include</code> directive in your Makefile.</p> <p>Assuming you have a <code>.venv</code> file with environment variable assignments like:</p> <pre><code>VAR1=value1\nVAR2=value2\n</code></pre> <p>You can include these variables in your Makefile as follows:</p> <pre><code>include .venv\nrule:\necho $(VAR1)\necho $(VAR2)\n</code></pre> <p>In this example, the <code>.venv</code> file is included in the Makefile, and its  variables (<code>VAR1</code> and <code>VAR2</code>) are accessible and can be used in the rules.</p>"}, {"location": "mlops/tools/makefile/#opening-a-browser-with-a-python-script", "title": "Opening a Browser with a Python Script", "text": "<p>If you want to open a browser from within a Python script, you can make use of  the <code>webbrowser</code> module. This module provides a high-level interface for   displaying web-based documents to users. Here's an example script that opens    a URL in the default web browser:</p> <pre><code>define BROWSER_PYSCRIPT\nimport os, webbrowser, sys\nfrom urllib.request import pathname2url\nwebbrowser.open('file://' + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\nBROWSER := poetry run python -c '$$BROWSER_PYSCRIPT'\n</code></pre> <p>In this script, the <code>webbrowser.open()</code> function is used to open the specified  URL in the default web browser. When you run the script, it will launch the   browser and navigate to the given URL.</p> <p>You can integrate this Python script into your Makefile by creating a rule that  executes the script. For example:</p> <pre><code>coverage:\npoetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage html --rcfile=pyproject.toml\n    $(BROWSER) htmlcov/index.html\n</code></pre> <p>Running <code>make open_browser</code> will execute the <code>open_browser.py</code></p>"}, {"location": "mlops/tools/makefile/#script-template", "title": "Script template", "text": "<pre><code>.PHONY: clean clean-build clean-pyc clean-test create-network db-start\nprecommit precommit_style pylint precommit_security test_check_coverage coverage\n install install_dev docs test_benchmark\n\ndefine BROWSER_PYSCRIPT # (1)\nimport os, webbrowser, sys\n\nfrom urllib.request import pathname2url\n\nwebbrowser.open('file://' + pathname2url(os.path.abspath(sys.argv[1])))\nendef\nexport BROWSER_PYSCRIPT\n\nBROWSER := poetry run python -c '$$BROWSER_PYSCRIPT'\nclean: clean-build clean-pyc clean-test\n\nclean-build:\n    rm -fr build/\n    rm -fr dist/\n    rm -fr .eggs/\n    find . -name '*.egg-info' -exec rm -fr {} +\n    find . -name '*.egg' -exec rm -f {} +\n\nclean-pyc:\n    find . -name '*.pyc' -exec rm -f {} +\n    find . -name '*.pyo' -exec rm -f {} +\n    find . -name '__pycache__' -exec rm -fr {} +\n\nclean-test:\n    rm -f .coverage\n    rm -fr htmlcov/\n    rm -fr .pytest_cache\n\ncompose-build:\n    docker compose -f docker-compose.yml build --no-cache\n\ncompose-up:\n    docker compose -f docker-compose.yml up\n\ncreate-network:\n    docker network create my-docker-network\n\ndb-start:\n    docker compose up -d postgres\n\ndocs:\n    poetry run mkdocs build\n\nclean-docs:\n    rm -fr site/\n\nprecommit:\n    poetry run pre-commit run -a\n\nprecommit_security:\n    SKIP=mypy,flakeheaven,black,isort,pycln,check-docstring-first,\n    check-case-conflict,trailing-whitespace,end-of-file-fixer,debug-statements,\n    check-ast,check-json,check-yaml,no-commit-to-branch poetry run pre-commit\n     run -a\n\npylint:\n    pylint --disable=R,C src/\n\nprecommit_style:\n    SKIP=detect-aws-credentials,detect-private-key,check-added-large-files,\n    bandit,no-commit-to-branch poetry run pre-commit run -a\n\ntest:\n    poetry run pytest -vvv --benchmark-skip\n\ntest_check_coverage:\n    # Check pyproject.toml configuration\npoetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage report --rcfile=pyproject.toml\n\ncoverage:\n    poetry run coverage run --rcfile=pyproject.toml -m pytest --benchmark-skip\n    poetry run coverage html --rcfile=pyproject.toml\n    $(BROWSER) htmlcov/index.html\n\ninstall: clean\n    poetry install --no-dev\n\ninstall_dev: clean\n    poetry install\n\ntest_benchmark:\n    poetry run pytest -vvv --benchmark-autosave --benchmark-only\n\ninit:\n    poetry install\n    poetry run ipython kernel install --name 'my_kernel_name' --user\n    dvc pull\n</code></pre> <ol> <li> This is the way to declare a function inside a Makefile</li> </ol>"}, {"location": "python/environments/", "title": "Virtual environments", "text": ""}, {"location": "python/environments/#virtualenv", "title": "virtualenv", "text": ""}, {"location": "python/environments/#create-an-environment", "title": "Create an environment", "text": "<pre><code>python -m venv .venv\nsource .venv/bin/activate\n</code></pre> <p>You can check the wheter the environment was activated or not doing:</p> <pre><code>which pip\n</code></pre> <p>And you should see something similar to:</p> <pre><code>usr/folder/.venv/bin/pip\n</code></pre>"}, {"location": "python/pytest/", "title": "Pytests", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#pytest", "title": "Pytest", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#test-layouts", "title": "Test layouts", "text": "<ul> <li>Directory layout starts with <code>tests</code></li> <li>From <code>tests</code> you can add anything like <code>unit</code>, <code>functional</code> or other meaningful   names like <code>database</code></li> <li>Files need to be pre-fixed with <code>test_</code></li> <li>Test functions need to be prefixed with <code>test_</code></li> <li>Test classes need to be prefixed with <code>Test</code></li> </ul>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#how-syntax-works", "title": "How syntax works", "text": "<p>Tests can be functions or classes</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#functions", "title": "Functions", "text": "<pre><code>def test_my_function():\nassert 1 == 1\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#classes", "title": "Classes", "text": "<p>Classes do not need inheritance:</p> <p></p> <pre><code># This function is here for convenience only, in a real-world scenario this function\n# would be elsewhere in a package\ndef str_to_int(string):\n'''\n    Parses a string number into an integer, optionally converting to a float\n    and rounding down.\n    You can pass '1.1' which returns 1\n    ['1'] -&gt; raises RuntimeError\n    '''\nerror_msg = 'Unable to convert to integer: '%s'' % str(string)\ntry:\ninteger = float(string.replace(',', '.'))\nexcept AttributeError:\n# this might be a integer already, so try to use it, otherwise raise\n# the original exception\nif isinstance(string, (int, float)):\ninteger = string\nelse:\nraise RuntimeError(error_msg)\nexcept (TypeError, ValueError):\nraise RuntimeError(error_msg)\nreturn int(integer)\n# When you create yout class test you have special methods\nclass TestStrToInt:\ndef setup_method(self):\nprint('\\nthis is setup')\ndef teardown_method(self):\nprint('\\nthis is teardown')\ndef setup_class(cls):\nprint('\\nthis is setup class')\ndef teardown_class(cls):\nprint('\\nthis is teardown class')\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\nassert result == 2\ndef test_round_down_lesser_half(self):\nresult = str_to_int('1.2')\nassert result == 2\n</code></pre> <p>That setup_class is executed before a test in a class and happens just once, and setup_method is executed before every test in the class.</p> <p>You can use these special methods to run code before all tests in a class or before each one.</p> <p>You can see the ouput here:</p> Ouptut example <pre><code>======================================= test session starts =======================================\nplatform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0\nrootdir: /home/coder/python-testing/notebooks/lesson2\ncollected 2 items\ntest-classes/test_classes.py FF                                                             [100%]\n============================================ FAILURES =============================================\n__________________________________ TestStrToInt.test_rounds_down __________________________________\nself = &lt;test_classes.TestStrToInt object at 0x7f9e8a8c8220&gt;\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest-classes/test_classes.py:44: AssertionError\n-------------------------------------- Captured stdout setup --------------------------------------\nthis is setup class\nthis is setup\n------------------------------------ Captured stdout teardown -------------------------------------\nthis is teardown\n____________________________ TestStrToInt.test_round_down_lesser_half _____________________________\nself = &lt;test_classes.TestStrToInt object at 0x7f9e8a8c8340&gt;\ndef test_round_down_lesser_half(self):\nresult = str_to_int('1.2')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest-classes/test_classes.py:48: AssertionError\n-------------------------------------- Captured stdout setup --------------------------------------\nthis is setup\n------------------------------------ Captured stdout teardown -------------------------------------\nthis is teardown\nthis is teardown class\n===================================== short test summary info =====================================\nFAILED test-classes/test_classes.py::TestStrToInt::test_rounds_down - assert 1 == 2\nFAILED test-classes/test_classes.py::TestStrToInt::test_round_down_lesser_half - assert 1 == 2\n======================================== 2 failed in 0.02s ========================================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#run-tests", "title": "Run tests", "text": "<p>In the test directory</p> <pre><code>pytest -vvvv tests/\n</code></pre> Ouptut example <pre><code>============================= test session starts ==============================\nplatform linux2 -- Python 2.7.17, pytest-3.6.4, py-1.8.0, pluggy-0.7.1 -- /usr/bin/python2\ncachedir: .pytest_cache\nrootdir: /content, inifile:\ncollecting 0 items\ncollecting 2 items\ncollecting 2 items\ncollecting 2 items\ncollected 2 items\ntest_util.py::TestFloats::test_rounds_down FAILED                        [ 50%]\ntest_util.py::TestFloats::test_round_down_lesser_half FAILED             [100%]\n=================================== FAILURES ===================================\n_________________________ TestFloats.test_rounds_down __________________________\nself = &lt;test_util.TestFloats instance at 0x7fbf26d90870&gt;\ndef test_rounds_down(self):\nresult = str_to_int('1.99')\n&gt;       assert result == 2\nE       assert 1 == 2\ntest_util.py:42: AssertionError\nshow more (open the raw output data in a text editor) ...\nthis is teardown\nthis is teardown class\n=========================== 2 failed in 0.04 seconds ===========================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#testing-failures", "title": "Testing failures", "text": "<p>Enter to the python debugger where your code is failing:</p> <pre><code>pytest --pdb test_failure_output.py\n</code></pre> <p>Once entered in the debugger you can type <code>h</code> to see the commands that you can use.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#another-commands-for-pytest", "title": "Another commands for pytest", "text": "<ul> <li><code>--collect-only</code> -&gt; Only collect tests, don't execute them</li> <li><code>-x</code> -&gt; Stop at the first failure</li> </ul> <p>To see all type:</p> <pre><code>pytest --help\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#plugins", "title": "Plugins", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#pytest-xdist", "title": "pytest-xdist", "text": "<p>Gives you the ability to run instance for running your test using the <code>-n</code> cli parameter.</p> <pre><code>pytest -n 4 test/\n</code></pre> <p>Going to set 4 differents runner instances and run them at the same time.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#nbval", "title": "nbval", "text": "<p>The plugin adds functionality to py.test to recognise and collect Jupyter  notebooks. The intended purpose of the tests is to determine whether execution   of the stored inputs match the stored outputs of the .ipynb file. Whilst also    ensuring that the notebooks are running without errors.</p> <p>The tests were designed to ensure that Jupyter notebooks (especially those for  reference and documentation), are executing consistently.</p> <p>Each cell is taken as a test, a cell that doesn't reproduce the expected output  will fail.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#installation", "title": "Installation", "text": "<pre><code>pip install nbval\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#usage", "title": "Usage", "text": "<pre><code>python -m pytest --nbval notebooks/\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#other-functionalities", "title": "Other functionalities", "text": "", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#parametrize-tests", "title": "Parametrize tests", "text": "<p>Parametrize tests it's like put a for loop over the tests that you want to expect  the same result using the same function. The problem with plain for loops it's  that the output it's a little bit confused. You don't know really where the error  is located and if the rest of the loop it's going to be cover. So a good  option it's parametrize tests.</p> str_to_bool function: <pre><code>  def str_to_bool(val):\n'''\n    Convert a string representation of truth to True or False\n    True values are 'y', 'yes', or ''; case-insensitive\n    False values are 'n', or 'no'; case-insensitive\n    Raises ValueError if 'val' is anything else.\n    '''\ntrue_vals = ['yes', 'y', '']\nfalse_vals = ['no', 'n']\ntry:\nval = val.lower()\nexcept AttributeError:\nval = str(val).lower()\nif val in true_vals:\nreturn True\nelif val in false_vals:\nreturn False\nelse:\nraise ValueError('Invalid input value: %s' % val)\n</code></pre> <pre><code> import pytest\nfrom src impport str_to_bool # function to convert string to bool\n@pytest.mark.parametrize('value', ['y', 'yes', ''])\ndef test_is_true(value):\nresult = str_to_bool(value)\nassert result is True\n</code></pre> Example output: <pre><code>======================================= test session starts =======================================\nplatform linux -- Python 3.8.10, pytest-7.2.0, pluggy-1.0.0\nrootdir: /home/coder/python-testing/notebooks/lesson2\ncollected 3 items\nparametrize/test_utils.py ...                                                               [100%]\n======================================== 3 passed in 0.01s ========================================\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixtures", "title": "Fixtures", "text": "<p>In pytest, fixtures are a way to provide data or test-doubles (mocks, stubs, etc) to your tests. They are created using the <code>@pytest.fixture</code> decorator and then injected into your tests as arguments. Fixtures are meant to simplify test setup and teardown code, and they help to make your tests more modular and scalable.</p> <p>Here's a basic example of how to use a fixture in pytest:</p> <pre><code>import pytest\n# Define a fixture\n@pytest.fixture\ndef my_fixture():\nreturn 'Hello, World!'\n# Use the fixture in a test\ndef test_hello(my_fixture):\nassert my_fixture == 'Hello, World!'\n</code></pre> <p>In this example, the <code>my_fixture</code> fixture is defined to return the string <code>'Hello, World!'</code>. Then, in the <code>test_hello</code> test, <code>my_fixture</code> is injected as an argument. When pytest runs this test, it first calls the my_fixture fixture function and then passes its return value to <code>test_hello</code>.</p> <p>Here's a more complex example where a fixture is used for setup and teardown:</p> <pre><code>import pytest\n# Define a fixture\n@pytest.fixture\ndef database():\ndb = setup_database()  # Setup code\nyield db  # This is what will be injected into your tests\nteardown_database(db)  # Teardown code\n# Use the fixture in a test\ndef test_db(database):\nassert database.is_connected()\n</code></pre> <p>In this example, the <code>database</code> fixture is used to manage a database connection. The setup_database function is called to establish the connection, and then the connection object is yielded to the test. After the test runs, the <code>teardown_database</code> function is called to clean up the connection.</p>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixture-scopes", "title": "fixture scopes", "text": "<p>Fixture scope determines when a fixture is set up and torn down. The possible  scopes are function, class, module, package or session:</p> <ul> <li><code>function</code>: The default scope, the fixture is set up and torn down for each    test function.</li> <li><code>class</code>: The fixture is set up and torn down for each test class.</li> <li><code>module</code>: The fixture is set up and torn down once per test module.</li> <li><code>package</code>: The fixture is set up and torn down once per test package.</li> <li><code>session</code>: The fixture is set up once when the test session starts, and is    torn down once at the end of the test session.</li> </ul> <pre><code>import pytest\n@pytest.fixture(scope='module')\ndef module_fixture():\n# Setup code here\nyield 'Hello, Module!'\n# Teardown code here\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#fixture-dependencies", "title": "Fixture dependencies", "text": "<p>Fixtures can use other fixtures. This is often useful when you want to  modularize your fixtures for reuse and better organization.</p> <pre><code>import pytest\n@pytest.fixture\ndef order():\nreturn {'name': 'Burger', 'price': 7.99}\n@pytest.fixture\ndef cart(order):\nreturn [order]\ndef test_cart(cart):\nassert len(cart) == 1\n````\n#### conftest\nThe conftest.py file serves as a means of providing fixtures for an entire directory\nof tests. Any fixture defined in conftest.py will be automatically available to all\ntest files in the same directory and subdirectories.\n```python\n# conftest.py\nimport pytest\n@pytest.fixture\ndef my_fixture():\nreturn 'Available Everywhere'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#temporal-directories", "title": "temporal directories", "text": "<p>The tmpdir fixture is a built-in pytest fixture that creates a temporary directory unique to the test invocation, which is automatically cleaned up after the test.</p> <pre><code>class TestMyClass:\ndef test_write_Yes(self, tmpdir):\npath = str(tmpdir.join('test_value'))\nwrite_integer('Yes', path)\nwith open(path, 'r') as _f:\nvalue = _f.read()\nassert value == 'True'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/pytest/#monkeypatch", "title": "Monkeypatch", "text": "<p>The monkeypatch fixture helps to safely set/delete an attribute, dictionary item or environment variable or to modify sys.path for importing.</p> <pre><code>def test_monkeypatch(monkeypatch):\nresult = {'HELLO': 'world'}\nmonkeypatch.setenv('HELLO', 'monkeypatched')\nassert result['HELLO'] == 'monkeypatched'\n</code></pre>", "tags": ["Python", "Tests", "Development"]}, {"location": "python/apis/best_practices/", "title": "Best practices", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#api-best-practices-with-fastapi-in-python", "title": "API Best Practices with FastAPI in Python", "text": "<p>Building APIs is a common task for backend developers, and they serve as the backbone of many modern web and mobile applications. However, designing and building an API can be a complex process, and it's important to follow best practices to ensure the resulting API is robust, reliable, and easy to use. This article presents API best practices with specific examples using FastAPI, a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#1-design-first-code-later", "title": "1. Design First, Code Later", "text": "<p>It's important to have a clear plan before starting to code your API. This means having a detailed specification of your API, including the routes, methods, parameters, and expected responses. You can create such a specification using the OpenAPI standard, which FastAPI supports out of the box.</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n@app.get('/items/{item_id}')\nasync def read_item(item_id: int):\nreturn {'item_id': item_id}\n</code></pre> <p>In the above example, the OpenAPI schema is automatically generated and can be accessed at the <code>/docs</code> endpoint.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#2-consistent-and-restful-routing", "title": "2. Consistent and RESTful Routing", "text": "<p>Make sure your API routes are consistent and follow RESTful principles. This means using the correct HTTP methods (GET, POST, PUT, DELETE, etc.) and having meaningful, predictable URLs. FastAPI makes it easy to define these with Python decorators.</p> <pre><code>@app.get('/users/{user_id}')\nasync def read_user(user_id: int):\n# code to get user\n@app.post('/users/')\nasync def create_user(user: User):\n# code to create user\n</code></pre> HTTP Method Description Idempotent Safe GET Retrieves the current state of a resource. Read Only Yes Yes POST Creates a new resource. Write Only No No PUT Replaces the current state of a resource with a new state. Update existing Yes No PATCH Applies partial modifications to a resource. No No DELETE Deletes a resource. Yes No HEAD Similar to GET but only retrieves the headers of a response. DOes it exist? Yes Yes OPTIONS Returns the HTTP methods that the server supports for the specified URL. Yes Yes <ul> <li>Idempotent means that multiple identical requests should have the same effect    as a single request.</li> <li>Safe means that the method only retrieves data and does not modify it.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#3-use-request-validation", "title": "3. Use Request Validation", "text": "<p>FastAPI supports request validation using Pydantic models, which allow you to define the expected shape of the data using Python type hints. This can significantly reduce the amount of boilerplate validation code you need to write.</p> <pre><code>from pydantic import BaseModel\nclass Item(BaseModel):\nname: str\ndescription: str\nprice: float\ntax: float = None\n@app.post('/items/')\nasync def create_item(item: Item):\n# code to create item\n</code></pre> <p>In this example, FastAPI will automatically validate that the incoming request data matches the <code>Item</code> model, and if it doesn't, it will return a helpful error message.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#4-error-handling", "title": "4. Error Handling", "text": "<p>You should anticipate and handle errors gracefully in your API. FastAPI provides the HTTPException class which you can raise to return a specific HTTP status code and message.</p> <pre><code>from fastapi import HTTPException\n@app.get('/items/{item_id}')\nasync def read_item(item_id: int):\nitem = get_item(item_id)\nif not item:\nraise HTTPException(status_code=404, detail='Item not found')\nreturn item\n</code></pre> Error Type HTTP Status Code Description Bad Request 400 The server could not understand the request due to invalid syntax. Unauthorized 401 The client must authenticate itself to get the requested response. Forbidden 403 The client does not have access rights to the content; that is, it is unauthorized, so server is rejecting to give proper response. Not Found 404 The server can not find the requested resource. Method Not Allowed 405 The method specified in the request is not allowed for the resource identified by the request URI. Conflict 409 This response is sent when a request conflicts with the current state of the server. Internal Server Error 500 The server has encountered a situation it doesn't know how to handle. Service Unavailable 503 The server is not ready to handle the request. Common causes are a server that is down for maintenance or that is overloaded.", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#5-rate-limiting", "title": "5. Rate Limiting", "text": "<p>Protect your API from abuse and overuse by implementing rate limiting. While  FastAPI doesn't have built-in support for rate limiting, you can use   third-party libraries such as SlowApi.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#6-use-asynchronous-code", "title": "6. Use Asynchronous Code", "text": "<p>FastAPI supports asynchronous request handling using Python's async and await keywords. This can improve the performance of your API by allowing it to handle other requests while waiting for IO-bound tasks (like database queries) to complete.</p> <pre><code>@app.get('/items/')\nasync def read_items():\nitems = await get_all_items()  # (1)\nreturn items\n</code></pre> <ol> <li>An async function that gets all items from the database</li> </ol>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/best_practices/#7-logging-and-monitoring", "title": "7. Logging and Monitoring", "text": "<p>An essential aspect of maintaining and troubleshooting APIs is having robust logging and monitoring in place. Here are some best practices for logging and monitoring in FastAPI:</p> <ol> <li> <p>Use the standard library logging module: The logging module is a built-in    Python library that provides a flexible and powerful way to log messages from    your application. FastAPI has built-in support for the logging module, so    you can easily integrate it into your application\u200b1\u200b.</p> <pre><code>import logging\nlogger = logging.getLogger(__name__)\n@app.post('/items/')\nasync def create_item(item: Item):\nlogger.info(f'Creating item: {item.name}')\n# code to create item\n</code></pre> </li> <li> <p>Avoid using print() to log messages: print() does not provide the same level     of control and flexibility as other logging methods. Instead, use FastAPI\u2019s     built-in logger for more control over how messages are logged, including setting     log levels, adding contextual information, and formatting logs for easier readability\u200b1\u200b.</p> </li> <li> <p>Log as much information as possible: By logging detailed information, such as     the request URL, query parameters, headers, body, response status code, and more,     developers can easily pinpoint where an issue occurred and what caused it.     Use structured loggers to log data in a consistent format that can be easily     parsed by other tools\u200b1\u200b.</p> </li> <li> <p>Log exceptions: Logging exceptions allows developers to quickly identify and     debug errors in their applications. By logging exceptions, developers can     easily pinpoint where an issue occurred and what caused it. To log exceptions     with FastAPI, use a library like Python\u2019s built-in logging module\u200b1\u200b.</p> </li> <li> <p>Add context to your logs: Adding context to your logs helps you quickly identify     the source of an issue. By adding contextual information such as request and     response data, user IDs, or other relevant details, you can easily pinpoint     where the issue originated from\u200b1\u200b.</p> </li> <li> <p>Use a structured logger: Structured logging is a way of formatting log messages     so that they are easier to read and parse. FastAPI provides built-in support     for structured logging via its Logging middleware\u200b1\u200b.</p> </li> <li> <p>Configure your logger for production: Configuring your logger for production     ensures that the right information is being logged at the right time. This     includes setting up log levels so that only important messages are recorded,     and configuring the format of the logs so they are easy to read and interpret.     Additionally, ensure that sensitive data is not included in the logs, and     that access to the logs is restricted to authorized personnel\u200b1\u200b.</p> </li> <li> <p>Use an external service to store and analyze logs: Using an external service     to store logs is beneficial because it allows for more efficient log     management. Additionally, these services typically offer a range of features     such as real-time monitoring, alerting, and reporting capabilities\u200b1\u200b.</p> </li> </ol>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/", "title": "FastAPI", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#fastapi", "title": "FastApi", "text": "<p>FastAPI is a modern, fast (high-performance), web framework for building APIs with  Python 3.6+ based on standard Python type hints.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#installation", "title": "Installation", "text": "<p>To install FastAPI, you'll need a Python version of 3.6 or greater. You can install  it using pip:</p> <pre><code>pip install fastapi\npip install uvicorn\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#basic-example", "title": "Basic example", "text": "<p>Here's a basic example of a FastAPI application:</p> <pre><code>from fastapi import FastAPI\napp = FastAPI()\n@app.get('/')\ndef read_root():\nreturn {'Hello': 'World'}\n</code></pre> <p>You can run the application using Uvicorn:</p> <pre><code>uvicorn main:app --reload\n</code></pre> <p>This command refers to:</p> <ul> <li><code>uvicorn</code>: Python framework that allows us to run a python application.</li> <li><code>main</code>: the file main.py (the Python 'module').</li> <li><code>app</code>: the object created inside of main.py with the line app = FastAPI().</li> <li><code>--reload</code>: make the server restart after code changes. Only do this for development.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#path-parameters", "title": "Path parameters", "text": "<p>You can define path parameters by putting them in curly braces {} in the path  of the  route decorator:</p> <pre><code>@app.get('/items/{item_id}')\ndef read_item(item_id: int):\nreturn {'item_id': item_id}\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#query-parameters", "title": "Query Parameters", "text": "<p>If you want the client to send additional data, but not in the path, you can use  query parameters:</p> <pre><code>from typing import Optional\n@app.get('/items/')\ndef read_items(q: Optional[str] = None):\nif q:\nreturn {'item': q}\nreturn {'item': 'not found'}\n</code></pre> <p>In this case, <code>q</code> is an optional string query parameter.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#request-body", "title": "Request Body", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#type-hints", "title": "Type hints", "text": "<p>FastAPI automatically recognizes Python type hints in the function parameters:</p> <pre><code>from pydantic import BaseModel\nclass Item(BaseModel):\nname: str\ndescription: Optional[str] = None\nprice: float\ntax: Optional[float] = None\n@app.post('/items/')\ndef create_item(item: Item):\nreturn item\n</code></pre> <p>In this example, the item body request parameter is declared to be of type Item.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#example-script", "title": "Example script", "text": "<pre><code>from datetime import datetime\nfrom os.path import dirname, abspath, join\nfrom fastapi import FastAPI\nfrom fastapi.responses import FileResponse # for serving files\nfrom fastapi.staticfiles import StaticFiles # for serving static files\nfrom pydantic import BaseModel\ncurrent_dir = dirname(abspath(__file__)) # get the path of the current script\nstatic_path = join(current_dir, 'static')\napp = FastAPI()\napp.mount('/ui', StaticFiles(directory=static_path), name='ui')\nclass Body(BaseModel):\nstrftime: str\n@app.get('/')\ndef root():\nhtml_path = join(static_path, 'index.html')\nreturn FileResponse(html_path)\n@app.post('/generate')\ndef generate(body: Body):\n'''\n    Generate the current time given a strftime template. For example:\n    '%Y-%m-%dT%H:%M:%S.%f'\n    '''\ntmpl = body.strftime or '%Y-%m-%dT%H:%M:%S.%f'\nreturn {'date': datetime.now().strftime(tmpl)}\n@app.post('/azure_cognitive')\ndef azure_cognitive(body: Body):\n'''\n    Put here your code to create an Azure Cognitive service endpoint!\n    '''\nreturn {'result': None} # Change None\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#why-use-uvicorn", "title": "Why use uvicorn", "text": "<p>Uvicorn is an ASGI (Asynchronous Server Gateway Interface) server that is used to  run your FastAPI application. The ASGI specification fills in the gap left by the     traditional WSGI servers used for synchronous Python web applications, and allows    for greater concurrency and the use of long-lived connections, which are required     for modern web applications that need to handle things like WebSockets and HTTP/2.</p> <p>Here are some of the features that Uvicorn provides when used with FastAPI:</p> <ul> <li> <p>Performance: Uvicorn is one of the fastest ASGI servers due to its minimal and   highly optimized code base. It's built on uvloop and httptools, which are   themselves fast asynchronous I/O and HTTP parsing libraries, respectively.</p> </li> <li> <p>Concurrency: By supporting the ASGI specification, Uvicorn allows FastAPI applications to handle many connections concurrently. This is great for applications that need to handle long-lived connections, such as WebSocket connections, in addition to regular HTTP requests.</p> </li> <li> <p>Hot Reload: Uvicorn supports hot reloading, which means it can automatically restart the server whenever it detects changes to your source code. This is extremely useful during development.</p> </li> <li> <p>WebSockets and HTTP/2 Support: ASGI servers like Uvicorn can handle long-lived  connections, such as WebSocket connections, which can be used for real-time   communication between the server and the client. They also support HTTP/2,   which can provide performance benefits over HTTP/1.</p> </li> <li> <p>Integration with FastAPI: FastAPI is built to work seamlessly with Uvicorn and other ASGI servers. This means you can take full advantage of all the features provided by FastAPI and ASGI, while still getting the performance benefits of Uvicorn.</p> </li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/fastapi/#serving-a-ml-model", "title": "Serving a ML Model", "text": "<p>An example using Hugginface</p> <p>Better to use ProcessPoolExecutor</p> <pre><code>import asyncio\nimport time\nfrom concurrent.futures import ProcessPoolExecutor\nfrom fastapi import FastAPI, Request\nfrom sentence_transformers import SentenceTransformer\napp = FastAPI()\nsbertmodel = None\ndef create_model():\nglobal sbertmodel\nsbertmodel = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n# if you try to run all predicts concurrently, it will result in CPU trashing.\npool = ProcessPoolExecutor(max_workers=1, initializer=create_model)\ndef model_predict():\nts = time.time()\nvector = sbertmodel.encode('How big is London')\nreturn vector\nasync def vector_search(vector):\n# simulate I/O call (e.g. Vector Similarity Search using a VectorDB)\nawait asyncio.sleep(0.005)\n@app.get('/')\nasync def entrypoint(request: Request):\nloop = asyncio.get_event_loop()\nts = time.time()\n# worker should be initialized outside endpoint to avoid cold start\nvector = await loop.run_in_executor(pool, model_predict)\nprint(f\"Model  : {int((time.time() - ts) * 1000)}ms\")\nts = time.time()\nawait vector_search(vector)\nprint(f\"io task: {int((time.time() - ts) * 1000)}ms\")\nreturn \"ok\"\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/", "title": "Flask", "text": "", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#flask", "title": "Flask", "text": "<p>Flask is a web framework for Python, it's lightweight and easy to understand.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#setup", "title": "Setup", "text": "<p>First, we need to install Flask. We can do this with pip:</p> <pre><code>pip install flask\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#you-first-flask-application", "title": "You first Flask application", "text": "<p>Here's a basic Flask application:</p> <pre><code>from flask import Flask\napp = Flask(__name__)\n@app.route('/')\ndef hello_world():\nreturn 'Hello, World!'\nif __name__ == '__main__':\napp.run()\n</code></pre> <p>Here's what this code does:</p> <ul> <li><code>from flask import Flask</code> imports the Flask module.</li> <li><code>app = Flask(__name__)</code> creates an instance of the Flask class for our application.</li> <li><code>@app.route('/')</code> is a decorator that tells Flask what URL should trigger the   function that follows.</li> <li><code>def hello_world():</code> defines a function that returns the string 'Hello, World!'</li> <li><code>if __name__ == '__main__': app.run()</code> runs the application on the local   development server.</li> </ul> <p>To run the application, save the above code in a file called app.py, then run  python app.py from your terminal. You should see output indicating that the  server is running, and you can visit http://localhost:5000 in your web browser   to view your application.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#run-flask-api", "title": "Run flask api", "text": "<p>You can now run your Flask app by using the following command in your terminal:</p> <pre><code>python app.py\n</code></pre> <p>Then, open your web browser and navigate to <code>http://127.0.0.1:5000/</code>. You should see the text 'Hello, World!' displayed.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#add-more-routes", "title": "Add more routes", "text": "<p>Flask allows you to add more decorators to create more routes. Here's how you can  create a new route:</p> <pre><code>@app.route('/about')\ndef about():\nreturn 'About Page'\n</code></pre> <p>Now, if you navigate to <code>http://127.0.0.1:5000/about</code>, you will see the text 'About Page'.</p>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#script-example", "title": "Script example", "text": "<pre><code>from flask import Flask, abort\napp = Flask(__name__)\n@app.route('/') # / equals to the root of the website\ndef hello_world():\nreturn 'Hello, World!'\n@app.route('/error')\ndef error():\nabort(500, 'oooh some error!')\nif __name__ == '__main__':\napp.run(debug=True, port=8000, host='0.0.0.0')\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#llm-model-with-flask-roberta", "title": "LLM model with flask (RoBERTa)", "text": "<pre><code>from flask import Flask, request, jsonify\nimport torch\nimport numpy as np\nfrom transformers import RobertaTokenizer\nimport onnxruntime\napp = Flask(__name__)\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')\nsession = onnxruntime.InferenceSession('roberta-sequence-classification-9.onnx')\ndef to_numpy(tensor):\nreturn (\ntensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n)\n@app.route('/')\ndef home():\nreturn '&lt;h2&gt;RoBERTa sentiment analysis&lt;/h2&gt;'\n@app.route('/predict', methods=['POST'])\ndef predict():\ninput_ids = torch.tensor(\ntokenizer.encode(request.json[0], add_special_tokens=True)\n).unsqueeze(\n0\n)\ninputs = {session.get_inputs()[0].name: to_numpy(input_ids)}\nout = session.run(None, inputs)\nresult = np.argmax(out)\nreturn jsonify({'positive': bool(result)})\nif __name__ == '__main__':\napp.run(host='0.0.0.0', port=5000, debug=True)\n</code></pre> <ul> <li><code>roberta-sequence-classification-9.onnx</code> The model was donwloaded.</li> </ul>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/apis/flask/#send-a-post", "title": "Send a post", "text": "<pre><code>curl -X POST --header 'Content-Type: application/json'\\\n--data '['using curl is not to my liking']'\\\nhttp:/127.0.0.1:5000/predict\n</code></pre>", "tags": ["Development", "Application Programming Interface"]}, {"location": "python/cli/argparse/", "title": "Argparse", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#argparse", "title": "Argparse", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#introduction-to-argparse-in-python", "title": "Introduction to <code>argparse</code> in Python", "text": "<p><code>argparse</code> is a powerful module in Python that provides a convenient way to parse command-line arguments and options. It simplifies the process of building command-line interfaces (CLIs) for your Python scripts or applications. In this tutorial, we will explore the basics of <code>argparse</code> and learn how to use it effectively.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#installation-of-argparse", "title": "Installation of argparse", "text": "<p><code>argparse</code> is included in the Python standard library, so there is no need for  additional installation.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#getting-started", "title": "Getting Started", "text": "<p>To start using <code>argparse</code>, you need to import the module:</p> <pre><code>import argparse\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#creating-a-parser", "title": "Creating a Parser", "text": "<p>The first step in using <code>argparse</code> is to create an ArgumentParser object, which will  handle the parsing of command-line arguments. You can create a parser by invoking  the <code>argparse.ArgumentParser()</code> constructor:</p> <pre><code>parser = argparse.ArgumentParser()\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#adding-arguments", "title": "Adding Arguments", "text": "<p>Once you have a parser, you can add arguments to it. An argument is defined by invoking the <code>add_argument()</code> method on the parser object. Here\\'s an example that adds a positional argument:</p> <pre><code>parser.add_argument('filename', help='name of the file to process')\n</code></pre> <p>In the example above, we added a positional argument called <code>filename</code> that represents the name of the file to be processed. The <code>help</code> parameter provides a description of the argument, which is displayed when the user requests help information.</p> <p>You can also add optional arguments using the <code>add_argument()</code> method. Here's an example that adds an optional argument called <code>--verbose</code>:</p> <pre><code>parser.add_argument(\n'--verbose', action='store_true', help='increase output verbosity'\n)\n</code></pre> <p>In the example above, we added a positional argument called filename that represents the name of the file to be processed. The help parameter provides a description of the argument, which is displayed when the user requests help information.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#parsing-arguments", "title": "Parsing Arguments", "text": "<p>After you have added the desired arguments, you need to parse the command-line arguments provided by the user. This is done by invoking the <code>parse_args()</code> method on the parser object:</p> <pre><code>args = parser.parse_args()\n</code></pre> <p>The <code>parse_args()</code> method returns an object containing the values of the parsed  arguments. You can access the values by using dot notation on the <code>args</code> object.   For example, to access the value of the <code>filename</code> argument:</p> <pre><code>print(args.filename)\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#putting-it-all-together", "title": "Putting It All Together", "text": "<p>Here's an example that combines the concepts discussed above:</p> <pre><code>import argparse\nparser = argparse.ArgumentParser()\nparser.add_argument('filename', help='name of the file to process')\nparser.add_argument('--verbose', action='store_true', help='increase output verbosity')\nargs = parser.parse_args()\nprint('Processing file:', args.filename)\nif args.verbose:\nprint('Verbose mode enabled.')\n</code></pre> <p>ssuming this script is saved as <code>script.py</code>, you can run it from the command line as follows:</p> <pre><code>python script.py myfile.txt\n</code></pre> <p>If you include the <code>--verbose</code> flag:</p> <pre><code>python script.py myfile.txt --verbose\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/argparse/#example-script-of-argparse", "title": "Example script of argparse", "text": "<pre><code>import argparse\ndef greet_user(name, age=None, greeting='Hello', uppercase=False):\n# Modify greeting based on options\nif uppercase:\ngreeting = greeting.upper()\n# Greet the user\noutput = f'{greeting}, {name}!'\nif age:\noutput += f' You are {age} years old.'\nreturn output\nif __name__ == '__main__':\n# Create a parser\nparser = argparse.ArgumentParser(description='Script to greet a user')\n# Add arguments\nparser.add_argument('name', help='name of the user')  # positional argument\nparser.add_argument('--age', type=int, help='age of the user')  # optional argument with type validation\nparser.add_argument('--greeting', choices=['Hello', 'Hi', 'Hola'], default='Hello',\nhelp='choose a greeting from the given options')  # optional argument with choices and default value\nparser.add_argument('--uppercase', action='store_true', help='convert greeting to uppercase')  # optional argument with flag\n# Parse the arguments\nargs = parser.parse_args()\n# Call the greet_user function with the provided arguments\noutput = greet_user(args.name, args.age, args.greeting, args.uppercase)\n# Print the greeting\nprint(output)\n</code></pre> <p>Clarifications:     * When an argument has a action, by default is <code>False</code> until you put it in the     call to enable it, in this case it is <code>--sort</code>argument.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/", "title": "Click", "text": "", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#click-command-line-python-framework-tutorial", "title": "Click Command Line Python Framework Tutorial", "text": "<p>Click is a Python package that allows for the creation of beautiful command line interfaces (CLI) in a composable way. It's perfect for building command line applications and supports lazy argument parsing.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#installation", "title": "Installation", "text": "<p>First, we need to install Click. You can do this using pip:</p> <pre><code>pip install click\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#a-basic-click-command", "title": "A Basic Click Command", "text": "<p>Here is an example of a simple Click command:</p> <pre><code>import click\n@click.command()\ndef hello():\nclick.echo('Hello Click!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this script,<code>@click.command()</code> is a decorator which tells Click that this +\u00a1 function is a command line command. <code>click.echo</code> is a function that prints text to the console.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#command-with-arguments", "title": "Command with Arguments", "text": "<p>We can add arguments to our Click commands as follows:</p> <pre><code>import click\n@click.command()\n@click.argument('name')\ndef hello(name):\nclick.echo(f'Hello {name}!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this example, <code>@click.argument('name')</code> is another decorator which adds an argument to our command. Now, when you run the command, you need to provide an additional piece of information.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#command-with-options", "title": "Command with Options", "text": "<p>Options are like arguments, but they are optional and have default values:</p> <pre><code>import click\n@click.command()\n@click.option('--greeting', default='Hello', help='Change the greeting.')\n@click.argument('name')\ndef hello(greeting, name):\nclick.echo(f'{greeting} {name}!')\nif __name__ == '__main__':\nhello()\n</code></pre> <p>In this example, <code>@click.option('--greeting', default='Hello', help='Change the greeting.')</code> is an option. You can provide it when you run the command, or you can leave it out to use the default value.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#grouping-commands", "title": "Grouping commands", "text": "<p>You can group multiple commands together:</p> <pre><code>import click\n@click.group()\ndef cli():\npass\n@cli.command()\ndef initdb():\nclick.echo('Initialized the database')\n@cli.command()\ndef dropdb():\nclick.echo('Dropped the database')\nif __name__ == '__main__':\ncli()\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#example-click-script", "title": "Example click script", "text": "<pre><code>import click\n@click.group()\ndef cli():\n'Example CLI group. This is a Click command-line interface.'\npass\n@cli.command()\n@click.argument('name', nargs=1)\ndef greet(name):\n'This command greets NAME.'\nclick.echo(f'Hello, {name}!')\n@cli.command()\n@click.argument('name', nargs=1)\n@click.option('--count', default=1, help='Number of greetings.')\n@click.option('--greeting', '-g', default='Hello', help='Customize the greeting.')\ndef repeat(name, count, greeting):\n'This command repeats a greeting to NAME, COUNT times.'\nfor _ in range(count):\nclick.echo(f'{greeting}, {name}!')\n@cli.group()\ndef maths():\n'Maths commands group.'\npass\n@maths.command()\n@click.argument('numbers', nargs=-1, type=int)\ndef sum(numbers):\n'This command sums up NUMBERS.'\nclick.echo(f'Sum: {sum(numbers)}')\n@maths.command()\n@click.argument('numbers', nargs=-1, type=int)\ndef multiply(numbers):\n'This command multiplies NUMBERS.'\nresult = 1\nfor num in numbers:\nresult *= num\nclick.echo(f'Product: {result}')\n@cli.command()\n@click.argument('filepath', type=click.Path(exists=True))\ndef showfile(filepath):\n'This command shows the content of a FILEPATH.'\nwith open(filepath, 'r') as file:\ncontent = file.read()\nclick.echo(content)\nif __name__ == '__main__':\ncli()\n</code></pre> <p>This script provides a command-line interface with a <code>greet</code> command that accepts one argument (a name to greet), a <code>repeat</code> command that accepts a name and optional count and greeting options (to repeat the greeting a certain number of times), and a <code>maths</code> group with <code>sum</code> and <code>multiply</code> commands that perform mathematical operations on a list of numbers.</p> <p>You can run these commands like so:</p> <pre><code>python myscript.py greet Bob\npython myscript.py repeat Alice --count 3 --greeting Hi\npython myscript.py maths sum 1 2 3 4 5\npython myscript.py maths multiply 1 2 3 4 5\npython myscript.py showfile /path/to/file\n</code></pre>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#argument-vs-option", "title": "Argument vs Option", "text": "<p>In the context of command-line interfaces, the terms 'argument' and 'option' have specific meanings:</p> <ul> <li> <p>Argument: An argument is a value that is passed directly to a command. Arguments   are positional, meaning that their order matters. They are typically mandatory,    although some commands may allow optional arguments. For example, in the     command <code>cp source.txt dest.txt</code>, <code>source.txt</code> and <code>dest.txt</code> are arguments.</p> </li> <li> <p>Option: An option modifies the behavior of a command. It is usually prefixed    by a - (single dash for single-character options) or -- (double dash for    multi-character options). Options may or may not require a value to be provided.     For example, in the command <code>ls -l</code>, <code>-l</code> is an option that modifies the behavior     of the ls command.</p> </li> </ul> <p>In the context of the Click library in Python:</p> <ul> <li> <p>Argument: Click treats arguments similarly to how they are treated in     command-line interfaces. They are positional and are defined using the     <code>@click.argument()</code> decorator.</p> </li> <li> <p>Option: Click treats options as modifiers of commands. They are defined using     the <code>@click.option()</code> decorator, and can have default values. Options in Click     are always optional, and if not provided in the command line, the default value     is used.</p> </li> </ul> <p>Here's an example of a command in Click that uses both arguments and options:</p> <pre><code>@click.command()\n@click.argument('filename')\n@click.option('--verbose', is_flag=True, help='Enable verbose mode.')\ndef process(filename, verbose):\n'''Process a file. If --verbose is provided, print detailed information.'''\nif verbose:\nclick.echo(f'Processing file in verbose mode: {filename}')\nelse:\nclick.echo(f'Processing file: {filename}')\n</code></pre> <p>In this example, filename is an argument, and <code>--verbose</code> is an option. The <code>is_flag=True</code> argument to <code>@click.option()</code> means that <code>--verbose</code> is a flag that does not take a value; its presence in the command line sets verbose to True.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/cli/click/#create-an-unique-unique-entrypoint-in-click", "title": "Create an unique unique entrypoint in click", "text": "<p>If you have multiple command groups defined across different files, you can import them into a main file and create an entry point for your application. This way, you can keep your command groups logically separated, which can be beneficial as your application grows.</p> <p>Here's an example:</p> <p>Let's say you have two Python files, <code>file1.py</code> and <code>file2.py</code>, with different c ommand groups.</p> <p><code>file1.py</code>:</p> <pre><code>import click\n@click.group()\ndef group1():\n'''This is group1 commands.'''\npass\n@group1.command()\ndef command1():\nclick.echo('Executing command1 from group1.')\n@group1.command()\ndef command2():\nclick.echo('Executing command2 from group1.')\n</code></pre> <p><code>file2.py</code>:</p> <pre><code>import click\n@click.group()\ndef group2():\n'''This is group2 commands.'''\npass\n@group2.command()\ndef command1():\nclick.echo('Executing command1 from group2.')\n@group2.command()\ndef command2():\nclick.echo('Executing command2 from group2.')\n</code></pre> <p>You can then create an entry point in a main file (for example, main.py) that imports these groups and adds them to the main command group:</p> <p><code>main.py</code>:</p> <pre><code>import click\nfrom file1 import group1\nfrom file2 import group2\n@click.group()\ndef cli():\n'''This is the main entry point.'''\npass\ncli.add_command(group1)\ncli.add_command(group2)\nif __name__ == '__main__':\ncli()\n</code></pre> <p>Now, when you run <code>python main.py</code>, you'll have access to both command groups, <code>group1</code> and <code>group2</code>, and all their respective commands. The commands can be accessed like this:</p> <pre><code>python main.py group1 command1\n</code></pre> <p>Please note that the import statements in <code>main.py</code> assume that <code>file1.py</code> and <code>file2.py</code> are in the same directory as <code>main.py</code>. If they're not, you'll need to adjust the import statements to match your directory structure.</p>", "tags": ["Development", "Command Line interface"]}, {"location": "python/packaging/setuptools/", "title": "Setuptools", "text": "", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#the-setuppy-python-file", "title": "The setup.py python file", "text": "<p><code>setup.py</code> is a Python file that provides information about a module/package that you have created. It's the build script for setuptools. It tells setuptools about your package (such as the name and version) as well as files to include.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#creating-a-basic-setuppy-file", "title": "Creating a Basic <code>setup.py</code> File", "text": "<p>Here's an example of a basic setup.py file:</p> <pre><code>from setuptools import setup, find_packages\nsetup(\nname='MyPackage',\nversion='0.1',\npackages=find_packages(),\n)\n</code></pre> <p>In this example, we're importing <code>setup</code> and <code>find_packages</code> from setuptools.  <code>setup</code> is the function that sets up your package, and <code>find_packages</code>  automatically discovers all packages and subpackages.</p> <p>The <code>name</code> argument is the name of your package, and <code>version</code> is the current version of your package.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#adding-more-information", "title": "Adding More Information", "text": "<p>We can add more information about our package in the setup.py file:</p> <pre><code>setup(\nname='MyPackage',\nversion='0.1',\npackages=find_packages(),\ndescription='A sample Python package',\n# you cand add this using a function from the requierements.txt\ninstall_requires = ['click==7.3.0', 'colorama'],\nentry_points='''\n    [console_scripts]\n    command1=src.main:command1\n    '''\nlong_description=open('README.txt').read(),\nauthor='Your Name',\nauthor_email='your.email@example.com',\nurl='http://example.com/MyPackage/',\nlicense='LICENSE.txt',\n)\n</code></pre> <p>Here, description provides a short description of the package. <code>long_description</code>  can be a detailed description, which we are reading from a file named <code>README.txt</code>.  <code>author</code> and <code>author_email</code> are self-explanatory. <code>url</code> is the URL for the homepage  of the package. <code>license</code> specifies the license under which the package is released.</p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#including-additional-files-manifestin", "title": "Including Additional Files - `MANIFEST.in", "text": "<p>To include additional files such as the README, you can use the <code>MANIFEST.in</code> file. Create a file named <code>MANIFEST.in</code> in the same directory as setup.py and list any additional files you want to include in the package:</p> <pre><code>include README.txt\ninclude LICENSE.txt\n</code></pre> <p>You can also specify this directly in the setup.py file:</p> <pre><code>setup(\n# \u2026\ninclude_package_data=True,\npackage_data={\n'': ['README.txt', 'LICENSE.txt'],\n},\n)\n</code></pre> <p>Here, <code>include_package_data=True</code> tells setuptools to include any data files specified in package_data or <code>MANIFEST.in.</code></p>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#installing-the-package", "title": "Installing the Package", "text": "<p>Once you've written your <code>setup.py</code>, you can install your package using <code>pip</code>:</p> <pre><code>pip install .\n</code></pre>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#installing-editable", "title": "Installing editable", "text": "<p>This command installs the package in the current directory. If you want to install  the package in editable mode (i.e., changes to the source code are immediately  available without needing to reinstall the package), you can use:</p> PipPyhton <pre><code>pip install -e .\n</code></pre> <pre><code>python setup.py develop\n</code></pre>", "tags": ["Development", "Packaging"]}, {"location": "python/packaging/setuptools/#tutorials", "title": "Tutorials", "text": "<ul> <li>Setuptools official documentaion</li> </ul>", "tags": ["Development", "Packaging"]}]}